---
title: "ARMA/ARIMA/SARIMA Models"
editor_options: 
  chunk_output_type: inline
toc: false
---

```{r include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Fitting NYC Arrests Series by Crime

```{r setup, include = FALSE}
library(reticulate)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(forecast)
library(TSstudio)
library(tseries)
library(gridExtra)
library(kableExtra)
library(astsa)
load("data/arrests_ts.Rdata")
```

::: panel-tabset
# Total

We are fitting a SARIMA model as we saw a clear seasonal component in the previous section. We need to determine the following parameters:

-   p: The order of AR terms.
-   d: The degree of differencing needed to make the data stationary.
-   q: The order of MA terms.
-   P: The seasonal order of AR terms.
-   D: The seasonal degree of differencing.
-   Q: The seasonal order of MA terms.
-   s: The seasonal period.

## ACF and PACF

```{r}
ggtsdisplay(arrests_ts)
```

## First Difference

```{r, echo=FALSE}
arrests_ts %>% diff %>% ggtsdisplay()
```

The series looks to be stationary at this level, though we should try a seasonal differencing. The seasonal period `s` is clearly 12. There are spikes at 1 and 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot.

## Seasonal Difference

```{r, echo=FALSE}
arrests_ts %>% diff(12) %>% ggtsdisplay()
```

This looks less stationary than a simple first differencing, we will try `D` = 0 and 1.

We will try the following parameters:

-   p: 1, 2, 4
-   d: 0, 1
-   q: 1, 4
-   P: 1
-   D: 0, 1
-   Q: 1, 2, 3

## Build Model

```{r, echo=FALSE}
# p <- 1
# d <- 1
# q <- 1
# P <- 1
# D <- 0
# Q <- 1
i <- 1

temp <- data.frame()
ls <- matrix(rep(NA,9*22), nrow=22)

for(p in c(1,2,3)){
  for(q in c(1,4)){
    for(d in c(1)){
      for(P in c(1)){
        for(Q in c(1,2,3)){
          for(D in c(0,1)){
            if(p + d + q + P + D + Q<= 9){
              model <- Arima(arrests_ts, order = c(p, d, q), seasonal = c(P, D, Q))
              ls[i,] <- c(p, d, q, P, D, Q, model$aic, model$bic, model$aicc)
              i <- i + 1
            }
          }
        }
      }
    }
  }
}

temp <- as.data.frame(ls)
names(temp) <- c("p","d","q","P", "D", "Q", "AIC","BIC","AICc")
```

## Model Selection and Diagnostics

### Minimum AIC

```{r, echo=FALSE}
kable(temp[which.min(temp$AIC),])
```

### Minimum BIC

```{r, echo = FALSE}
kable(temp[which.min(temp$BIC),])
```

### Minimum AICc

```{r, echo = FALSE}
kable(temp[which.min(temp$AICc),])
```

It is clear that the best model is one with parameters $p=1, d=1, q=1, P=1, D=1, Q=1$. We now check the model diagnostics.

```{r, echo = FALSE}
set.seed(621)
model_output <- capture.output(sarima(arrests_ts, 1, 1, 1, 1, 1, 1, 12))
```

The Ljung-Box statistic p-values suggest that there is no correlation between residuals, meaning we have a good model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.

### Auto-Arima

```{r, echo = FALSE}
auto_model <- auto.arima(arrests_ts)
auto_model
```

The information criteria are all better that the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12. This model is absolutely worth considering moving forward.

### Fitted vs. Actual

```{r, echo=FALSE}
model_fit <- Arima(arrests_ts, order = c(1, 1, 1), seasonal = c(1, 1, 1))
plot(arrests_ts, col = "blue")
lines(fitted(model_fit), col = "green")
lines(fitted(auto_model), col = "red")
legend(x = "topright", legend = c("arrests_ts", "fit1", "fit2"), fill = 4:1)
```

Both fitted models look similar to the actual time series. The two fitted models look incredibly similar, so we will just go with the one we fit by hand.

## Seasonal Cross Validation

1 step ahead and 12 steps ahead

## Forecasting

```{r, echo=FALSE}
autoplot(arrests_ts) +
  autolayer(meanf(arrests_ts, h = 36),
            series = "Mean", PI = FALSE) +
  autolayer(naive(arrests_ts, h = 36),
            series = "Naïve", PI = FALSE) +
  autolayer(snaive(arrests_ts, h = 36),
            series = "SNaïve", PI = FALSE) +
  autolayer(rwf(arrests_ts, h = 36, drift = TRUE),
              series = "Drift", PI = FALSE) +
  autolayer(forecast(model_fit, 36),
            series = "Fit", PI = FALSE) +
  guides(color = guide_legend(title = "Forecast"))
```

Both models appear to be better than SNaïve as they do a better job of capturing the trend.

SNaïve model error measurements:

```{r, echo = FALSE}
accuracy(snaive(arrests_ts, h = 36))
```

Fitted model error measurements:

```{r, echo = FALSE}
summary(model_fit)
summary(auto_model)
```

The model error measurements the model are all much lower than the SNaïve benchmark method.

```{r, echo = FALSE}
model_fit %>% forecast %>% autoplot()
```

Fit a normalized model for later comparison.

```{r, "code-fold = TRUE"}
model_fit <- Arima(scale(arrests_ts), order = c(1, 1, 1), seasonal = c(1, 1, 1))
summary(model_fit)
```

# Assault

We are fitting a SARIMA model as we saw a clear seasonal component in the previous section. We need to determine the following parameters:

-   p: The order of AR terms.
-   d: The degree of differencing needed to make the data stationary.
-   q: The order of MA terms.
-   P: The seasonal order of AR terms.
-   D: The seasonal degree of differencing.
-   Q: The seasonal order of MA terms.
-   s: The seasonal period.

## ACF and PACF

```{r}
ggtsdisplay(assault_ts)
```

## First Difference

```{r, echo=FALSE}
assault_ts %>% diff %>% ggtsdisplay()
```

The series looks to be stationary at this level, though we should try a seasonal differencing. The seasonal period `s` is clearly 12. There are spikes at 1 and 2 on the ACF plot and spikes at 1 and 2 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot.

## Seasonal Difference

```{r, echo=FALSE}
assault_ts %>% diff(12) %>% ggtsdisplay()
```

This looks less stationary than a simple first differencing, we will try `D` = 0 and 1.

We will try the following parameters:

-   p: 1, 2
-   d: 1
-   q: 1, 2
-   P: 1
-   D: 0, 1
-   Q: 1, 2, 3

## Build Model

```{r, echo=FALSE}
i <- 1

temp <- data.frame()
ls <- matrix(rep(NA,9*12), nrow=12)

for(p in c(1,2)){
  for(q in c(1,2)){
    for(d in c(1)){
      for(P in c(1)){
        for(Q in c(1,2,3)){
          for(D in c(0,1)){
            if(p + d + q + P + D + Q<= 7){
              model <- Arima(assault_ts, order = c(p, d, q), seasonal = c(P, D, Q))
              ls[i,] <- c(p, d, q, P, D, Q, model$aic, model$bic, model$aicc)
              i <- i + 1
            }
          }
        }
      }
    }
  }
}

temp <- as.data.frame(ls)
names(temp) <- c("p","d","q","P", "D", "Q", "AIC","BIC","AICc")
```

## Model Selection and Diagnostics

### Minimum AIC

```{r, echo=FALSE}
kable(temp[which.min(temp$AIC),])
```

### Minimum BIC

```{r, echo = FALSE}
kable(temp[which.min(temp$BIC),])
```

### Minimum AICc

```{r, echo = FALSE}
kable(temp[which.min(temp$AICc),])
```

It is clear that the best model is one with parameters $p=1, d=1, q=1, P=1, D=1, Q=1$. We now check the model diagnostics.

```{r  echo = FALSE}
set.seed(621)
model_output <- capture.output(sarima(assault_ts, 1, 1, 1, 1, 1, 1, 12))
```

The Ljung-Box statistic p-values suggest that there is no correlation between residuals at some lag values, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.

### Auto-Arima

```{r, echo = FALSE}
auto_model <- auto.arima(assault_ts)
auto_model
```

The information criteria are all worse than the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12. This model is not worth considering moving forward.

### Fitted vs. Actual

```{r, echo=FALSE}
model_fit <- Arima(assault_ts, order = c(1, 1, 1), seasonal = c(1, 1, 1))
plot(assault_ts, col = "blue")
lines(fitted(model_fit), col = "green")
legend(x = "topright", legend = c("assault_ts", "fit1"), fill = 4:1)
```

The fitted model looks similar to the actual time series.

## Seasonal Cross Validation

1 step ahead and 12 steps ahead

## Forecasting

```{r, echo=FALSE}
autoplot(assault_ts) +
  autolayer(meanf(assault_ts, h = 36),
            series = "Mean", PI = FALSE) +
  autolayer(naive(assault_ts, h = 36),
            series = "Naïve", PI = FALSE) +
  autolayer(snaive(assault_ts, h = 36),
            series = "SNaïve", PI = FALSE) +
  autolayer(rwf(assault_ts, h = 36, drift = TRUE),
              series = "Drift", PI = FALSE) +
  autolayer(forecast(model_fit, 36),
            series = "Fit", PI = FALSE) +
  guides(color = guide_legend(title = "Forecast"))
```

This model appears to be better than SNaïve as it does a better job of capturing the trend.

SNaïve model error measurements:

```{r, echo = FALSE}
accuracy(snaive(assault_ts, h = 36))
```

Fitted model error measurements:

```{r, echo = FALSE}
summary(model_fit)
summary(auto_model)
```

The model error measurements the model are all much lower than the SNaïve benchmark method.

```{r, echo = FALSE}
model_fit %>% forecast %>% autoplot()
```

# Controlled Substance Possession

We are fitting a SARIMA model as we saw a clear seasonal component in the previous section. We need to determine the following parameters:

-   p: The order of AR terms.
-   d: The degree of differencing needed to make the data stationary.
-   q: The order of MA terms.
-   P: The seasonal order of AR terms.
-   D: The seasonal degree of differencing.
-   Q: The seasonal order of MA terms.
-   s: The seasonal period.

## ACF and PACF

```{r}
ggtsdisplay(controlled_pos_ts)
```

## First Difference

```{r, echo=FALSE}
controlled_pos_ts %>% diff %>% ggtsdisplay()
```

The series looks to be stationary at this level, though we should try a seasonal differencing. The seasonal period `s` is clearly 12. There are spikes at 1, 3, 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot.

## Seasonal Difference

```{r, echo=FALSE}
controlled_pos_ts %>% diff(12) %>% ggtsdisplay()
```

This looks less stationary than a simple first differencing, we will try `D` = 0 and 1.

We will try the following parameters:

-   p: 1, 2, 4
-   d: 1
-   q: 1, 3, 4
-   P: 1
-   D: 0, 1
-   Q: 1, 2, 3

## Build Model

```{r, echo=FALSE}
i <- 1

temp <- data.frame()
ls <- matrix(rep(NA,9*23), nrow=23)

for(p in c(1,2)){
  for(q in c(1,2)){
    for(d in c(1)){
      for(P in c(1)){
        for(Q in c(1,2,3)){
          for(D in c(0,1)){
            if(p + d + q + P + D + Q <= 9){
              model <- Arima(controlled_pos_ts, order = c(p, d, q), seasonal = c(P, D, Q))
              ls[i,] <- c(p, d, q, P, D, Q, model$aic, model$bic, model$aicc)
              i <- i + 1
            }
          }
        }
      }
    }
  }
}

temp <- as.data.frame(ls)
names(temp) <- c("p","d","q","P", "D", "Q", "AIC","BIC","AICc")
```

## Model Selection and Diagnostics

### Minimum AIC

```{r, echo=FALSE}
kable(temp[which.min(temp$AIC),])
```

### Minimum BIC

```{r, echo = FALSE}
kable(temp[which.min(temp$BIC),])
```

### Minimum AICc

```{r, echo = FALSE}
kable(temp[which.min(temp$AICc),])
```

It is clear that the best model is one with parameters $p=1, d=1, q=1, P=1, D=1, Q=1$. We now check the model diagnostics.

```{r  echo = FALSE}
set.seed(621)
model_output <- capture.output(sarima(controlled_pos_ts, 1, 1, 1, 1, 1, 1, 12))
```

The Ljung-Box statistic p-values suggest that there is no correlation between residuals at some lag values, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.

### Auto-Arima

```{r, echo = FALSE}
auto_model <- auto.arima(controlled_pos_ts)
auto_model
```

The information criteria are all worse than the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12. This model is not worth considering moving forward.

### Fitted vs. Actual

```{r, echo=FALSE}
model_fit <- Arima(controlled_pos_ts, order = c(1, 1, 1), seasonal = c(1, 1, 1))
plot(controlled_pos_ts, col = "blue")
lines(fitted(model_fit), col = "green")
legend(x = "topright", legend = c("controlled_pos_ts", "fit1"), fill = 4:1)
```

The fitted model looks similar to the actual time series.

## Seasonal Cross Validation

1 step ahead and 12 steps ahead

## Forecasting

```{r, echo=FALSE}
autoplot(controlled_pos_ts) +
  autolayer(meanf(controlled_pos_ts, h = 36),
            series = "Mean", PI = FALSE) +
  autolayer(naive(controlled_pos_ts, h = 36),
            series = "Naïve", PI = FALSE) +
  autolayer(snaive(controlled_pos_ts, h = 36),
            series = "SNaïve", PI = FALSE) +
  autolayer(rwf(controlled_pos_ts, h = 36, drift = TRUE),
              series = "Drift", PI = FALSE) +
  autolayer(forecast(model_fit, 36),
            series = "Fit", PI = FALSE) +
  guides(color = guide_legend(title = "Forecast"))
```

This model appears to be better than SNaïve as it does a better job of capturing the trend.

SNaïve model error measurements:

```{r, echo = FALSE}
accuracy(snaive(controlled_pos_ts, h = 36))
```

Fitted model error measurements:

```{r, echo = FALSE}
summary(model_fit)
summary(auto_model)
```

The model error measurements the model are all much lower than the SNaïve benchmark method.

```{r, echo = FALSE}
model_fit %>% forecast %>% autoplot()
```

# Marijuana Possession

We are fitting a SARIMA model as we saw a clear seasonal component in the previous section. We need to determine the following parameters:

-   p: The order of AR terms.
-   d: The degree of differencing needed to make the data stationary.
-   q: The order of MA terms.
-   P: The seasonal order of AR terms.
-   D: The seasonal degree of differencing.
-   Q: The seasonal order of MA terms.
-   s: The seasonal period.

## ACF and PACF

```{r}
ggtsdisplay(marijuana_pos_ts)
```

## First Difference

```{r, echo=FALSE}
marijuana_pos_ts %>% diff %>% ggtsdisplay()
```

There still looks to be a trend at this level, we should try to difference one more time. We should also try a seasonal differencing at a seasonal period of 12.

## Second Difference

```{r}
marijuana_pos_ts %>% diff() %>% diff() %>% ggtsdisplay()
```

This looks more stationary, we should determine parameters to try from here.

The seasonal period `s` is clearly 12. There are spikes at 1 and 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot.

## Seasonal Difference

```{r, echo=FALSE}
marijuana_pos_ts %>% diff(12) %>% ggtsdisplay()
```

This looks less stationary than a simple first differencing, we will try `D` = 0 and 1.

We will try the following parameters:

-   p: 1, 2, 4
-   d: 1, 2
-   q: 1, 4
-   P: 1
-   D: 0, 1
-   Q: 1, 2, 3

## Build Model

```{r, echo=FALSE}
i <- 1

temp <- data.frame()
ls <- matrix(rep(NA,9*32), nrow=32)

for(p in c(1,2,4)){
  for(q in c(1,4)){
    for(d in c(1,2)){
      for(P in c(1)){
        for(Q in c(1,2,3)){
          for(D in c(0,1)){
            if(p + d + q + P + D + Q<= 9){
              model <- Arima(marijuana_pos_ts, order = c(p, d, q), seasonal = c(P, D, Q))
              ls[i,] <- c(p, d, q, P, D, Q, model$aic, model$bic, model$aicc)
              i <- i + 1
            }
          }
        }
      }
    }
  }
}

temp <- as.data.frame(ls)
names(temp) <- c("p","d","q","P", "D", "Q", "AIC","BIC","AICc")
```

## Model Selection and Diagnostics

### Minimum AIC

```{r, echo=FALSE}
kable(temp[which.min(temp$AIC),])
```

### Minimum BIC

```{r, echo = FALSE}
kable(temp[which.min(temp$BIC),])
```

### Minimum AICc

```{r, echo = FALSE}
kable(temp[which.min(temp$AICc),])
```

It is clear that the best model is one with parameters $p=1, d=1, q=1, P=1, D=1, Q=1$. We now check the model diagnostics.

```{r  echo = FALSE}
set.seed(621)
model_output <- capture.output(sarima(marijuana_pos_ts, 1, 1, 1, 1, 1, 1, 12))
```

The Ljung-Box statistic p-values suggest that there is no correlation between residuals at some lag values, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.

### Auto-Arima

```{r, echo = FALSE}
auto_model <- auto.arima(marijuana_pos_ts)
auto_model
```

The information criteria are all slightly better than the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12, but not so much better that this model would be worth considering moving forward.

### Fitted vs. Actual

```{r, echo=FALSE}
model_fit <- Arima(marijuana_pos_ts, order = c(1, 1, 1), seasonal = c(1, 1, 1))
plot(marijuana_pos_ts, col = "blue")
lines(fitted(model_fit), col = "green")
legend(x = "topright", legend = c("marijuana_pos_ts", "fit1"), fill = 4:1)
```

The fitted model looks similar to the actual time series.

## Seasonal Cross Validation

1 step ahead and 12 steps ahead

## Forecasting

```{r, echo=FALSE}
autoplot(marijuana_pos_ts) +
  autolayer(meanf(marijuana_pos_ts, h = 36),
            series = "Mean", PI = FALSE) +
  autolayer(naive(marijuana_pos_ts, h = 36),
            series = "Naïve", PI = FALSE) +
  autolayer(snaive(marijuana_pos_ts, h = 36),
            series = "SNaïve", PI = FALSE) +
  autolayer(rwf(marijuana_pos_ts, h = 36, drift = TRUE),
              series = "Drift", PI = FALSE) +
  autolayer(forecast(model_fit, 36),
            series = "Fit", PI = FALSE) +
  guides(color = guide_legend(title = "Forecast"))
```

This model appears to be better than SNaïve as it does a better job of capturing the trend.

SNaïve model error measurements:

```{r, echo = FALSE}
accuracy(snaive(marijuana_pos_ts, h = 36))
```

Fitted model error measurements:

```{r, echo = FALSE}
summary(model_fit)
summary(auto_model)
```

The model error measurements the model are all much lower than the SNaïve benchmark method.

```{r, echo = FALSE}
model_fit %>% forecast %>% autoplot()
```

It doesn't really make sense to forecast here as we can be sure that there will be no more arrests for marijuana possession, but it is good to know that we have fitted a good model. We can predict what would have happened to the level of arrests if the policy changes that were instituded over the years never happened.

# Motor Vehicle Theft

We are fitting an ARIMA model as we did not see a clear seasonal component in the previous section. We need to determine the following parameters:

-   p: The order of AR terms.
-   d: The degree of differencing needed to make the data stationary.
-   q: The order of MA terms.

## ACF and PACF

```{r}
ggtsdisplay(motor_theft_ts)
```

## First Difference

```{r, echo=FALSE}
motor_theft_ts %>% diff %>% ggtsdisplay()
```

This looks to be slightly more stationary than the original series, we can proceed with the model build process. There are spikes at 1 and 3 on the ACF plot and spikes at 1, 2 on the PACF plot.

We will try the following parameters:

-   p: 1, 2
-   d: 0, 1
-   q: 1, 3

## Build Model

```{r, echo=FALSE}
i <- 1

temp <- data.frame()
ls <- matrix(rep(NA,6*8), nrow=8)

for(p in c(1,2)){
  for(q in c(1,3)){
    for(d in c(0,1)){
      if(p + d + q <= 8){
        model <- Arima(motor_theft_ts, order = c(p, d, q))
        ls[i,] <- c(p, d, q, model$aic, model$bic, model$aicc)
        i <- i + 1
      }
    }
  }
}

temp <- as.data.frame(ls)
names(temp) <- c("p","d","q", "AIC","BIC","AICc")
```

## Model Selection and Diagnostics

### Minimum AIC

```{r, echo=FALSE}
kable(temp[which.min(temp$AIC),])
```

### Minimum BIC

```{r, echo = FALSE}
kable(temp[which.min(temp$BIC),])
```

### Minimum AICc

```{r, echo = FALSE}
kable(temp[which.min(temp$AICc),])
```

It is clear that the best model is one with parameters $p=2, d=1, q=3$. We now check the model diagnostics.

```{r  echo = FALSE}
set.seed(621)
model_output <- capture.output(sarima(motor_theft_ts, 2, 1, 3))
```

The Ljung-Box statistic p-values suggest that there may be correlation between residuals at some lag values, meaning we might not have a good enough model. We will still proceed with the model ARIMA(2, 1, 3).

### Auto-Arima

```{r, echo = FALSE}
auto_model <- auto.arima(motor_theft_ts)
auto_model
```

The information criteria are all slightly better than the previously used model of ARIMA(2, 1, 3), and it considers the seasonal component, so this model would be worth considering moving forward.

### Fitted vs. Actual

```{r, echo=FALSE}
model_fit <- Arima(motor_theft_ts, order = c(2, 1, 3))
plot(motor_theft_ts, col = "blue")
lines(fitted(model_fit), col = "green")
lines(fitted(auto_model), col = "red")
legend(x = "topright", legend = c("motor_theft_ts", "fit1", "auto_fit"), fill = 4:1)
```

The fitted model and auto mdoel look similar to the actual time series.

## Seasonal Cross Validation

1 step ahead and 12 steps ahead

## Forecasting

```{r, echo=FALSE}
autoplot(motor_theft_ts) +
  autolayer(meanf(motor_theft_ts, h = 36),
            series = "Mean", PI = FALSE) +
  autolayer(naive(motor_theft_ts, h = 36),
            series = "Naïve", PI = FALSE) +
  autolayer(snaive(motor_theft_ts, h = 36),
            series = "SNaïve", PI = FALSE) +
  autolayer(rwf(motor_theft_ts, h = 36, drift = TRUE),
              series = "Drift", PI = FALSE) +
  autolayer(forecast(model_fit, 36),
            series = "Fit", PI = FALSE) +
  autolayer(forecast(auto_model, 36),
            series = "Auto Fit", PI = FALSE) +
  guides(color = guide_legend(title = "Forecast"))
```

Both models appear to be better than SNaïve as they does a better job of capturing the trend. It is hard to tell which is better between the fit and the auto fit.

SNaïve model error measurements:

```{r, echo = FALSE}
accuracy(snaive(motor_theft_ts, h = 36))
```

Fitted model error measurements:

```{r, echo = FALSE}
summary(model_fit)
summary(auto_model)
```

The model error measurements the model are all much lower than the SNaïve benchmark method. The seasonal, auto fitted model slightly outperforms the non-seasonal ARIMA model, so we will go with that model.

```{r, echo = FALSE}
auto_model %>% forecast %>% autoplot()
```

# Murder

We are fitting an ARIMA model as we did not see a clear seasonal component in the previous section. We need to determine the following parameters:

-   p: The order of AR terms.
-   d: The degree of differencing needed to make the data stationary.
-   q: The order of MA terms.

## ACF and PACF

```{r}
ggtsdisplay(murder_ts)
```

## First Difference

```{r, echo=FALSE}
murder_ts %>% diff %>% ggtsdisplay()
```

This looks to be much more stationary than the original series, we can proceed with the model build process. There are spikes at 1 on the ACF plot and spikes at 1 and 2 on the PACF plot.

We will try the following parameters:

-   p: 1
-   d: 0, 1
-   q: 1, 2

## Build Model

```{r, echo=FALSE}
i <- 1

temp <- data.frame()
ls <- matrix(rep(NA,6*4), nrow=4)

for(p in c(1)){
  for(q in c(1,2)){
    for(d in c(0,1)){
      if(p + d + q <= 8){
        model <- Arima(murder_ts, order = c(p, d, q))
        ls[i,] <- c(p, d, q, model$aic, model$bic, model$aicc)
        i <- i + 1
      }
    }
  }
}

temp <- as.data.frame(ls)
names(temp) <- c("p","d","q", "AIC","BIC","AICc")
```

## Model Selection and Diagnostics

### Minimum AIC

```{r, echo=FALSE}
kable(temp[which.min(temp$AIC),])
```

### Minimum BIC

```{r, echo = FALSE}
kable(temp[which.min(temp$BIC),])
```

### Minimum AICc

```{r, echo = FALSE}
kable(temp[which.min(temp$AICc),])
```

It is clear that the best model is one with parameters $p=1, d=1, q=2$. We now check the model diagnostics.

```{r  echo = FALSE}
set.seed(621)
model_output <- capture.output(sarima(murder_ts, 1, 1, 2))
```

The Ljung-Box statistic p-values suggest that there may be correlation between residuals at some lag values, meaning we might not have a good enough model. We will still proceed with the model ARIMA(1, 1, 2).

### Auto-Arima

```{r, echo = FALSE}
auto_model <- auto.arima(murder_ts)
auto_model
```

The information criteria are all slightly better than the previously used model of ARIMA(1, 1, 2), but not so much better that this model would be worth considering moving forward.

### Fitted vs. Actual

```{r, echo=FALSE}
model_fit <- Arima(murder_ts, order = c(1, 1, 2))
plot(murder_ts, col = "blue")
lines(fitted(model_fit), col = "green")
legend(x = "topleft", legend = c("murder_ts", "fit1"), fill = 4:1)
```

The fitted model and auto mdoel look similar to the actual time series.

## Seasonal Cross Validation

1 step ahead and 12 steps ahead

## Forecasting

```{r, echo=FALSE}
autoplot(murder_ts) +
  autolayer(meanf(murder_ts, h = 36),
            series = "Mean", PI = FALSE) +
  autolayer(naive(murder_ts, h = 36),
            series = "Naïve", PI = FALSE) +
  autolayer(snaive(murder_ts, h = 36),
            series = "SNaïve", PI = FALSE) +
  autolayer(rwf(murder_ts, h = 36, drift = TRUE),
              series = "Drift", PI = FALSE) +
  autolayer(forecast(model_fit, 36),
            series = "Fit", PI = FALSE) +
  guides(color = guide_legend(title = "Forecast"))
```

The fitted model is better than SNaïve as they does a better job of capturing the trend.

SNaïve model error measurements:

```{r, echo = FALSE}
accuracy(snaive(murder_ts, h = 36))
```

Fitted model error measurements:

```{r, echo = FALSE}
summary(model_fit)
```

The model error measurements the model are all much lower than the SNaïve benchmark method.

```{r, echo = FALSE}
model_fit %>% forecast %>% autoplot()
```

# Robbery

We are fitting a SARIMA model as we saw a clear seasonal component in the previous section. We need to determine the following parameters:

-   p: The order of AR terms.
-   d: The degree of differencing needed to make the data stationary.
-   q: The order of MA terms.
-   P: The seasonal order of AR terms.
-   D: The seasonal degree of differencing.
-   Q: The seasonal order of MA terms.
-   s: The seasonal period.

## ACF and PACF

```{r}
ggtsdisplay(robbery_ts)
```

## First Difference

```{r, echo=FALSE}
robbery_ts %>% diff %>% ggtsdisplay()
```

The series looks to be stationary here. We should also try a seasonal differencing at a seasonal period of 12.

The seasonal period `s` is clearly 12. There are spikes at 1 and 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot.

## Seasonal Difference

```{r, echo=FALSE}
robbery_ts %>% diff(12) %>% ggtsdisplay()
```

This looks less stationary than a simple first differencing, we will try `D` = 0 and 1.

We will try the following parameters:

-   p: 1, 2, 4
-   d: 1
-   q: 1, 4
-   P: 1
-   D: 0, 1
-   Q: 1, 2, 3

## Build Model

```{r, echo=FALSE}
i <- 1

temp <- data.frame()
ls <- matrix(rep(NA,9*19), nrow=19)

for(p in c(1,2,4)){
  for(q in c(1,4)){
    for(d in c(1)){
      for(P in c(1)){
        for(Q in c(1,2,3)){
          for(D in c(0,1)){
            if(p + d + q + P + D + Q<= 9){
              tryCatch({
                model <- Arima(robbery_ts, order = c(p, d, q), seasonal = c(P, D, Q))
                ls[i,] <- c(p, d, q, P, D, Q, model$aic, model$bic, model$aicc)
              }, error = function(err) {
                cat("error: ", p, d, q, P, D, Q, "\n")
              }, finally = {
                i <- i + 1
              })
            }
          }
        }
      }
    }
  }
}

temp <- as.data.frame(ls)
names(temp) <- c("p","d","q","P", "D", "Q", "AIC","BIC","AICc")
```

## Model Selection and Diagnostics

### Minimum AIC

```{r, echo=FALSE}
kable(temp[which.min(temp$AIC),])
```

### Minimum BIC

```{r, echo = FALSE}
kable(temp[which.min(temp$BIC),])
```

### Minimum AICc

```{r, echo = FALSE}
kable(temp[which.min(temp$AICc),])
```

It is clear that the best model is one with parameters $p=1, d=1, q=1, P=1, D=1, Q=2$. We now check the model diagnostics.

```{r, echo = FALSE}
set.seed(621)
model_output <- capture.output(sarima(robbery_ts, 1, 1, 1, 1, 1, 2, 12))
```

The Ljung-Box statistic p-values suggest that there is no correlation between residuals, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 2)12.

### Auto-Arima

```{r, echo = FALSE}
auto_model <- auto.arima(robbery_ts)
auto_model
```

The information criteria are all worse than the previously used model of SARIMA(1, 1, 1)(1, 1, 2)12, this model is not worth considering moving forward.

### Fitted vs. Actual

```{r, echo=FALSE}
model_fit <- Arima(robbery_ts, order = c(1, 1, 1), seasonal = c(1, 1, 2))
plot(robbery_ts, col = "blue")
lines(fitted(model_fit), col = "green")
legend(x = "topright", legend = c("robbery_ts", "fit1"), fill = 4:1)
```

The fitted model looks similar to the actual time series.

## Seasonal Cross Validation

1 step ahead and 12 steps ahead

## Forecasting

```{r, echo=FALSE}
autoplot(robbery_ts) +
  autolayer(meanf(robbery_ts, h = 36),
            series = "Mean", PI = FALSE) +
  autolayer(naive(robbery_ts, h = 36),
            series = "Naïve", PI = FALSE) +
  autolayer(snaive(robbery_ts, h = 36),
            series = "SNaïve", PI = FALSE) +
  autolayer(rwf(robbery_ts, h = 36, drift = TRUE),
              series = "Drift", PI = FALSE) +
  autolayer(forecast(model_fit, 36),
            series = "Fit", PI = FALSE) +
  guides(color = guide_legend(title = "Forecast"))
```

This model appears to be better than SNaïve as it does a better job of capturing the trend.

SNaïve model error measurements:

```{r, echo = FALSE}
accuracy(snaive(robbery_ts, h = 36))
```

Fitted model error measurements:

```{r, echo = FALSE}
summary(model_fit)
```

The model error measurements the model are all much lower than the SNaïve benchmark method.

```{r, echo = FALSE}
model_fit %>% forecast %>% autoplot()
```
:::
