[
  {
    "objectID": "data_sources.html",
    "href": "data_sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "The data needed for this project can be divided as such:\n\n\n\n\nflowchart TD\nraw(Raw Crime Data)\nindividuals(\"Victimization and Recidivism\")\ngovernment(Government Programs) \neconomy(Economic Data)\n\n\n\n\n\n\n\nRaw Crime Data\nThe Socrata Open Data API (SODA) gives access to many sources of government data, including crime data over long periods of time. Large cities across the country maintain their own sites and databases which are accesible through this API.\n \n\n\nVictims and Recidivism\nThe National Crime Victimization Survey (NCVS) Series is an important source of information on criminal victimization.\n\n\n\n\n\nOther more specific studies such as “Gender, Mental Illness, and Crime in the United States” (Thompson 2011) help to give more detail on why crimes were commited by getting more detail on the individuals who commited them. In addition there is a useful dataset published by the Department of Justice with recidivism data, meaning data specifically looking at which criminals commit crimes again once they are out of prison and which ones do not.\n\n\nGovernment Programs\nDetailed time-series data is not critical here, but rather an understanding of which government programs were executed at what times and by which levels of government. This information provides context to the other datasets such as those on raw crime stats. An important part of the analysis will be comparing the raw data before, during and after these government programs.\n\n\nEconomic Data\nEconomic data can be accessed in similar ways as crime data, as the data needed for this project is maintained by the government. For example, comparing unemployment rate against crime rate could be a useful comparison. This unemployment data would be obtained from FRED. An example with data from here can be seen below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nThompson, Melissa. 2011. “Gender, Mental Illness, and Crime in the United States, 2004: Version 1.” ICPSR - Interuniversity Consortium for Political; Social Research. https://doi.org/10.3886/ICPSR27521.V1."
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "This graph shows the adjusted closing price for each of the five largest energy companies by market share. They all follow a similar pattern which is to be expected as they are in the same industry. However, it seems like Chevron (CVX) is the most volatile, though this could be due to it being the highest price per share. They are all fairly even on the year, being at a similar place now as they were at the start of the year, though Chevron is down, another difference between it and the rest of the group.\n\n\n\n\n\n\nThis graph shows the temperature at the National Arboretum from January through September in 2021. It follows a predictable pattern, with the highest temps in the summer and lowest in the winter. The lowest temps seem normal, but the high temps seem low. 83 was the highest temp recorded, which is much lower than temps we are having now, for example. This may have something to do with the time it was recorded, as if it was early in the morning then the low temps would be accurate but it would get hotter throughout the day, meaning the high temps are not accurate.\n\n\n\n\n\n\nThis graph shows the unemployment rate (percentage of people who are unemployed) in the United States since 1948. Various economic events can be seen, such as the 2008 recession and most notably, the COVID pandemic. There are large spikes in unemployment rate, followed by a more gradual decrease each time. Knowing this, we can assume there was a recession like event in late 1982 when there was a spike even higher than then one in 2008."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Crime\nTime-series data for crime throughout the United States is readily available and the variety of attempts the government has made over the years makes looking at how crime has changed over those years an interesting task.\n\n\nBig Picture\nDetermining which attempts by the government at city, state, and national levels have been successful and which attempts have been unsuccessful is an incredibly important task. Doing this will help the government to make better decisions to not only keep people safe, but also put their citizens in the best position to life prosperous lives, arguably the most important role the government can play.\nUnderstanding why criminals commit the crimes they do can help keep citizens safe, but it can also help potential criminals find a better path in life. Being aware of signals that suggest a crime is likely to be committed can help to stop those crimes before they happen. We can also use these data to help address problems that led criminals to commit crimes in the first place. Rehabilitation is an often overlooked part of the criminal justice process and something that can be addressed if we know more about why people commit the crimes they do.\n\n\nLiterature Review\nThere have been countless attempts to understand crime over the years, one interesting one being found in the Handbook of Labor Economics titled “The economics of crime” by Richard B. Freeman. He takes an interesting look at crime through an economic lens, describing this attempt as focusing on “…the effect of incentives on criminal behavior, the way decisions interact in a market setting; and the use of a benefit-cost framework to assess alternative strategies to reduce crime (Freeman 1999). This is a useful approach when looking at time-series data as economic data is possibly the most widely used application of such data.\nThis approach can also be seen when analyzing crime alongside the economy, as this can reveal many motivating factors for committing crime. A 2002 paper demonstrated how the declining labor market in the 1980’s, for example, coincided with increasing crime rates among young men during the same time period. The opposite trend was observed in the following decade (Gould, Weinberg, and Mustard 2002). Papers like these help to bridge the gap between the economy and crime and demonstrate how multi-disciplinary the subject truly is.\n\n\nAnalytical Angles\n\n\n\n\nflowchart TD\n\ncrime(Crime) --&gt; types(Types of Crime)\ncrime --&gt; policy(Policy Impact)\ncrime --&gt; reasons(Reasons for Crime)\ncrime --&gt; rehab(Rehabilitation)\ncrime --&gt; economy(The Economy)\n\ntypes --&gt; types2(Violent vs. Non-Violent)\npolicy --&gt; policy2(Government Action)\nreasons --&gt; reasons2(Drugs, Mental Health,\\n Economic Situation, etc.)\nrehab --&gt; rehab2(How to Help Criminals)\neconomy --&gt; economy2(Economic Influences\\n on Crime)\n\n\n\n\n\n\n\n\nGuiding Questions\n\nHow can we use the economy as a means by which to analyze crime data?\nWhat are some metrics we can use to determine the effectiveness of government action?\nAre there changes that we need to make in how we sentence criminals?\nIs it more effective to try and stop crimes before they are commited or to help those who commit crimes after they do so?\nWhat ethical concerns do we need to be aware of if we are attempting to predict who will commit crimes?\nIs action needed the most at the city, state, or federal level?\nIn terms of rehabilitation, are there some people who are simply beyond help?\nIs it worth looking at other countries to see what works and what doesn’t? Or are all countries more or less the same in their relationship with crime?\nWhich area of crime would be the most effective to attempt reform in?\nCould we quantify the negative effect different crimes have on society?\n\n\n\n\n\n\n\n\n\nReferences\n\nFreeman, Richard B. 1999. “Chapter 52 The Economics of Crime.” In Handbook of Labor Economics, 3:3529–71. Elsevier. https://doi.org/10.1016/S1573-4463(99)30043-2.\n\n\nGould, Eric D., Bruce A. Weinberg, and David B. Mustard. 2002. “Crime Rates and Local Labor Market Opportunities in the United States: 1979–1997.” Review of Economics and Statistics 84 (1): 45–61. https://doi.org/10.1162/003465302317331919."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thomas Sigall\nGeorgetown DSAN"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "What is a Time Series?\nAny metric that is measured over regular time intervals makes a Time Series. A time series is a sequence of data points or observations collected or recorded over a period of time at specific, equally spaced intervals. Each data point in a time series is associated with a particular time stamp or time period, making it possible to analyze and study how a particular variable or phenomenon changes over time. Time series data can be found in various domains and can represent a wide range of phenomena, including financial data, economic indicators, weather measurements, stock prices, sales figures, and more.\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones. The analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\nKey characteristics of time series data include:\nTemporal Order: Time series data is ordered chronologically, with each data point representing an observation at a specific point in time. The order of data points is critical for understanding trends and patterns over time.\nEqually Spaced Intervals: In most cases, time series data is collected at regular intervals, such as hourly, daily, weekly, monthly, or yearly. However, irregularly spaced time series data can also exist.\nDependency: Time series data often exhibits temporal dependency, meaning that the value at a given time is influenced by or related to the values at previous times. This dependency can take various forms, including trends, seasonality. This serial correlation is called as autocorrelation.\nComponents: Time series data can typically be decomposed into various components, including:\nTrend: The long-term movement or direction in the data. Seasonality: Repeating patterns or cycles that occur at fixed intervals. Noise/Irregularity: Random fluctuations or variability in the data that cannot be attributed to the trend or seasonality.\nApplications: Time series data is widely used for various applications, including forecasting future values, identifying patterns and anomalies, understanding underlying trends, and making informed decisions based on historical data.\nAnalyzing time series data involves techniques like time series decomposition, smoothing, statistical modeling, and forecasting. This class will cover but not be limited to traditional time series modeling including ARIMA, SARIMA, the multivariate Time Series modeling including; ARIMAX, SARIMAX, and VAR models, Financial Time Series modeling including; ARCH, GARCH models, and E-GARCH, M-GARCH..ect, Bayesian structural time series (BSTS) models, Spectral Analysis and Deep Learning Techniques for Time Series. Researchers and analysts use software tools like Python, R, and specialized time series libraries to work with and analyze time series data effectively.\nTime series analysis is essential in fields such as finance, economics, epidemiology, environmental science, engineering, and many others, as it provides insights into how variables change over time and allows for the development of predictive models to forecast future trends and outcomes."
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFreeman (1999)↩︎\nGould, Weinberg, and Mustard (2002)↩︎"
  },
  {
    "objectID": "data_sources.html#raw-data",
    "href": "data_sources.html#raw-data",
    "title": "Data Sources",
    "section": "Raw Data",
    "text": "Raw Data\nThe Socrata Open Data API (SODA) gives access to many sources of government data, including crime data over long periods of time. Large cities across the country maintain their own sites and databases which are accesible through this API."
  },
  {
    "objectID": "data_sources.html#studies",
    "href": "data_sources.html#studies",
    "title": "Data Sources",
    "section": "Studies",
    "text": "Studies\nThe National Crime Victimization Survey (NCVS) Series is an important source of information on criminal victimization.\n\n\n\n\n\nOther more specific studies such as “Gender, Mental Illness, and Crime in the United States” (Thompson 2011) help to give more detail on why crimes were commited by getting more detail on the individuals who commited them. In addition there is a useful dataset published by the Department of Justice"
  },
  {
    "objectID": "exploratory_data_analysis.html",
    "href": "exploratory_data_analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "TotalAssaultControlled Substance PossessionControlled Substance SaleLarcenyMarijuana PossessionMarijuana SaleMotor Vehicle TheftMurderRobbery\n\n\n\n\n\n\n\n\nThere appears to be some seasonality in the plot, as there are noticeable dips in arrest rate towards the end of each year. There was a slight upward trend towards the end of the 2000’s but that changed dramatically during the 2010’s into a clear downward trend. Since the start of the current decade the trend has reversed course, however. It is hard to tell if the time series is multiplicative or additive, as it seems to hold different characteristics at different points, though this possibly suggests a multiplicative time series. The change in trend could also be part of a longer cycle, though it is impossible to tell from this graph as it only goes back to 2006.\n\n\n\n\n\n\n\n\nLooking at the lag plot up to a lag of 12 months, we can see that the data are clearly not random, as a linear shape exists in each lag that we observe here. Seasonality is harder to observe here, as each month does not appear to be clustered alongside other observations of the same month. This could be due to the large number of points, however.\n\n\n\n\n\n\nIf we look up to a lag of 48 months, we can see that as the lag increases, the data get more random. The Lag 48 plot exhibiting less of a linear pattern than the lag 12 plot, which is fairly linear.\n\n\n\n\n\n\n\n\nWe can see the trend here that we were able to observe on the initial graph. There is horizontal movement during the first half of the observed period and then a clear downward trend until 2020. After that, it reverses and increases until the present day.\nRemoving the trend allows us to see some seasonality. Once the seasonality has been averaged out as it has been here, the clear dips in arrests towards the end of each year remain and there is a clear pattern between the large dips at the end of each month.\nThere seems to be more noise in the more recent data, which could be something interesting to look into.\n\n\n\n\n\n\n\n\nThe ACF values are decaying towards zero as lag increases, which is evidence against seasonality.\n\n\n\n\n\nSpikes can be seen at lags of 1, 2, 12, and 13.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  arrests_ts\nDickey-Fuller = -2.7629, Lag order = 5, p-value = 0.2564\nalternative hypothesis: stationary\n\n\nThe ADF tests returns a p-value of 0.26, meaning we do not have enough evidence to reject the null hypotheses. This series is not stationary.\n\n\n\n\ndetrend &lt;- resid(lm(arrests_by_date$total ~ arrests_by_date$month, na.action = NULL))\ndifference &lt;- diff(arrests_ts)\n\n\ngrid.arrange(autoplot(arrests_ts),\n             autoplot(as.ts(detrend)), \n             autoplot(difference), \n             ncol = 1)\n\n\n\n\nThe detrended series (the middle plot) does not seem to be vastly different from the original series. This could be evidence that there is not much of a trend in the data. The differenced series looks to be stationary, which suggests that there may be a trend.\n\nggtsdisplay(difference)\n\n\n\n\nThis ACF has positive spikes at 12, 24, and 36, meaning once the trend is removed, the seasonality becomes evident. The ACF of the differenced time series provides strong evidence in support of seasonality.\n\n\n\n\n\n\n\n\n\ninsert text here\n\n\n\n\n\n\n\n\ninsert text\n\n\n\n\n\n\ninsert text"
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "It is important to get an understanding of what types of crimes are commonly committed in New York City, the area we are currently looking at. We want to look at crimes that give us a good idea of the state of public safety in New York City. Those crimes along with their total counts from 2006-2022 are listed below.\nHomicide, Burglary, Drug-Related Offenses, Assault, Motor Vehicle Theft, Robbery, Drug-Related Violent Crimes, Domestic Violence\nWeapons possession?\n\n\n\n\n\npd_desc\ntotal\n\n\n\n\nassault\n681166\n\n\ncontrolled_substance_possession\n336350\n\n\ncontrolled_substance_sale\n241811\n\n\nlarceny\n396885\n\n\nmarijuana_possession\n445336\n\n\nmarijuana_sale\n67848\n\n\nmotor_vehicle_theft\n19099\n\n\nmurder\n16997\n\n\nrobbery\n179078\n\n\n\n\n\n\n\n# A tibble: 9 × 2\n  pd_desc                          total\n  &lt;chr&gt;                            &lt;int&gt;\n1 assault                         681166\n2 controlled_substance_possession 336350\n3 controlled_substance_sale       241811\n4 larceny                         396885\n5 marijuana_possession            445336\n6 marijuana_sale                   67848\n7 motor_vehicle_theft              19099\n8 murder                           16997\n9 robbery                         179078\n\n\nThese categories include multiple types of crimes, each of which is outlined below.\n\narrests_by_crime_detail &lt;- df %&gt;%\n  filter(str_detect(pd_desc, \"ASSAULT|ROBBERY|MARIJUANA, POSSESSION|MARIJUANA, SALE|CE,P|NCE, PE,I|CE, I|E,S|E, S|MURDER,UNCLASSIFIED|AUTO|LARCENY\")) %&gt;% group_by(pd_desc) %&gt;%\n  summarize(total = n())\n\narrests_by_crime_detail\n\n# A tibble: 46 × 2\n   pd_desc                                 total\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 AGGRAVATED GRAND LARCENY OF ATM           434\n 2 ASSAULT 2,1,PEACE OFFICER               17301\n 3 ASSAULT 2,1,UNCLASSIFIED               197972\n 4 ASSAULT 3                              460078\n 5 ASSAULT POLICE/PEACE OFFICER             4729\n 6 CONTROLLED SUBSTANCE, INTENT T           2053\n 7 CONTROLLED SUBSTANCE, INTENT TO SELL 5  16135\n 8 CONTROLLED SUBSTANCE, SALE 4             1498\n 9 CONTROLLED SUBSTANCE, SALE 5             4022\n10 CONTROLLED SUBSTANCE,POSSESS.            2727\n# ℹ 36 more rows\n\n\nLets look at how each of these crimes is changing over time.\n\n\n\n\n\n\nThis graph includes the eight most common types of arrests made over the time period. Some interesting patterns reveal themselves when the data are categorized as such, particularly the arrests for marijuana possession. They spike around 2011 but then drop off quickly and today are non-existent. This is an excellent example to use to observe the effect that real-world events have on the trends we observe here. In 2014, New York City mayor Bill de Blasio told the NYPD to stop arrests for marijuana possession and instead issue tickets in attempts to decriminalize marijuana (Dizard 2014). A drop in arrests is clearly seen around that time period. Around the same time, New York Governor Andrew Cuomo signed legislation which would allow the use of cannabis for medicinal purposes (Campbell 2014). Then, in 2021, recreational cannabis was legalized for adults over 21, up to a specific amount. Since then, the NYPD has not listed any crime related to marijuana, as evidenced by there being no arrests for possession on the graph past 2021."
  },
  {
    "objectID": "exploratory_data_analysis.html#initial-visualization",
    "href": "exploratory_data_analysis.html#initial-visualization",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "TotalAssault 3Marijuana Possession 4, 5Controlled Substance Possession 7Assault 2, 1, or UnclassifiedLarceny PetitWeaponsIntoxicated Driving, Alcohol\n\n\n\n\n\n\n\n\nThere appears to be some seasonality in the plot, as there are noticeable dips in arrest rate towards the end of each year. There was a slight upward trend towards the end of the 2000’s but that changed dramatically during the 2010’s into a clear downward trend. Since the start of the current decade the trend has reversed course, however. It is hard to tell if the time series is multiplicative or additive, as it seems to hold different characteristics at different points, though this possibly suggests a multiplicative time series. The change in trend could also be part of a longer cycle, though it is impossible to tell from this graph as it only goes back to 2006.\n\n\n\n\n\n\n\n\nLooking at the lag plot up to a lag of 12 months, we can see that the data are clearly not random, as a linear shape exists in each lag that we observe here. Seasonality is harder to observe here, as each month does not appear to be clustered alongside other observations of the same month. This could be due to the large number of points, however.\n\n\n\n\n\n\nIf we look up to a lag of 48 months, we can see that as the lag increases, the data get more random. The Lag 48 plot exhibiting less of a linear pattern than the lag 12 plot, which is fairly linear.\n\n\n\n\n\n\n\n\nWe can see the trend here that we were able to observe on the initial graph. There is horizontal movement during the first half of the observed period and then a clear downward trend until 2020. After that, it reverses and increases until the present day.\nRemoving the trend allows us to see some seasonality. Once the seasonality has been averaged out as it has been here, the clear dips in arrests towards the end of each year remain and there is a clear pattern between the large dips at the end of each month.\nThere seems to be more noise in the more recent data, which could be something interesting to look into.\n\n\n\n\n\n\n\n\nThe ACF values are decaying towards zero as lag increases, which is evidence against seasonality.\n\n\n\n\n\nSpikes can be seen at lags of 1, 2, 12, and 13.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  arrests_ts\nDickey-Fuller = -2.7629, Lag order = 5, p-value = 0.2564\nalternative hypothesis: stationary\n\n\nThe ADF tests returns a p-value of 0.26, meaning we do not have enough evidence to reject the null hypotheses. This series is not stationary.\n\n\n\n\ndetrend &lt;- resid(lm(arrests_by_date$total ~ arrests_by_date$month, na.action = NULL))\ndifference &lt;- diff(arrests_ts)\n\n\ngrid.arrange(autoplot(arrests_ts),\n             autoplot(as.ts(detrend)), \n             autoplot(difference), \n             ncol = 1)\n\n\n\n\nThe detrended series (the middle plot) does not seem to be vastly different from the original series. This could be evidence that there is not much of a trend in the data. The differenced series looks to be stationary, which suggests that there may be a trend.\n\nggtsdisplay(difference)\n\n\n\n\nThis ACF has positive spikes at 12, 24, and 36, meaning once the trend is removed, the seasonality becomes evident. The ACF of the differenced time series provides strong evidence in support of seasonality."
  },
  {
    "objectID": "exploratory_data_analysis.html#lag-plot",
    "href": "exploratory_data_analysis.html#lag-plot",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "arrests_ts &lt;- ts(arrests_by_date$count, start = c(2006, 1), frequency = 12)\n\nts_lags(arrests_ts, lags = 1:12)\n\n\n\n\n\nLooking at the lag plot up to a lag of 12 months, we can see that the data are clearly not random, as a linear shape exists in each lag that we observe here. Seasonality is harder to observe here, as each month does not appear to be clustered alongside other observations of the same month. This could be due to the large number of points, however.\n\nts_lags(arrests_ts, lags = c(12, 24, 36, 48))\n\n\n\n\n\nIf we look up to a lag of 48 months, we can see that as the lag increases, the data get more random. The Lag 48 plot exhibiting less of a linear pattern than the lag 12 plot, which is fairly linear."
  },
  {
    "objectID": "exploratory_data_analysis.html#decomposition",
    "href": "exploratory_data_analysis.html#decomposition",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "We can see the trend here that we were able to observe on the initial graph. There is horizontal movement during the first half of the observed period and then a clear downward trend until 2020. After that, it reverses and increases until the present day.\nRemoving the trend allows us to see some seasonality. Once the seasonality has been averaged out as it has been here, the clear dips in arrests towards the end of each year remain and there is a clear pattern between the large dips at the end of each month.\nThere seems to be more noise in the more recent data, which could be something interesting to look into."
  },
  {
    "objectID": "data_visualization.html#by-crime",
    "href": "data_visualization.html#by-crime",
    "title": "Data Visualization",
    "section": "",
    "text": "It is important to get an understanding of what types of crimes are commonly committed in New York City, the area we are currently looking at. We want to look at crimes that give us a good idea of the state of public safety in New York City. Those crimes along with their total counts from 2006-2022 are listed below.\nHomicide, Burglary, Drug-Related Offenses, Assault, Motor Vehicle Theft, Robbery, Drug-Related Violent Crimes, Domestic Violence\nWeapons possession?\n\n\n\n\n\npd_desc\ntotal\n\n\n\n\nassault\n681166\n\n\ncontrolled_substance_possession\n336350\n\n\ncontrolled_substance_sale\n241811\n\n\nlarceny\n396885\n\n\nmarijuana_possession\n445336\n\n\nmarijuana_sale\n67848\n\n\nmotor_vehicle_theft\n19099\n\n\nmurder\n16997\n\n\nrobbery\n179078\n\n\n\n\n\n\n\n# A tibble: 9 × 2\n  pd_desc                          total\n  &lt;chr&gt;                            &lt;int&gt;\n1 assault                         681166\n2 controlled_substance_possession 336350\n3 controlled_substance_sale       241811\n4 larceny                         396885\n5 marijuana_possession            445336\n6 marijuana_sale                   67848\n7 motor_vehicle_theft              19099\n8 murder                           16997\n9 robbery                         179078\n\n\nThese categories include multiple types of crimes, each of which is outlined below.\n\narrests_by_crime_detail &lt;- df %&gt;%\n  filter(str_detect(pd_desc, \"ASSAULT|ROBBERY|MARIJUANA, POSSESSION|MARIJUANA, SALE|CE,P|NCE, PE,I|CE, I|E,S|E, S|MURDER,UNCLASSIFIED|AUTO|LARCENY\")) %&gt;% group_by(pd_desc) %&gt;%\n  summarize(total = n())\n\narrests_by_crime_detail\n\n# A tibble: 46 × 2\n   pd_desc                                 total\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 AGGRAVATED GRAND LARCENY OF ATM           434\n 2 ASSAULT 2,1,PEACE OFFICER               17301\n 3 ASSAULT 2,1,UNCLASSIFIED               197972\n 4 ASSAULT 3                              460078\n 5 ASSAULT POLICE/PEACE OFFICER             4729\n 6 CONTROLLED SUBSTANCE, INTENT T           2053\n 7 CONTROLLED SUBSTANCE, INTENT TO SELL 5  16135\n 8 CONTROLLED SUBSTANCE, SALE 4             1498\n 9 CONTROLLED SUBSTANCE, SALE 5             4022\n10 CONTROLLED SUBSTANCE,POSSESS.            2727\n# ℹ 36 more rows\n\n\nLets look at how each of these crimes is changing over time.\n\n\n\n\n\n\nThis graph includes the eight most common types of arrests made over the time period. Some interesting patterns reveal themselves when the data are categorized as such, particularly the arrests for marijuana possession. They spike around 2011 but then drop off quickly and today are non-existent. This is an excellent example to use to observe the effect that real-world events have on the trends we observe here. In 2014, New York City mayor Bill de Blasio told the NYPD to stop arrests for marijuana possession and instead issue tickets in attempts to decriminalize marijuana (Dizard 2014). A drop in arrests is clearly seen around that time period. Around the same time, New York Governor Andrew Cuomo signed legislation which would allow the use of cannabis for medicinal purposes (Campbell 2014). Then, in 2021, recreational cannabis was legalized for adults over 21, up to a specific amount. Since then, the NYPD has not listed any crime related to marijuana, as evidenced by there being no arrests for possession on the graph past 2021."
  },
  {
    "objectID": "exploratory_data_analysis.html#acf-and-pacf",
    "href": "exploratory_data_analysis.html#acf-and-pacf",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The ACF values are decaying towards zero as lag increases, which is evidence against seasonality.\n\n\n\n\n\nSpikes can be seen at lags of 1, 2, 12, and 13."
  },
  {
    "objectID": "exploratory_data_analysis.html#augmented-dickey-fuller-test",
    "href": "exploratory_data_analysis.html#augmented-dickey-fuller-test",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Augmented Dickey-Fuller Test\n\ndata:  arrests_ts\nDickey-Fuller = -2.7629, Lag order = 5, p-value = 0.2564\nalternative hypothesis: stationary\n\n\nThe ADF tests returns a p-value of 0.26, meaning we do not have enough evidence to reject the null hypotheses. This series is not stationary."
  },
  {
    "objectID": "exploratory_data_analysis.html#detrend-and-difference",
    "href": "exploratory_data_analysis.html#detrend-and-difference",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "detrend &lt;- resid(lm(arrests_by_date$total ~ arrests_by_date$month, na.action = NULL))\ndifference &lt;- diff(arrests_ts)\n\n\ngrid.arrange(autoplot(arrests_ts),\n             autoplot(as.ts(detrend)), \n             autoplot(difference), \n             ncol = 1)\n\n\n\n\nThe detrended series (the middle plot) does not seem to be vastly different from the original series. This could be evidence that there is not much of a trend in the data. The differenced series looks to be stationary, which suggests that there may be a trend.\n\nggtsdisplay(difference)\n\n\n\n\nThis ACF has positive spikes at 12, 24, and 36, meaning once the trend is removed, the seasonality becomes evident. The ACF of the differenced time series provides strong evidence in support of seasonality."
  },
  {
    "objectID": "exploratory_data_analysis.html#lag-plots",
    "href": "exploratory_data_analysis.html#lag-plots",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Looking at the lag plot up to a lag of 12 months, we can see that the data are clearly not random, as a linear shape exists in each lag that we observe here. Seasonality is harder to observe here, as each month does not appear to be clustered alongside other observations of the same month. This could be due to the large number of points, however.\n\n\n\n\n\n\nIf we look up to a lag of 48 months, we can see that as the lag increases, the data get more random. The Lag 48 plot exhibiting less of a linear pattern than the lag 12 plot, which is fairly linear."
  },
  {
    "objectID": "arma_arima_sarima_models.html",
    "href": "arma_arima_sarima_models.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "All three difference plots look similar in their stationarity, so we will check \\(d=1,2,3\\). From the ACF and PACF plots, we determine that we will check \\(q=1,2,4,5\\) and \\(p=1,2,3,4\\).\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n3766.511\n3776.451\n3766.632\n\n\n1\n2\n1\n3765.516\n3775.440\n3765.637\n\n\n1\n3\n1\n3883.211\n3893.121\n3883.333\n\n\n1\n1\n2\n3764.168\n3777.420\n3764.370\n\n\n1\n2\n2\n3756.268\n3769.501\n3756.471\n\n\n1\n3\n2\n3761.431\n3774.644\n3761.635\n\n\n1\n1\n4\n3764.044\n3783.924\n3764.473\n\n\n1\n2\n4\n3755.507\n3775.357\n3755.938\n\n\n1\n3\n4\n3752.797\n3772.617\n3753.230\n\n\n1\n1\n5\n3765.763\n3788.956\n3766.338\n\n\n1\n2\n5\n3752.597\n3775.754\n3753.174\n\n\n2\n1\n1\n3764.254\n3777.506\n3764.456\n\n\n2\n2\n1\n3762.597\n3775.831\n3762.801\n\n\n2\n3\n1\n3834.959\n3848.173\n3835.163\n\n\n2\n1\n2\n3766.236\n3782.802\n3766.541\n\n\n2\n2\n2\n3762.872\n3779.413\n3763.178\n\n\n2\n3\n2\n3758.903\n3775.420\n3759.211\n\n\n2\n1\n4\n3765.657\n3788.849\n3766.231\n\n\n2\n2\n4\n3752.816\n3775.974\n3753.393\n\n\n2\n1\n5\n3725.809\n3752.314\n3726.551\n\n\n3\n1\n1\n3766.209\n3782.775\n3766.513\n\n\n3\n2\n1\n3764.153\n3780.694\n3764.459\n\n\n3\n3\n1\n3828.259\n3844.775\n3828.566\n\n\n3\n1\n2\n3765.955\n3785.834\n3766.384\n\n\n3\n2\n2\n3753.919\n3773.769\n3754.350\n\n\n3\n3\n2\n3760.547\n3780.367\n3760.980\n\n\n3\n1\n4\n3725.629\n3752.135\n3726.371\n\n\n4\n1\n1\n3764.753\n3784.632\n3765.182\n\n\n4\n2\n1\n3756.035\n3775.885\n3756.466\n\n\n4\n3\n1\n3805.134\n3824.954\n3805.567\n\n\n4\n1\n2\n3763.442\n3786.635\n3764.017\n\n\n4\n2\n2\n3757.657\n3780.815\n3758.235\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n27\n3\n1\n4\n3725.629\n3752.135\n3726.371\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n27\n3\n1\n4\n3725.629\n3752.135\n3726.371\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n27\n3\n1\n4\n3725.629\n3752.135\n3726.371\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=3, d=1, q=4\\). We now check the model diagnostics.\n\n\n\n\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1      ar2     ar3     ma1      ma2      ma3     ma4  constant\n      -1.2801  -0.2081  0.4542  0.8945  -0.4709  -0.7972  0.0207  -79.3632\ns.e.   0.2006   0.3481  0.2000  0.2089   0.2354   0.0754  0.1467   50.3485\n\nsigma^2 estimated as 4866953:  log likelihood = -1853.72,  aic = 3725.45\n\n$degrees_of_freedom\n[1] 195\n\n$ttable\n         Estimate      SE  t.value p.value\nar1       -1.2801  0.2006  -6.3823  0.0000\nar2       -0.2081  0.3481  -0.5977  0.5507\nar3        0.4542  0.2000   2.2715  0.0242\nma1        0.8945  0.2089   4.2829  0.0000\nma2       -0.4709  0.2354  -2.0005  0.0468\nma3       -0.7972  0.0754 -10.5709  0.0000\nma4        0.0207  0.1467   0.1411  0.8879\nconstant -79.3632 50.3485  -1.5763  0.1166\n\n$AIC\n[1] 18.35196\n\n$AICc\n[1] 18.35561\n\n$BIC\n[1] 18.49885\n\n\nThere is some concern about the correlation between residuals according to the p values for the Ljung-Box statistic, but we will continue evaluating the model regardless.\n\n\n\n\n\nSeries: arrests_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4337  -0.7444  -0.6934\ns.e.  0.1367   0.1018   0.0554\n\nsigma^2 = 3187995:  log likelihood = -1703.68\nAIC=3415.36   AICc=3415.57   BIC=3428.37\n\n\nThe information criteria are all better that the previously used model of ARIMA(3, 1, 4). This model is absolutely worth considering moving forward.\n\n\n\n\n\n\n\n\nBoth fitted models look similar to the actual time series.\n\n\n\n\n\n\n\n\n\n\n\nBoth models appear to be better than SNaïve as they do a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                  ME     RMSE     MAE       MPE     MAPE MASE     ACF1\nTraining set -948.75 3352.033 2586.25 -6.206225 13.38735    1 0.717891\n\n\nFitted model error measurements:\n\n\nSeries: arrests_ts \nARIMA(3,1,4) with drift \n\nCoefficients:\n          ar1      ar2     ar3     ma1      ma2      ma3     ma4     drift\n      -1.2801  -0.2081  0.4542  0.8945  -0.4709  -0.7972  0.0207  -79.3632\ns.e.   0.2006   0.3481  0.2000  0.2089   0.2354   0.0754  0.1467   50.3485\n\nsigma^2 = 5066628:  log likelihood = -1853.72\nAIC=3725.45   AICc=3726.38   BIC=3755.27\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 5.597487 2200.704 1595.247 -0.9085461 6.803609 0.6168187\n                     ACF1\nTraining set -0.002591217\n\n\nSeries: arrests_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4337  -0.7444  -0.6934\ns.e.  0.1367   0.1018   0.0554\n\nsigma^2 = 3187995:  log likelihood = -1703.68\nAIC=3415.36   AICc=3415.57   BIC=3428.37\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE    MASE       ACF1\nTraining set -121.183 1714.047 1202.141 -0.5315764 5.725607 0.46482 0.02141618\n\n\nThe model error measurements for both models are all much lower than the SNaïve benchmark method, so both models can work."
  },
  {
    "objectID": "arma_arima_sarima_models.html#third-difference",
    "href": "arma_arima_sarima_models.html#third-difference",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "All three difference plots look similar in their stationarity, so we will check \\(d=1,2,3\\). From the ACF and PACF plots, we determine that we will check \\(q=1,2,4,5\\) and \\(p=1,2,3,4\\).\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n3766.511\n3776.451\n3766.632\n\n\n1\n2\n1\n3765.516\n3775.440\n3765.637\n\n\n1\n3\n1\n3883.211\n3893.121\n3883.333\n\n\n1\n1\n2\n3764.168\n3777.420\n3764.370\n\n\n1\n2\n2\n3756.268\n3769.501\n3756.471\n\n\n1\n3\n2\n3761.431\n3774.644\n3761.635\n\n\n1\n1\n4\n3764.044\n3783.924\n3764.473\n\n\n1\n2\n4\n3755.507\n3775.357\n3755.938\n\n\n1\n3\n4\n3752.797\n3772.617\n3753.230\n\n\n1\n1\n5\n3765.763\n3788.956\n3766.338\n\n\n1\n2\n5\n3752.597\n3775.754\n3753.174\n\n\n2\n1\n1\n3764.254\n3777.506\n3764.456\n\n\n2\n2\n1\n3762.597\n3775.831\n3762.801\n\n\n2\n3\n1\n3834.959\n3848.173\n3835.163\n\n\n2\n1\n2\n3766.236\n3782.802\n3766.541\n\n\n2\n2\n2\n3762.872\n3779.413\n3763.178\n\n\n2\n3\n2\n3758.903\n3775.420\n3759.211\n\n\n2\n1\n4\n3765.657\n3788.849\n3766.231\n\n\n2\n2\n4\n3752.816\n3775.974\n3753.393\n\n\n2\n1\n5\n3725.809\n3752.314\n3726.551\n\n\n3\n1\n1\n3766.209\n3782.775\n3766.513\n\n\n3\n2\n1\n3764.153\n3780.694\n3764.459\n\n\n3\n3\n1\n3828.259\n3844.775\n3828.566\n\n\n3\n1\n2\n3765.955\n3785.834\n3766.384\n\n\n3\n2\n2\n3753.919\n3773.769\n3754.350\n\n\n3\n3\n2\n3760.547\n3780.367\n3760.980\n\n\n3\n1\n4\n3725.629\n3752.135\n3726.371\n\n\n4\n1\n1\n3764.753\n3784.632\n3765.182\n\n\n4\n2\n1\n3756.035\n3775.885\n3756.466\n\n\n4\n3\n1\n3805.134\n3824.954\n3805.567\n\n\n4\n1\n2\n3763.442\n3786.635\n3764.017\n\n\n4\n2\n2\n3757.657\n3780.815\n3758.235"
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-selection-and-diagnostics",
    "href": "arma_arima_sarima_models.html#model-selection-and-diagnostics",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "p\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n27\n3\n1\n4\n3725.629\n3752.135\n3726.371\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n27\n3\n1\n4\n3725.629\n3752.135\n3726.371\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n27\n3\n1\n4\n3725.629\n3752.135\n3726.371\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=3, d=1, q=4\\). We now check the model diagnostics.\n\n\n\n\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1      ar2     ar3     ma1      ma2      ma3     ma4  constant\n      -1.2801  -0.2081  0.4542  0.8945  -0.4709  -0.7972  0.0207  -79.3632\ns.e.   0.2006   0.3481  0.2000  0.2089   0.2354   0.0754  0.1467   50.3485\n\nsigma^2 estimated as 4866953:  log likelihood = -1853.72,  aic = 3725.45\n\n$degrees_of_freedom\n[1] 195\n\n$ttable\n         Estimate      SE  t.value p.value\nar1       -1.2801  0.2006  -6.3823  0.0000\nar2       -0.2081  0.3481  -0.5977  0.5507\nar3        0.4542  0.2000   2.2715  0.0242\nma1        0.8945  0.2089   4.2829  0.0000\nma2       -0.4709  0.2354  -2.0005  0.0468\nma3       -0.7972  0.0754 -10.5709  0.0000\nma4        0.0207  0.1467   0.1411  0.8879\nconstant -79.3632 50.3485  -1.5763  0.1166\n\n$AIC\n[1] 18.35196\n\n$AICc\n[1] 18.35561\n\n$BIC\n[1] 18.49885\n\n\nThere is some concern about the correlation between residuals according to the p values for the Ljung-Box statistic, but we will continue evaluating the model regardless.\n\n\n\n\n\nSeries: arrests_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4337  -0.7444  -0.6934\ns.e.  0.1367   0.1018   0.0554\n\nsigma^2 = 3187995:  log likelihood = -1703.68\nAIC=3415.36   AICc=3415.57   BIC=3428.37\n\n\nThe information criteria are all better that the previously used model of ARIMA(3, 1, 4). This model is absolutely worth considering moving forward.\n\n\n\n\n\n\n\n\nBoth fitted models look similar to the actual time series."
  },
  {
    "objectID": "arma_arima_sarima_models.html#forecasting",
    "href": "arma_arima_sarima_models.html#forecasting",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Both models appear to be better than SNaïve as they do a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                  ME     RMSE     MAE       MPE     MAPE MASE     ACF1\nTraining set -948.75 3352.033 2586.25 -6.206225 13.38735    1 0.717891\n\n\nFitted model error measurements:\n\n\nSeries: arrests_ts \nARIMA(3,1,4) with drift \n\nCoefficients:\n          ar1      ar2     ar3     ma1      ma2      ma3     ma4     drift\n      -1.2801  -0.2081  0.4542  0.8945  -0.4709  -0.7972  0.0207  -79.3632\ns.e.   0.2006   0.3481  0.2000  0.2089   0.2354   0.0754  0.1467   50.3485\n\nsigma^2 = 5066628:  log likelihood = -1853.72\nAIC=3725.45   AICc=3726.38   BIC=3755.27\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 5.597487 2200.704 1595.247 -0.9085461 6.803609 0.6168187\n                     ACF1\nTraining set -0.002591217\n\n\nSeries: arrests_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4337  -0.7444  -0.6934\ns.e.  0.1367   0.1018   0.0554\n\nsigma^2 = 3187995:  log likelihood = -1703.68\nAIC=3415.36   AICc=3415.57   BIC=3428.37\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE     MAPE    MASE       ACF1\nTraining set -121.183 1714.047 1202.141 -0.5315764 5.725607 0.46482 0.02141618\n\n\nThe model error measurements for both models are all much lower than the SNaïve benchmark method, so both models can work."
  },
  {
    "objectID": "exploratory_data_analysis.html#by-crime",
    "href": "exploratory_data_analysis.html#by-crime",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "TotalAssaultControlled Substance PossessionControlled Substance SaleLarcenyMarijuana PossessionMarijuana SaleMotor Vehicle TheftMurderRobbery\n\n\n\n\n\n\n\n\nThere appears to be some seasonality in the plot, as there are noticeable dips in arrest rate towards the end of each year. There was a slight upward trend towards the end of the 2000’s but that changed dramatically during the 2010’s into a clear downward trend. Since the start of the current decade the trend has reversed course, however. It is hard to tell if the time series is multiplicative or additive, as it seems to hold different characteristics at different points, though this possibly suggests a multiplicative time series. The change in trend could also be part of a longer cycle, though it is impossible to tell from this graph as it only goes back to 2006.\n\n\n\n\n\n\n\n\nLooking at the lag plot up to a lag of 12 months, we can see that the data are clearly not random, as a linear shape exists in each lag that we observe here. Seasonality is harder to observe here, as each month does not appear to be clustered alongside other observations of the same month. This could be due to the large number of points, however.\n\n\n\n\n\n\nIf we look up to a lag of 48 months, we can see that as the lag increases, the data get more random. The Lag 48 plot exhibiting less of a linear pattern than the lag 12 plot, which is fairly linear.\n\n\n\n\n\n\n\n\nWe can see the trend here that we were able to observe on the initial graph. There is horizontal movement during the first half of the observed period and then a clear downward trend until 2020. After that, it reverses and increases until the present day.\nRemoving the trend allows us to see some seasonality. Once the seasonality has been averaged out as it has been here, the clear dips in arrests towards the end of each year remain and there is a clear pattern between the large dips at the end of each month.\nThere seems to be more noise in the more recent data, which could be something interesting to look into.\n\n\n\n\n\n\n\n\nThe ACF values are decaying towards zero as lag increases, which is evidence against seasonality.\n\n\n\n\n\nSpikes can be seen at lags of 1, 2, 12, and 13.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  arrests_ts\nDickey-Fuller = -2.7629, Lag order = 5, p-value = 0.2564\nalternative hypothesis: stationary\n\n\nThe ADF tests returns a p-value of 0.26, meaning we do not have enough evidence to reject the null hypotheses. This series is not stationary.\n\n\n\n\ndetrend &lt;- resid(lm(arrests_by_date$total ~ arrests_by_date$month, na.action = NULL))\ndifference &lt;- diff(arrests_ts)\n\n\ngrid.arrange(autoplot(arrests_ts),\n             autoplot(as.ts(detrend)), \n             autoplot(difference), \n             ncol = 1)\n\n\n\n\nThe detrended series (the middle plot) does not seem to be vastly different from the original series. This could be evidence that there is not much of a trend in the data. The differenced series looks to be stationary, which suggests that there may be a trend.\n\nggtsdisplay(difference)\n\n\n\n\nThis ACF has positive spikes at 12, 24, and 36, meaning once the trend is removed, the seasonality becomes evident. The ACF of the differenced time series provides strong evidence in support of seasonality.\n\n\n\n\n\n\n\n\n\ninsert text here\n\n\n\n\n\n\n\n\ninsert text\n\n\n\n\n\n\ninsert text"
  }
]