[
  {
    "objectID": "data_sources.html",
    "href": "data_sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "The data needed for this project can be divided as such:\nflowchart TD\nraw(Raw Crime Data)\ngovernment(Government Programs) \neconomy(Economic Data)"
  },
  {
    "objectID": "data_sources.html#general-economic-statistics",
    "href": "data_sources.html#general-economic-statistics",
    "title": "Data Sources",
    "section": "General Economic Statistics",
    "text": "General Economic Statistics\nEconomic data can be accessed in similar ways as crime data, as the data needed for this project is maintained by the government. For example, comparing unemployment rate against crime rate could be a useful comparison. This unemployment data would be obtained from FRED. An example with data from here can be seen below."
  },
  {
    "objectID": "data_sources.html#specific-asset-data",
    "href": "data_sources.html#specific-asset-data",
    "title": "Data Sources",
    "section": "Specific Asset Data",
    "text": "Specific Asset Data\n\nWhen fitting financial time series models, certain stocks can give important information about changes in crime rates due to their relation to crimes. For example, CoreCivic , a company that owns and operates private prisons across the United States, would be a worthwhile stock to look at to attempt to better understand crime rates. This data will be obtained through the quantmod package in R."
  },
  {
    "objectID": "interrupted_series.html",
    "href": "interrupted_series.html",
    "title": "Interrupted Time Series",
    "section": "",
    "text": "Imports\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(zoo)\nlibrary(kableExtra)\nlibrary(tseries)\nlibrary(forecast)\nlibrary(TSstudio)\nlibrary(astsa)\nload(\"data/arrests_ts.Rdata\")"
  },
  {
    "objectID": "interrupted_series.html#data-visualization",
    "href": "interrupted_series.html#data-visualization",
    "title": "Interrupted Time Series",
    "section": "Data Visualization",
    "text": "Data Visualization\nFirst, lets look again at marijuana possession arrests over time, this time with policy interventions labelled on the graph.\n\n\nCode\nplot(marijuana_pos_ts,\n    bty=\"n\", pch=19, col=\"gray\",\n    xlim = c(2005,2023),\n    xlab = \"Year\",\n    ylab = \"\")\nabline(v = 2014.917, col = \"firebrick\", lty = 2)\ntext(2015, 1.5, \"Possession Fines Begin\\n(Nov. 2014)\", col=\"firebrick\", cex=1, pos=4 )\ntitle(\"Marijuana Possession Arrests\")\n\n\n\n\n\nThere definitely appears to be a difference in marijuana possession arrests before and after the intervention, which occured in November 2014 when New York City Mayor Bill De Blasio announced that tickets would be given to those possessing marijuana, rather than arresting offenders (Dizard 2014). We should proceed with our interrupted time series analysis, modeling the events before the interruption and seeing how they would have looked had the change never occurred."
  },
  {
    "objectID": "interrupted_series.html#observing-treatment",
    "href": "interrupted_series.html#observing-treatment",
    "title": "Interrupted Time Series",
    "section": "Observing Treatment",
    "text": "Observing Treatment\nHere we can see exactly when the treatment was applied, whether or not each point occured when the treatment was active, and the time since the treatment was applied for those variables that were under the influence of that treatment. This is important in determining if the effects of the treatment are short or long term effects.\n\n\nCode\ndf &lt;- data.frame(marijuana_pos_ts) %&gt;%\n    rename(Y = Series.1) %&gt;%\n    mutate(\"T\" = row_number())\n\ndf &lt;- df %&gt;% \n  mutate(\"T\" = row_number(),\n         \"D\" = if_else(df$\"T\" &gt; 107, 1, 0),\n         \"P\" = if_else(df$\"T\" &gt; 107, row_number() - 107, 0))\n\ndf$Y &lt;- round(df$Y, 3)\n\ndf.temp &lt;- rbind(head(df, 3),\n                 c(\"...\",\"...\",\"...\",\"...\"),\n                 df[105:107,],\n                 c(\"Start\",\"Treatment\",\"-\",\"-\"),\n                 df[108:110,],\n                 c(\"...\",\"...\",\"...\",\"...\"),\n                 tail(df, 3))\n\nrow.names(df.temp) &lt;- NULL\nkbl(df.temp) %&gt;%\n    kable_paper(full_width = F)\n\n\n\n\n\n\n\nY\nT\nD\nP\n\n\n\n\n0.276\n1\n0\n0\n\n\n0.22\n2\n0\n0\n\n\n0.442\n3\n0\n0\n\n\n...\n...\n...\n...\n\n\n0.118\n105\n0\n0\n\n\n0.224\n106\n0\n0\n\n\n-0.502\n107\n0\n0\n\n\nStart\nTreatment\n-\n-\n\n\n-1.04\n108\n1\n1\n\n\n-0.852\n109\n1\n2\n\n\n-0.685\n110\n1\n3\n\n\n...\n...\n...\n...\n\n\n-1.353\n202\n1\n95\n\n\n-1.353\n203\n1\n96\n\n\n-1.353\n204\n1\n97"
  },
  {
    "objectID": "interrupted_series.html#fitting-y-and-counterfactuals",
    "href": "interrupted_series.html#fitting-y-and-counterfactuals",
    "title": "Interrupted Time Series",
    "section": "Fitting Y and Counterfactuals",
    "text": "Fitting Y and Counterfactuals\n\n\nCode\nregTS &lt;- lm(Y ~ T + D + P, data = df)\npred1 &lt;- predict(regTS, df)\n\ndf2 &lt;- as.data.frame(cbind(T = rep(1 : 365), D = rep(0), P = rep(0)))\npred2 &lt;- predict(regTS, df2)\n\nplot(df$T, df$Y,\n    bty=\"n\", pch=19, col=\"gray\",\n    xlim = c(0,204),\n    xlab = \"Time\",\n    ylab = \"\")\n\nlines(rep(1:107), pred1[1:107], col = \"dodgerblue4\", lwd = 3)\nlines(rep(108:204), pred1[108:204], col = \"dodgerblue4\", lwd = 3)\nlines(rep(108:204), pred2[108:204], col = \"darkorange2\", lwd = 3, lty = 5)\n\n# interruption line\nabline(v = 107, col = \"firebrick\", lty = 2)\ntext(108, 1.5, \"Possession Fines Begin\\n(Nov. 2014)\", col=\"firebrick\", cex=1, pos=4 )\n\ntitle(\"Marijuana Possession Arrests\")\n\n\n\n\n\nWe can confidently say that intervention resulted in a change in marijuana possession arrests. They decreasing before, but the change resulted in a short and long term decrease in arrests."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Crime as a Time Series",
    "section": "",
    "text": "Crime\nTime-series data for crime throughout the United States is readily available and the variety of attempts the government has made over the years makes looking at how crime has changed over those years an interesting task.\n\n\nBig Picture\nDetermining which attempts by the government at city, state, and national levels have been successful and which attempts have been unsuccessful is an incredibly important task. Doing this will help the government to make better decisions to not only keep people safe, but also put their citizens in the best position to life prosperous lives, arguably the most important role the government can play.\nUnderstanding the patterns in crimes committed can help keep citizens safe, but it can also help potential criminals find a better path in life. Knowing when we may be due for an increase in certain types of crimes can help us to take proactive, rather than reactive measures to those potential increases. A detailed understanding of crime patters is arguably one of the the most important things a government cam do in keeping its citizens safe and prosperous.\n\n\nLiterature Review\nThere have been countless attempts to understand crime over the years, one interesting one being found in the Handbook of Labor Economics titled “The economics of crime” by Richard B. Freeman. He takes an interesting look at crime through an economic lens, describing this attempt as focusing on “…the effect of incentives on criminal behavior, the way decisions interact in a market setting; and the use of a benefit-cost framework to assess alternative strategies to reduce crime (Freeman 1999). This is a useful approach when looking at time-series data as economic data is possibly the most widely used application of such data.\nThis approach can also be seen when analyzing crime alongside the economy, as this can reveal many motivating factors for committing crime. A 2002 paper demonstrated how the declining labor market in the 1980’s, for example, coincided with increasing crime rates among young men during the same time period. The opposite trend was observed in the following decade (Gould, Weinberg, and Mustard 2002). Papers like these help to bridge the gap between the economy and crime and demonstrate how multi-disciplinary the subject truly is.\n\n\nAnalytical Angles\n\n\n\n\nflowchart TD\n\ndata(Raw Crime Data) --&gt; crime(Model Crime)\ndata --&gt; indicators(Model Economic Indicators)\ndata --&gt; stocks(Model Stocks)\n\ncrime --&gt; intervention(Intervention Analysis)\nindicators --&gt; intervention\nstocks --&gt; intervention\n\n\n\n\n\n\nThe overall process can be seen in the flowchart above and can be understood as follows:\n\nRaw crime data will be obtained\nVarious models including those using just the raw crime data, models with economic indicators as exogenous variables, and financial models with related assets will all be fit.\nThe best and most accurate models from those will be used in an interrupted time series analysis using various events as the interventions to compare the effect those events had on crime rate to determine which events were the most and least successful at impacting those rates.\n\n\n\nGuiding Questions\n\nHow can we use the economy as a means by which to analyze crime data?\nWhat are some metrics we can use to determine the effectiveness of government action?\nWhat are good predictors of crime?\nIs it more effective to try and stop crimes before they are commited or to help those who commit crimes after they do so?\nWhat ethical concerns do we need to be aware of if we are attempting to predict what crimes will be committed?\nIs action needed the most at the city, state, or federal level?\nDoes the economy influence crime, does crime influence the economy, or is it a mix of both?\nIs it worth looking at other countries to see what works and what doesn’t? Or are all countries more or less the same in their relationship with crime?\nWhich area of crime would be the most effective to attempt reform in?\nCould we quantify the negative effect different crimes have on society?\n\n\n\n\n\n\n\n\n\nReferences\n\nFreeman, Richard B. 1999. “Chapter 52 The Economics of Crime.” In Handbook of Labor Economics, 3:3529–71. Elsevier. https://doi.org/10.1016/S1573-4463(99)30043-2.\n\n\nGould, Eric D., Bruce A. Weinberg, and David B. Mustard. 2002. “Crime Rates and Local Labor Market Opportunities in the United States: 1979–1997.” Review of Economics and Statistics 84 (1): 45–61. https://doi.org/10.1162/003465302317331919."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "df &lt;- read_csv(\"data/unem.csv\")\n\nRows: 405 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (1): NEWY636URN\ndate (1): DATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndf &lt;- df %&gt;%\n  rename(month = DATE,\n         unrate = NEWY636URN) %&gt;%\n  filter(month &gt; as.Date(\"2005-12-02\"),\n         month &lt; as.Date(\"2023-01-01\"))\n\n\nsave(df, file = \"data/unrate.Rdata\")"
  },
  {
    "objectID": "arma_arima_sarima_models.html",
    "href": "arma_arima_sarima_models.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "TotalAssaultControlled Substance PossessionMarijuana PossessionMotor Vehicle TheftMurderRobbery\n\n\nWe are fitting a SARIMA model as we saw a clear seasonal component in the previous section. We need to determine the following parameters:\n\np: The order of AR terms.\nd: The degree of differencing needed to make the data stationary.\nq: The order of MA terms.\nP: The seasonal order of AR terms.\nD: The seasonal degree of differencing.\nQ: The seasonal order of MA terms.\ns: The seasonal period.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe series looks to be stationary at this level, though we should try a seasonal differencing. The seasonal period s is clearly 12. There are spikes at 1 and 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot.\n\n\n\n\n\n\n\n\nThis looks less stationary than a simple first differencing, we will try D = 0 and 1.\nWe will try the following parameters:\n\np: 1, 2, 4\nd: 0, 1\nq: 1, 4\nP: 1\nD: 0, 1\nQ: 1, 2, 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-32.55671\n-15.99068\n-32.25214\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-32.55671\n-15.99068\n-32.25214\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-32.55671\n-15.99068\n-32.25214\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=1\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals, meaning we have a good model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.\n\n\n\n\n\nSeries: arrests_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4338  -0.7443  -0.6934\ns.e.  0.1368   0.1019   0.0554\n\nsigma^2 = 0.04578:  log likelihood = 20.95\nAIC=-33.89   AICc=-33.68   BIC=-20.88\n\n\nThe information criteria are all better that the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12. This model is absolutely worth considering moving forward.\n\n\n\n\n\n\n\n\nBoth fitted models look similar to the actual time series. The two fitted models look incredibly similar, so we will just go with the one we fit by hand.\n\n\n\n\n\n\n\n\n\nBoth models appear to be better than SNaïve as they do a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                     ME     RMSE      MAE       MPE     MAPE MASE     ACF1\nTraining set -0.1136878 0.401671 0.309908 -22.97998 217.0115    1 0.717891\n\n\nFitted model error measurements:\n\n\nSeries: arrests_ts \nARIMA(1,1,1)(1,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sar1     sma1\n      0.4358  -0.7447  -0.0331  -0.6763\ns.e.  0.1362   0.1012   0.1009   0.0774\n\nsigma^2 = 0.04599:  log likelihood = 21\nAIC=-32   AICc=-31.67   BIC=-15.74\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE    MAPE    MASE\nTraining set -0.01410203 0.2053272 0.1439894 39.06935 74.2216 0.46462\n                   ACF1\nTraining set 0.02119928\n\n\nSeries: arrests_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4338  -0.7443  -0.6934\ns.e.  0.1368   0.1019   0.0554\n\nsigma^2 = 0.04578:  log likelihood = 20.95\nAIC=-33.89   AICc=-33.68   BIC=-20.88\n\nTraining set error measures:\n                    ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.014476 0.2053915 0.1439828 37.82999 73.83899 0.4645986\n                   ACF1\nTraining set 0.02092654\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method.\n\n\n\n\n\nFit a normalized model for later comparison.\n\n\nSeries: scale(arrests_ts) \nARIMA(1,1,1)(1,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sar1     sma1\n      0.4358  -0.7447  -0.0331  -0.6763\ns.e.  0.1362   0.1012   0.1009   0.0774\n\nsigma^2 = 0.04599:  log likelihood = 21\nAIC=-32   AICc=-31.67   BIC=-15.74\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE    MAPE    MASE\nTraining set -0.01410203 0.2053272 0.1439894 39.06935 74.2216 0.46462\n                   ACF1\nTraining set 0.02119928\n\n\n\n\n\nOur final model equation is as follows:\nWe have a SARIMA(1,1,1)(1,1,1)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12}\\)\nordinary difference = \\((1-B)\\)\nseasonal difference = \\((1-B^{12})\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)(1-B^{12})x_t = (1 + \\theta_1B)(1 + \\Theta_1B^{12})w_t\\)\nWith Coefficients:\n\\((1 - 0.435B)(1 - (-0.0331)B^{12})(1-B)(1-B^{12})x_t = (1 + (-0.7447)B)(1 + (-0.6763)B^{12})w_t\\)\n\n\n\nWe are fitting a SARIMA model as we saw a clear seasonal component in the previous section. We need to determine the following parameters:\n\np: The order of AR terms.\nd: The degree of differencing needed to make the data stationary.\nq: The order of MA terms.\nP: The seasonal order of AR terms.\nD: The seasonal degree of differencing.\nQ: The seasonal order of MA terms.\ns: The seasonal period.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe series looks to be stationary at this level, though we should try a seasonal differencing. The seasonal period s is clearly 12. There are spikes at 1 and 2 on the ACF plot and spikes at 1 and 2 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot.\n\n\n\n\n\n\n\n\nThis looks less stationary than a simple first differencing, we will try D = 0 and 1.\nWe will try the following parameters:\n\np: 1, 2\nd: 1\nq: 1, 2\nP: 1\nD: 0, 1\nQ: 1, 2, 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n1\n1\n1\n215.2202\n231.4816\n215.5446\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n1\n1\n1\n215.2202\n231.4816\n215.5446\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n1\n1\n1\n215.2202\n231.4816\n215.5446\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=1\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals at some lag values, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.\n\n\n\n\n\nSeries: assault_ts \nARIMA(1,0,1)(0,1,1)[12] with drift \n\nCoefficients:\n         ar1      ma1     sma1    drift\n      0.9355  -0.3413  -0.8120  -0.0047\ns.e.  0.0321   0.0744   0.0588   0.0060\n\nsigma^2 = 0.1589:  log likelihood = -100.52\nAIC=211.05   AICc=211.37   BIC=227.34\n\n\nThe information criteria are all worse than the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12. This model is not worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model looks similar to the actual time series.\n\n\n\n\n\n\n\n\n\nThis model appears to be better than SNaïve as it does a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                      ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set -0.01932497 0.7578097 0.5323319 155.2004 422.9095    1 0.7024543\n\n\nFitted model error measurements:\n\n\nSeries: assault_ts \nARIMA(1,1,1)(1,1,1)[12] \n\nCoefficients:\n          ar1      ma1     sar1     sma1\n      -0.0638  -0.3161  -0.0817  -0.7733\ns.e.   0.1959   0.1884   0.0962   0.0745\n\nsigma^2 = 0.1639:  log likelihood = -102.61\nAIC=215.22   AICc=215.54   BIC=231.48\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01193998 0.3876284 0.2720116 -81.78453 154.0811 0.5109812\n                     ACF1\nTraining set -0.001187566\n\n\nSeries: assault_ts \nARIMA(1,0,1)(0,1,1)[12] with drift \n\nCoefficients:\n         ar1      ma1     sma1    drift\n      0.9355  -0.3413  -0.8120  -0.0047\ns.e.  0.0321   0.0744   0.0588   0.0060\n\nsigma^2 = 0.1589:  log likelihood = -100.52\nAIC=211.05   AICc=211.37   BIC=227.34\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.007988694 0.3826276 0.2657987 -42.81001 113.7842 0.4993101\n                    ACF1\nTraining set -0.01329669\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method.\n\n\n\n\n\n\n\n\nOur final model equation is as follows:\nWe have a SARIMA(1,1,1)(1,1,1)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12}\\)\nordinary difference = \\((1-B)\\)\nseasonal difference = \\((1-B^{12})\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)(1-B^{12})x_t = (1 + \\theta_1B)(1 + \\Theta_1B^{12})w_t\\)\nWith Coefficients:\n\\((1 - (-0.0638)B)(1 - (-0.0817)B^{12})(1-B)(1-B^{12})x_t = (1 + (-0.3161)B)(1 + (-0.7733)B^{12})w_t\\)\n\n\n\nWe are fitting a SARIMA model as we saw a clear seasonal component in the previous section. We need to determine the following parameters:\n\np: The order of AR terms.\nd: The degree of differencing needed to make the data stationary.\nq: The order of MA terms.\nP: The seasonal order of AR terms.\nD: The seasonal degree of differencing.\nQ: The seasonal order of MA terms.\ns: The seasonal period.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe series looks to be stationary at this level, though we should try a seasonal differencing. The seasonal period s is clearly 12. There are spikes at 1, 3, 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot.\n\n\n\n\n\n\n\n\nThis looks less stationary than a simple first differencing, we will try D = 0 and 1.\nWe will try the following parameters:\n\np: 1, 2, 4\nd: 1\nq: 1, 3, 4\nP: 1\nD: 0, 1\nQ: 1, 2, 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n13\n2\n1\n1\n1\n0\n1\n-31.9426\n-12.06336\n-31.51402\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n13\n2\n1\n1\n1\n0\n1\n-31.9426\n-12.06336\n-31.51402\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n13\n2\n1\n1\n1\n0\n1\n-31.9426\n-12.06336\n-31.51402\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=1\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals at some lag values, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.\n\n\n\n\n\nSeries: controlled_pos_ts \nARIMA(2,1,1)(2,0,0)[12] with drift \n\nCoefficients:\n          ar1      ar2      ma1    sar1    sar2    drift\n      -0.1522  -0.2874  -0.4061  0.3955  0.2736  -0.0015\ns.e.   0.1430   0.0932   0.1465  0.0679  0.0732   0.0168\n\nsigma^2 = 0.0494:  log likelihood = 16.96\nAIC=-19.92   AICc=-19.35   BIC=3.27\n\n\nThe information criteria are all worse than the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12. This model is not worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model looks similar to the actual time series.\n\n\n\n\n\n\n\n\n\nThis model appears to be better than SNaïve as it does a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                     ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set -0.1591141 0.3603143 0.2850547 57.74682 104.1726    1 0.4911806\n\n\nFitted model error measurements:\n\n\nSeries: controlled_pos_ts \nARIMA(1,1,1)(1,1,1)[12] \n\nCoefficients:\n         ar1      ma1    sar1     sma1\n      0.1299  -0.6667  0.0023  -0.7269\ns.e.  0.1172   0.0879  0.1046   0.0801\n\nsigma^2 = 0.04845:  log likelihood = 15.4\nAIC=-20.79   AICc=-20.47   BIC=-4.53\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.02399794 0.2107427 0.1484459 9.835336 42.68932 0.5207628\n                   ACF1\nTraining set 0.02970125\n\n\nSeries: controlled_pos_ts \nARIMA(2,1,1)(2,0,0)[12] with drift \n\nCoefficients:\n          ar1      ar2      ma1    sar1    sar2    drift\n      -0.1522  -0.2874  -0.4061  0.3955  0.2736  -0.0015\ns.e.   0.1430   0.0932   0.1465  0.0679  0.0732   0.0168\n\nsigma^2 = 0.0494:  log likelihood = 16.96\nAIC=-19.92   AICc=-19.35   BIC=3.27\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01010566 0.218415 0.1523486 -16.98116 61.98025 0.5344541\n                    ACF1\nTraining set -0.02147658\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method.\n\n\n\n\n\n\n\n\nOur final model equation is as follows:\nWe have a SARIMA(1,1,1)(1,1,1)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12}\\)\nordinary difference = \\((1-B)\\)\nseasonal difference = \\((1-B^{12})\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)(1-B^{12})x_t = (1 + \\theta_1B)(1 + \\Theta_1B^{12})w_t\\)\nWith Coefficients:\n\\((1 - (0.1299)B)(1 - (0.0023)B^{12})(1-B)(1-B^{12})x_t = (1 + (-0.6667)B)(1 + (-0.7269)B^{12})w_t\\)\n\n\n\nWe are fitting a SARIMA model as we saw a clear seasonal component in the previous section. We need to determine the following parameters:\n\np: The order of AR terms.\nd: The degree of differencing needed to make the data stationary.\nq: The order of MA terms.\nP: The seasonal order of AR terms.\nD: The seasonal degree of differencing.\nQ: The seasonal order of MA terms.\ns: The seasonal period.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere still looks to be a trend at this level, we should try to difference one more time. We should also try a seasonal differencing at a seasonal period of 12.\n\n\n\n\n\n\n\n\nThis looks more stationary, we should determine parameters to try from here.\nThe seasonal period s is clearly 12. There are spikes at 1 and 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot.\n\n\n\n\n\n\n\n\nThis looks less stationary than a simple first differencing, we will try D = 0 and 1.\nWe will try the following parameters:\n\np: 1, 2, 4\nd: 1, 2\nq: 1, 4\nP: 1\nD: 0, 1\nQ: 1, 2, 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-131.9417\n-115.3757\n-131.6371\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-131.9417\n-115.3757\n-131.6371\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-131.9417\n-115.3757\n-131.6371\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=1\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals at some lag values, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.\n\n\n\n\n\nSeries: marijuana_pos_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.2557  -0.5081  -0.6226\ns.e.  0.2258   0.2001   0.0552\n\nsigma^2 = 0.02916:  log likelihood = 65.08\nAIC=-122.16   AICc=-121.95   BIC=-109.15\n\n\nThe information criteria are all slightly better than the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12, but not so much better that this model would be worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model looks similar to the actual time series.\n\n\n\n\n\n\n\n\n\nThis model appears to be better than SNaïve as it does a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                     ME      RMSE       MAE      MPE     MAPE MASE     ACF1\nTraining set -0.1123487 0.3996005 0.2874182 1.037198 61.19043    1 0.832498\n\n\nFitted model error measurements:\n\n\nSeries: marijuana_pos_ts \nARIMA(1,1,1)(1,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sar1     sma1\n      0.2848  -0.5273  -0.0775  -0.5769\ns.e.  0.2306   0.2018   0.1100   0.0897\n\nsigma^2 = 0.02923:  log likelihood = 65.33\nAIC=-120.65   AICc=-120.33   BIC=-104.39\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01026806 0.1636941 0.1107387 0.6302746 23.70606 0.3852878\n                    ACF1\nTraining set 0.003563427\n\n\nSeries: marijuana_pos_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.2557  -0.5081  -0.6226\ns.e.  0.2258   0.2001   0.0552\n\nsigma^2 = 0.02916:  log likelihood = 65.08\nAIC=-122.16   AICc=-121.95   BIC=-109.15\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.01068143 0.1639416 0.1115361 1.036487 23.75623 0.3880621\n                    ACF1\nTraining set 0.002802718\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method.\n\n\n\n\n\nIt doesn’t really make sense to forecast here as we can be sure that there will be no more arrests for marijuana possession, but it is good to know that we have fitted a good model. We can predict what would have happened to the level of arrests if the policy changes that were instituded over the years never happened.\n\n\n\nOur final model equation is as follows:\nWe have a SARIMA(1,1,1)(1,1,1)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12}\\)\nordinary difference = \\((1-B)\\)\nseasonal difference = \\((1-B^{12})\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)(1-B^{12})x_t = (1 + \\theta_1B)(1 + \\Theta_1B^{12})w_t\\)\nWith Coefficients:\n\\((1 - (0.2848)B)(1 - (-0.0775)B^{12})(1-B)(1-B^{12})x_t = (1 + (-0.5273)B)(1 + (-0.5769)B^{12})w_t\\)\n\n\n\nWe are fitting an ARIMA model as we did not see a clear seasonal component in the previous section. We need to determine the following parameters:\n\np: The order of AR terms.\nd: The degree of differencing needed to make the data stationary.\nq: The order of MA terms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis looks to be slightly more stationary than the original series, we can proceed with the model build process. There are spikes at 1 and 3 on the ACF plot and spikes at 1, 2 on the PACF plot.\nWe will try the following parameters:\n\np: 1, 2\nd: 0, 1\nq: 1, 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n8\n2\n1\n3\n531.7265\n551.6058\n532.1551\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n8\n2\n1\n3\n531.7265\n551.6058\n532.1551\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n8\n2\n1\n3\n531.7265\n551.6058\n532.1551\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=2, d=1, q=3\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there may be correlation between residuals at some lag values, meaning we might not have a good enough model. We will still proceed with the model ARIMA(2, 1, 3).\n\n\n\n\n\nSeries: motor_theft_ts \nARIMA(1,1,2)(1,0,1)[12] \n\nCoefficients:\n         ar1      ma1     ma2    sar1     sma1\n      0.6836  -1.3614  0.3775  0.5699  -0.2578\ns.e.  0.1505   0.1767  0.1618  0.1867   0.2183\n\nsigma^2 = 0.7354:  log likelihood = -255.8\nAIC=523.6   AICc=524.03   BIC=543.48\n\n\nThe information criteria are all slightly better than the previously used model of ARIMA(2, 1, 3), and it considers the seasonal component, so this model would be worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model and auto mdoel look similar to the actual time series.\n\n\n\n\n\n\n\n\n\nBoth models appear to be better than SNaïve as they does a better job of capturing the trend. It is hard to tell which is better between the fit and the auto fit.\nSNaïve model error measurements:\n\n\n                     ME     RMSE       MAE     MPE     MAPE MASE      ACF1\nTraining set 0.01534954 1.102843 0.8651354 78.4586 280.3894    1 0.3607956\n\n\nFitted model error measurements:\n\n\nSeries: motor_theft_ts \nARIMA(2,1,3) \n\nCoefficients:\n         ar1      ar2      ma1     ma2      ma3\n      1.3083  -0.8110  -2.1161  1.9923  -0.8229\ns.e.  0.0575   0.1745   0.1212  0.3464   0.2057\n\nsigma^2 = 0.7673:  log likelihood = -259.86\nAIC=531.73   AICc=532.16   BIC=551.61\n\nTraining set error measures:\n                      ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set 0.008238411 0.8629927 0.673068 88.31137 136.4341 0.7779915\n                  ACF1\nTraining set 0.0891247\n\n\nSeries: motor_theft_ts \nARIMA(1,1,2)(1,0,1)[12] \n\nCoefficients:\n         ar1      ma1     ma2    sar1     sma1\n      0.6836  -1.3614  0.3775  0.5699  -0.2578\ns.e.  0.1505   0.1767  0.1618  0.1867   0.2183\n\nsigma^2 = 0.7354:  log likelihood = -255.8\nAIC=523.6   AICc=524.03   BIC=543.48\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.001864579 0.8448407 0.6522227 69.26282 148.5133 0.7538966\n                    ACF1\nTraining set 0.007715526\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method. The seasonal, auto fitted model slightly outperforms the non-seasonal ARIMA model, so we will go with that model.\n\n\n\n\n\n\n\n\nOur final model equation is as follows:\nWe have a SARIMA(1,1,2)(1,0,2)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B + \\theta_2B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12}\\)\nordinary difference = \\((1-B)\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)x_t = (1 + \\theta_1B + \\theta_2B)(1 + \\Theta_1B^{12})w_t\\)\nWith Coefficients:\n\\((1 - (0.6836)B)(1 - (0.5699)B^{12})(1-B)x_t = (1 + (-1.3614)B + (0.3775)2B)(1 + (-0.2578)B^{12})w_t\\)\n\n\n\nWe are fitting an ARIMA model as we did not see a clear seasonal component in the previous section. We need to determine the following parameters:\n\np: The order of AR terms.\nd: The degree of differencing needed to make the data stationary.\nq: The order of MA terms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis looks to be much more stationary than the original series, we can proceed with the model build process. There are spikes at 1 on the ACF plot and spikes at 1 and 2 on the PACF plot.\nWe will try the following parameters:\n\np: 1\nd: 0, 1\nq: 1, 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n4\n1\n1\n2\n432.8634\n446.1162\n433.0654\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n433.3898\n443.3294\n433.5104\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n4\n1\n1\n2\n432.8634\n446.1162\n433.0654\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=2\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there may be correlation between residuals at some lag values, meaning we might not have a good enough model. We will still proceed with the model ARIMA(1, 1, 2).\n\n\n\n\n\nSeries: murder_ts \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.7223\ns.e.   0.0674\n\nsigma^2 = 0.4858:  log likelihood = -214.64\nAIC=433.27   AICc=433.33   BIC=439.9\n\n\nThe information criteria are all slightly better than the previously used model of ARIMA(1, 1, 2), but not so much better that this model would be worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model and auto mdoel look similar to the actual time series.\n\n\n\n\n\n\n\n\n\nThe fitted model is better than SNaïve as they does a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                    ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set 0.1392678 0.9952668 0.7584773 76.53631 213.1399    1 0.2861511\n\n\nFitted model error measurements:\n\n\nSeries: murder_ts \nARIMA(1,1,2) \n\nCoefficients:\n         ar1      ma1     ma2\n      0.5921  -1.2472  0.3312\ns.e.  0.1946   0.2024  0.1613\n\nsigma^2 = 0.4799:  log likelihood = -212.43\nAIC=432.86   AICc=433.07   BIC=446.12\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.05427136 0.6859509 0.5223828 29.21775 153.0348 0.6887258\n                    ACF1\nTraining set -0.03840516\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method.\n\n\n\n\n\n\n\n\nOur final model equation is as follows:\nWe have an ARIMA(1,1,2) model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B + \\theta_2B\\)\nordinary difference = \\((1-B)\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1-B)x_t = (1 + \\theta_1B + \\theta_2B)w_t\\)\nWith Coefficients:\n\\((1 - (0.5921)B)(1-B)x_t = (1 + (-1.2472)B + (0.3312)2B)w_t\\)\n\n\n\nWe are fitting a SARIMA model as we saw a clear seasonal component in the previous section. We need to determine the following parameters:\n\np: The order of AR terms.\nd: The degree of differencing needed to make the data stationary.\nq: The order of MA terms.\nP: The seasonal order of AR terms.\nD: The seasonal degree of differencing.\nQ: The seasonal order of MA terms.\ns: The seasonal period.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe series looks to be stationary here. We should also try a seasonal differencing at a seasonal period of 12.\nThe seasonal period s is clearly 12. There are spikes at 1 and 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot.\n\n\n\n\n\n\n\n\nThis looks less stationary than a simple first differencing, we will try D = 0 and 1.\nWe will try the following parameters:\n\np: 1, 2, 4\nd: 1\nq: 1, 4\nP: 1\nD: 0, 1\nQ: 1, 2, 3\n\n\n\n\n\n\nerror:  1 1 1 1 1 1 \nerror:  1 1 1 1 0 2 \n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n4\n1\n1\n1\n1\n1\n2\n312.2259\n331.7395\n312.6824\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n4\n1\n1\n1\n1\n1\n2\n312.2259\n331.7395\n312.6824\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n4\n1\n1\n1\n1\n1\n2\n312.2259\n331.7395\n312.6824\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=2\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 2)12.\n\n\n\n\n\nSeries: robbery_ts \nARIMA(2,1,2)(1,0,0)[12] with drift \n\nCoefficients:\n          ar1      ar2     ma1      ma2    sar1    drift\n      -0.6359  -0.0516  0.0360  -0.3955  0.5189  -0.0009\ns.e.   0.4151   0.1324  0.4049   0.2949  0.0649   0.0294\n\nsigma^2 = 0.3224:  log likelihood = -172.23\nAIC=358.46   AICc=359.03   BIC=381.65\n\n\nThe information criteria are all worse than the previously used model of SARIMA(1, 1, 1)(1, 1, 2)12, this model is not worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model looks similar to the actual time series.\n\n\n\n\n\n\n\n\n\nThis model appears to be better than SNaïve as it does a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                      ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set -0.08022895 0.7739868 0.6020998 35.54048 220.9091    1 0.5657567\n\n\nFitted model error measurements:\n\n\nSeries: robbery_ts \nARIMA(1,1,1)(1,1,2)[12] \n\nCoefficients:\n         ar1      ma1     sar1    sma1     sma2\n      0.1080  -0.6706  -0.8787  0.2005  -0.7487\ns.e.  0.1279   0.0992   0.1785  0.2820   0.2073\n\nsigma^2 = 0.2684:  log likelihood = -150.11\nAIC=312.23   AICc=312.68   BIC=331.74\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.00740656 0.4947167 0.3760977 8.118388 150.3828 0.6246433\n                     ACF1\nTraining set -0.003315134\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method.\n\n\n\n\n\n\n\n\nOur final model equation is as follows:\nWe have a SARIMA(1,1,1)(1,1,2)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12} + \\Theta_2B^{24}\\)\nordinary difference = \\((1-B)\\)\nseasonal difference = \\((1-B^{12})\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)(1-B^{12})x_t = (1 + \\theta_1B)(1 + \\Theta_1B^{12} + \\Theta_2B^{24})w_t\\)\nWith Coefficients:\n\\((1 - (0.1080)B)(1 - (-0.8787)B^{12})(1-B)(1-B^{12})x_t = (1 + (-0.6706)B)(1 + (0.2005)B^{12} + (-0.7487)B^{24})w_t\\)"
  },
  {
    "objectID": "arma_arima_sarima_models.html#first-difference",
    "href": "arma_arima_sarima_models.html#first-difference",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "The series looks to be stationary at this level, though we should try a seasonal differencing. The seasonal period s is clearly 12. There are spikes at 1 and 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot."
  },
  {
    "objectID": "arma_arima_sarima_models.html#seasonal-difference",
    "href": "arma_arima_sarima_models.html#seasonal-difference",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This looks less stationary than a simple first differencing, we will try D = 0 and 1.\nWe will try the following parameters:\n\np: 1, 2, 4\nd: 0, 1\nq: 1, 4\nP: 1\nD: 0, 1\nQ: 1, 2, 3"
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-selection-and-diagnostics",
    "href": "arma_arima_sarima_models.html#model-selection-and-diagnostics",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "p\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-32.55671\n-15.99068\n-32.25214\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-32.55671\n-15.99068\n-32.25214\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-32.55671\n-15.99068\n-32.25214\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=1\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals, meaning we have a good model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.\n\n\n\n\n\nSeries: arrests_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4338  -0.7443  -0.6934\ns.e.  0.1368   0.1019   0.0554\n\nsigma^2 = 0.04578:  log likelihood = 20.95\nAIC=-33.89   AICc=-33.68   BIC=-20.88\n\n\nThe information criteria are all better that the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12. This model is absolutely worth considering moving forward.\n\n\n\n\n\n\n\n\nBoth fitted models look similar to the actual time series. The two fitted models look incredibly similar, so we will just go with the one we fit by hand."
  },
  {
    "objectID": "arma_arima_sarima_models.html#forecasting",
    "href": "arma_arima_sarima_models.html#forecasting",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Both models appear to be better than SNaïve as they do a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                     ME     RMSE      MAE       MPE     MAPE MASE     ACF1\nTraining set -0.1136878 0.401671 0.309908 -22.97998 217.0115    1 0.717891\n\n\nFitted model error measurements:\n\n\nSeries: arrests_ts \nARIMA(1,1,1)(1,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sar1     sma1\n      0.4358  -0.7447  -0.0331  -0.6763\ns.e.  0.1362   0.1012   0.1009   0.0774\n\nsigma^2 = 0.04599:  log likelihood = 21\nAIC=-32   AICc=-31.67   BIC=-15.74\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE    MAPE    MASE\nTraining set -0.01410203 0.2053272 0.1439894 39.06935 74.2216 0.46462\n                   ACF1\nTraining set 0.02119928\n\n\nSeries: arrests_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.4338  -0.7443  -0.6934\ns.e.  0.1368   0.1019   0.0554\n\nsigma^2 = 0.04578:  log likelihood = 20.95\nAIC=-33.89   AICc=-33.68   BIC=-20.88\n\nTraining set error measures:\n                    ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.014476 0.2053915 0.1439828 37.82999 73.83899 0.4645986\n                   ACF1\nTraining set 0.02092654\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method.\n\n\n\n\n\nFit a normalized model for later comparison.\n\n\nSeries: scale(arrests_ts) \nARIMA(1,1,1)(1,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sar1     sma1\n      0.4358  -0.7447  -0.0331  -0.6763\ns.e.  0.1362   0.1012   0.1009   0.0774\n\nsigma^2 = 0.04599:  log likelihood = 21\nAIC=-32   AICc=-31.67   BIC=-15.74\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE    MAPE    MASE\nTraining set -0.01410203 0.2053272 0.1439894 39.06935 74.2216 0.46462\n                   ACF1\nTraining set 0.02119928"
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-equation",
    "href": "arma_arima_sarima_models.html#model-equation",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Our final model equation is as follows:\nWe have a SARIMA(1,1,1)(1,1,1)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12}\\)\nordinary difference = \\((1-B)\\)\nseasonal difference = \\((1-B^{12})\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)(1-B^{12})x_t = (1 + \\theta_1B)(1 + \\Theta_1B^{12})w_t\\)\nWith Coefficients:\n\\((1 - 0.435B)(1 - (-0.0331)B^{12})(1-B)(1-B^{12})x_t = (1 + (-0.7447)B)(1 + (-0.6763)B^{12})w_t\\)"
  },
  {
    "objectID": "arma_arima_sarima_models.html#first-difference-1",
    "href": "arma_arima_sarima_models.html#first-difference-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "The series looks to be stationary at this level, though we should try a seasonal differencing. The seasonal period s is clearly 12. There are spikes at 1 and 2 on the ACF plot and spikes at 1 and 2 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot."
  },
  {
    "objectID": "arma_arima_sarima_models.html#seasonal-difference-1",
    "href": "arma_arima_sarima_models.html#seasonal-difference-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This looks less stationary than a simple first differencing, we will try D = 0 and 1.\nWe will try the following parameters:\n\np: 1, 2\nd: 1\nq: 1, 2\nP: 1\nD: 0, 1\nQ: 1, 2, 3"
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-selection-and-diagnostics-1",
    "href": "arma_arima_sarima_models.html#model-selection-and-diagnostics-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "p\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n1\n1\n1\n215.2202\n231.4816\n215.5446\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n1\n1\n1\n215.2202\n231.4816\n215.5446\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n1\n1\n1\n215.2202\n231.4816\n215.5446\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=1\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals at some lag values, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.\n\n\n\n\n\nSeries: assault_ts \nARIMA(1,0,1)(0,1,1)[12] with drift \n\nCoefficients:\n         ar1      ma1     sma1    drift\n      0.9355  -0.3413  -0.8120  -0.0047\ns.e.  0.0321   0.0744   0.0588   0.0060\n\nsigma^2 = 0.1589:  log likelihood = -100.52\nAIC=211.05   AICc=211.37   BIC=227.34\n\n\nThe information criteria are all worse than the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12. This model is not worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model looks similar to the actual time series."
  },
  {
    "objectID": "arma_arima_sarima_models.html#forecasting-1",
    "href": "arma_arima_sarima_models.html#forecasting-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This model appears to be better than SNaïve as it does a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                      ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set -0.01932497 0.7578097 0.5323319 155.2004 422.9095    1 0.7024543\n\n\nFitted model error measurements:\n\n\nSeries: assault_ts \nARIMA(1,1,1)(1,1,1)[12] \n\nCoefficients:\n          ar1      ma1     sar1     sma1\n      -0.0638  -0.3161  -0.0817  -0.7733\ns.e.   0.1959   0.1884   0.0962   0.0745\n\nsigma^2 = 0.1639:  log likelihood = -102.61\nAIC=215.22   AICc=215.54   BIC=231.48\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01193998 0.3876284 0.2720116 -81.78453 154.0811 0.5109812\n                     ACF1\nTraining set -0.001187566\n\n\nSeries: assault_ts \nARIMA(1,0,1)(0,1,1)[12] with drift \n\nCoefficients:\n         ar1      ma1     sma1    drift\n      0.9355  -0.3413  -0.8120  -0.0047\ns.e.  0.0321   0.0744   0.0588   0.0060\n\nsigma^2 = 0.1589:  log likelihood = -100.52\nAIC=211.05   AICc=211.37   BIC=227.34\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.007988694 0.3826276 0.2657987 -42.81001 113.7842 0.4993101\n                    ACF1\nTraining set -0.01329669\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method."
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-equation-1",
    "href": "arma_arima_sarima_models.html#model-equation-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Our final model equation is as follows:\nWe have a SARIMA(1,1,1)(1,1,1)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12}\\)\nordinary difference = \\((1-B)\\)\nseasonal difference = \\((1-B^{12})\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)(1-B^{12})x_t = (1 + \\theta_1B)(1 + \\Theta_1B^{12})w_t\\)\nWith Coefficients:\n\\((1 - (-0.0638)B)(1 - (-0.0817)B^{12})(1-B)(1-B^{12})x_t = (1 + (-0.3161)B)(1 + (-0.7733)B^{12})w_t\\)"
  },
  {
    "objectID": "arma_arima_sarima_models.html#first-difference-2",
    "href": "arma_arima_sarima_models.html#first-difference-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "The series looks to be stationary at this level, though we should try a seasonal differencing. The seasonal period s is clearly 12. There are spikes at 1, 3, 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot."
  },
  {
    "objectID": "arma_arima_sarima_models.html#seasonal-difference-2",
    "href": "arma_arima_sarima_models.html#seasonal-difference-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This looks less stationary than a simple first differencing, we will try D = 0 and 1.\nWe will try the following parameters:\n\np: 1, 2, 4\nd: 1\nq: 1, 3, 4\nP: 1\nD: 0, 1\nQ: 1, 2, 3"
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-selection-and-diagnostics-2",
    "href": "arma_arima_sarima_models.html#model-selection-and-diagnostics-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "p\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n13\n2\n1\n1\n1\n0\n1\n-31.9426\n-12.06336\n-31.51402\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n13\n2\n1\n1\n1\n0\n1\n-31.9426\n-12.06336\n-31.51402\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n13\n2\n1\n1\n1\n0\n1\n-31.9426\n-12.06336\n-31.51402\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=1\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals at some lag values, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.\n\n\n\n\n\nSeries: controlled_pos_ts \nARIMA(2,1,1)(2,0,0)[12] with drift \n\nCoefficients:\n          ar1      ar2      ma1    sar1    sar2    drift\n      -0.1522  -0.2874  -0.4061  0.3955  0.2736  -0.0015\ns.e.   0.1430   0.0932   0.1465  0.0679  0.0732   0.0168\n\nsigma^2 = 0.0494:  log likelihood = 16.96\nAIC=-19.92   AICc=-19.35   BIC=3.27\n\n\nThe information criteria are all worse than the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12. This model is not worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model looks similar to the actual time series."
  },
  {
    "objectID": "arma_arima_sarima_models.html#forecasting-2",
    "href": "arma_arima_sarima_models.html#forecasting-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This model appears to be better than SNaïve as it does a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                     ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set -0.1591141 0.3603143 0.2850547 57.74682 104.1726    1 0.4911806\n\n\nFitted model error measurements:\n\n\nSeries: controlled_pos_ts \nARIMA(1,1,1)(1,1,1)[12] \n\nCoefficients:\n         ar1      ma1    sar1     sma1\n      0.1299  -0.6667  0.0023  -0.7269\ns.e.  0.1172   0.0879  0.1046   0.0801\n\nsigma^2 = 0.04845:  log likelihood = 15.4\nAIC=-20.79   AICc=-20.47   BIC=-4.53\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.02399794 0.2107427 0.1484459 9.835336 42.68932 0.5207628\n                   ACF1\nTraining set 0.02970125\n\n\nSeries: controlled_pos_ts \nARIMA(2,1,1)(2,0,0)[12] with drift \n\nCoefficients:\n          ar1      ar2      ma1    sar1    sar2    drift\n      -0.1522  -0.2874  -0.4061  0.3955  0.2736  -0.0015\ns.e.   0.1430   0.0932   0.1465  0.0679  0.0732   0.0168\n\nsigma^2 = 0.0494:  log likelihood = 16.96\nAIC=-19.92   AICc=-19.35   BIC=3.27\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01010566 0.218415 0.1523486 -16.98116 61.98025 0.5344541\n                    ACF1\nTraining set -0.02147658\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method."
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-equation-2",
    "href": "arma_arima_sarima_models.html#model-equation-2",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Our final model equation is as follows:\nWe have a SARIMA(1,1,1)(1,1,1)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12}\\)\nordinary difference = \\((1-B)\\)\nseasonal difference = \\((1-B^{12})\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)(1-B^{12})x_t = (1 + \\theta_1B)(1 + \\Theta_1B^{12})w_t\\)\nWith Coefficients:\n\\((1 - (0.1299)B)(1 - (0.0023)B^{12})(1-B)(1-B^{12})x_t = (1 + (-0.6667)B)(1 + (-0.7269)B^{12})w_t\\)"
  },
  {
    "objectID": "arma_arima_sarima_models.html#first-difference-3",
    "href": "arma_arima_sarima_models.html#first-difference-3",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "There still looks to be a trend at this level, we should try to difference one more time. We should also try a seasonal differencing at a seasonal period of 12."
  },
  {
    "objectID": "arma_arima_sarima_models.html#second-difference",
    "href": "arma_arima_sarima_models.html#second-difference",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This looks more stationary, we should determine parameters to try from here.\nThe seasonal period s is clearly 12. There are spikes at 1 and 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot."
  },
  {
    "objectID": "arma_arima_sarima_models.html#seasonal-difference-3",
    "href": "arma_arima_sarima_models.html#seasonal-difference-3",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This looks less stationary than a simple first differencing, we will try D = 0 and 1.\nWe will try the following parameters:\n\np: 1, 2, 4\nd: 1, 2\nq: 1, 4\nP: 1\nD: 0, 1\nQ: 1, 2, 3"
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-selection-and-diagnostics-3",
    "href": "arma_arima_sarima_models.html#model-selection-and-diagnostics-3",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "p\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-131.9417\n-115.3757\n-131.6371\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-131.9417\n-115.3757\n-131.6371\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1\n0\n1\n-131.9417\n-115.3757\n-131.6371\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=1\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals at some lag values, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 1)12.\n\n\n\n\n\nSeries: marijuana_pos_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.2557  -0.5081  -0.6226\ns.e.  0.2258   0.2001   0.0552\n\nsigma^2 = 0.02916:  log likelihood = 65.08\nAIC=-122.16   AICc=-121.95   BIC=-109.15\n\n\nThe information criteria are all slightly better than the previously used model of SARIMA(1, 1, 1)(1, 1, 1)12, but not so much better that this model would be worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model looks similar to the actual time series."
  },
  {
    "objectID": "arma_arima_sarima_models.html#forecasting-3",
    "href": "arma_arima_sarima_models.html#forecasting-3",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This model appears to be better than SNaïve as it does a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                     ME      RMSE       MAE      MPE     MAPE MASE     ACF1\nTraining set -0.1123487 0.3996005 0.2874182 1.037198 61.19043    1 0.832498\n\n\nFitted model error measurements:\n\n\nSeries: marijuana_pos_ts \nARIMA(1,1,1)(1,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sar1     sma1\n      0.2848  -0.5273  -0.0775  -0.5769\ns.e.  0.2306   0.2018   0.1100   0.0897\n\nsigma^2 = 0.02923:  log likelihood = 65.33\nAIC=-120.65   AICc=-120.33   BIC=-104.39\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01026806 0.1636941 0.1107387 0.6302746 23.70606 0.3852878\n                    ACF1\nTraining set 0.003563427\n\n\nSeries: marijuana_pos_ts \nARIMA(1,1,1)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sma1\n      0.2557  -0.5081  -0.6226\ns.e.  0.2258   0.2001   0.0552\n\nsigma^2 = 0.02916:  log likelihood = 65.08\nAIC=-122.16   AICc=-121.95   BIC=-109.15\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.01068143 0.1639416 0.1115361 1.036487 23.75623 0.3880621\n                    ACF1\nTraining set 0.002802718\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method.\n\n\n\n\n\nIt doesn’t really make sense to forecast here as we can be sure that there will be no more arrests for marijuana possession, but it is good to know that we have fitted a good model. We can predict what would have happened to the level of arrests if the policy changes that were instituded over the years never happened."
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-equation-3",
    "href": "arma_arima_sarima_models.html#model-equation-3",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Our final model equation is as follows:\nWe have a SARIMA(1,1,1)(1,1,1)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12}\\)\nordinary difference = \\((1-B)\\)\nseasonal difference = \\((1-B^{12})\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)(1-B^{12})x_t = (1 + \\theta_1B)(1 + \\Theta_1B^{12})w_t\\)\nWith Coefficients:\n\\((1 - (0.2848)B)(1 - (-0.0775)B^{12})(1-B)(1-B^{12})x_t = (1 + (-0.5273)B)(1 + (-0.5769)B^{12})w_t\\)"
  },
  {
    "objectID": "arma_arima_sarima_models.html#first-difference-4",
    "href": "arma_arima_sarima_models.html#first-difference-4",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This looks to be slightly more stationary than the original series, we can proceed with the model build process. There are spikes at 1 and 3 on the ACF plot and spikes at 1, 2 on the PACF plot.\nWe will try the following parameters:\n\np: 1, 2\nd: 0, 1\nq: 1, 3"
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-selection-and-diagnostics-4",
    "href": "arma_arima_sarima_models.html#model-selection-and-diagnostics-4",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "p\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n8\n2\n1\n3\n531.7265\n551.6058\n532.1551\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n8\n2\n1\n3\n531.7265\n551.6058\n532.1551\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n8\n2\n1\n3\n531.7265\n551.6058\n532.1551\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=2, d=1, q=3\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there may be correlation between residuals at some lag values, meaning we might not have a good enough model. We will still proceed with the model ARIMA(2, 1, 3).\n\n\n\n\n\nSeries: motor_theft_ts \nARIMA(1,1,2)(1,0,1)[12] \n\nCoefficients:\n         ar1      ma1     ma2    sar1     sma1\n      0.6836  -1.3614  0.3775  0.5699  -0.2578\ns.e.  0.1505   0.1767  0.1618  0.1867   0.2183\n\nsigma^2 = 0.7354:  log likelihood = -255.8\nAIC=523.6   AICc=524.03   BIC=543.48\n\n\nThe information criteria are all slightly better than the previously used model of ARIMA(2, 1, 3), and it considers the seasonal component, so this model would be worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model and auto mdoel look similar to the actual time series."
  },
  {
    "objectID": "arma_arima_sarima_models.html#forecasting-4",
    "href": "arma_arima_sarima_models.html#forecasting-4",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Both models appear to be better than SNaïve as they does a better job of capturing the trend. It is hard to tell which is better between the fit and the auto fit.\nSNaïve model error measurements:\n\n\n                     ME     RMSE       MAE     MPE     MAPE MASE      ACF1\nTraining set 0.01534954 1.102843 0.8651354 78.4586 280.3894    1 0.3607956\n\n\nFitted model error measurements:\n\n\nSeries: motor_theft_ts \nARIMA(2,1,3) \n\nCoefficients:\n         ar1      ar2      ma1     ma2      ma3\n      1.3083  -0.8110  -2.1161  1.9923  -0.8229\ns.e.  0.0575   0.1745   0.1212  0.3464   0.2057\n\nsigma^2 = 0.7673:  log likelihood = -259.86\nAIC=531.73   AICc=532.16   BIC=551.61\n\nTraining set error measures:\n                      ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set 0.008238411 0.8629927 0.673068 88.31137 136.4341 0.7779915\n                  ACF1\nTraining set 0.0891247\n\n\nSeries: motor_theft_ts \nARIMA(1,1,2)(1,0,1)[12] \n\nCoefficients:\n         ar1      ma1     ma2    sar1     sma1\n      0.6836  -1.3614  0.3775  0.5699  -0.2578\ns.e.  0.1505   0.1767  0.1618  0.1867   0.2183\n\nsigma^2 = 0.7354:  log likelihood = -255.8\nAIC=523.6   AICc=524.03   BIC=543.48\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.001864579 0.8448407 0.6522227 69.26282 148.5133 0.7538966\n                    ACF1\nTraining set 0.007715526\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method. The seasonal, auto fitted model slightly outperforms the non-seasonal ARIMA model, so we will go with that model."
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-equation-4",
    "href": "arma_arima_sarima_models.html#model-equation-4",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Our final model equation is as follows:\nWe have a SARIMA(1,1,2)(1,0,2)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B + \\theta_2B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12}\\)\nordinary difference = \\((1-B)\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)x_t = (1 + \\theta_1B + \\theta_2B)(1 + \\Theta_1B^{12})w_t\\)\nWith Coefficients:\n\\((1 - (0.6836)B)(1 - (0.5699)B^{12})(1-B)x_t = (1 + (-1.3614)B + (0.3775)2B)(1 + (-0.2578)B^{12})w_t\\)"
  },
  {
    "objectID": "arma_arima_sarima_models.html#first-difference-5",
    "href": "arma_arima_sarima_models.html#first-difference-5",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This looks to be much more stationary than the original series, we can proceed with the model build process. There are spikes at 1 on the ACF plot and spikes at 1 and 2 on the PACF plot.\nWe will try the following parameters:\n\np: 1\nd: 0, 1\nq: 1, 2"
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-selection-and-diagnostics-5",
    "href": "arma_arima_sarima_models.html#model-selection-and-diagnostics-5",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "p\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n4\n1\n1\n2\n432.8634\n446.1162\n433.0654\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n433.3898\n443.3294\n433.5104\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n4\n1\n1\n2\n432.8634\n446.1162\n433.0654\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=2\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there may be correlation between residuals at some lag values, meaning we might not have a good enough model. We will still proceed with the model ARIMA(1, 1, 2).\n\n\n\n\n\nSeries: murder_ts \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.7223\ns.e.   0.0674\n\nsigma^2 = 0.4858:  log likelihood = -214.64\nAIC=433.27   AICc=433.33   BIC=439.9\n\n\nThe information criteria are all slightly better than the previously used model of ARIMA(1, 1, 2), but not so much better that this model would be worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model and auto mdoel look similar to the actual time series."
  },
  {
    "objectID": "arma_arima_sarima_models.html#forecasting-5",
    "href": "arma_arima_sarima_models.html#forecasting-5",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "The fitted model is better than SNaïve as they does a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                    ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set 0.1392678 0.9952668 0.7584773 76.53631 213.1399    1 0.2861511\n\n\nFitted model error measurements:\n\n\nSeries: murder_ts \nARIMA(1,1,2) \n\nCoefficients:\n         ar1      ma1     ma2\n      0.5921  -1.2472  0.3312\ns.e.  0.1946   0.2024  0.1613\n\nsigma^2 = 0.4799:  log likelihood = -212.43\nAIC=432.86   AICc=433.07   BIC=446.12\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.05427136 0.6859509 0.5223828 29.21775 153.0348 0.6887258\n                    ACF1\nTraining set -0.03840516\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method."
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-equation-5",
    "href": "arma_arima_sarima_models.html#model-equation-5",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Our final model equation is as follows:\nWe have an ARIMA(1,1,2) model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B + \\theta_2B\\)\nordinary difference = \\((1-B)\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1-B)x_t = (1 + \\theta_1B + \\theta_2B)w_t\\)\nWith Coefficients:\n\\((1 - (0.5921)B)(1-B)x_t = (1 + (-1.2472)B + (0.3312)2B)w_t\\)"
  },
  {
    "objectID": "arma_arima_sarima_models.html#first-difference-6",
    "href": "arma_arima_sarima_models.html#first-difference-6",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "The series looks to be stationary here. We should also try a seasonal differencing at a seasonal period of 12.\nThe seasonal period s is clearly 12. There are spikes at 1 and 4 on the ACF plot and spikes at 1, 2, and 4 on the PACF plot. Seasonal spikes occur at 1, 2, and 3 on the ACF plot, and 1 on the PACF plot."
  },
  {
    "objectID": "arma_arima_sarima_models.html#seasonal-difference-4",
    "href": "arma_arima_sarima_models.html#seasonal-difference-4",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This looks less stationary than a simple first differencing, we will try D = 0 and 1.\nWe will try the following parameters:\n\np: 1, 2, 4\nd: 1\nq: 1, 4\nP: 1\nD: 0, 1\nQ: 1, 2, 3"
  },
  {
    "objectID": "arma_arima_sarima_models.html#build-model-6",
    "href": "arma_arima_sarima_models.html#build-model-6",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "error:  1 1 1 1 1 1 \nerror:  1 1 1 1 0 2"
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-selection-and-diagnostics-6",
    "href": "arma_arima_sarima_models.html#model-selection-and-diagnostics-6",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "p\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n4\n1\n1\n1\n1\n1\n2\n312.2259\n331.7395\n312.6824\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n4\n1\n1\n1\n1\n1\n2\n312.2259\n331.7395\n312.6824\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n4\n1\n1\n1\n1\n1\n2\n312.2259\n331.7395\n312.6824\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=2\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 2)12.\n\n\n\n\n\nSeries: robbery_ts \nARIMA(2,1,2)(1,0,0)[12] with drift \n\nCoefficients:\n          ar1      ar2     ma1      ma2    sar1    drift\n      -0.6359  -0.0516  0.0360  -0.3955  0.5189  -0.0009\ns.e.   0.4151   0.1324  0.4049   0.2949  0.0649   0.0294\n\nsigma^2 = 0.3224:  log likelihood = -172.23\nAIC=358.46   AICc=359.03   BIC=381.65\n\n\nThe information criteria are all worse than the previously used model of SARIMA(1, 1, 1)(1, 1, 2)12, this model is not worth considering moving forward.\n\n\n\n\n\n\n\n\nThe fitted model looks similar to the actual time series."
  },
  {
    "objectID": "arma_arima_sarima_models.html#forecasting-6",
    "href": "arma_arima_sarima_models.html#forecasting-6",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "This model appears to be better than SNaïve as it does a better job of capturing the trend.\nSNaïve model error measurements:\n\n\n                      ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set -0.08022895 0.7739868 0.6020998 35.54048 220.9091    1 0.5657567\n\n\nFitted model error measurements:\n\n\nSeries: robbery_ts \nARIMA(1,1,1)(1,1,2)[12] \n\nCoefficients:\n         ar1      ma1     sar1    sma1     sma2\n      0.1080  -0.6706  -0.8787  0.2005  -0.7487\ns.e.  0.1279   0.0992   0.1785  0.2820   0.2073\n\nsigma^2 = 0.2684:  log likelihood = -150.11\nAIC=312.23   AICc=312.68   BIC=331.74\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.00740656 0.4947167 0.3760977 8.118388 150.3828 0.6246433\n                     ACF1\nTraining set -0.003315134\n\n\nThe model error measurements the model are all much lower than the SNaïve benchmark method."
  },
  {
    "objectID": "arma_arima_sarima_models.html#model-equation-6",
    "href": "arma_arima_sarima_models.html#model-equation-6",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "Our final model equation is as follows:\nWe have a SARIMA(1,1,1)(1,1,2)[12] model.\nComponents:\nAR: \\(\\phi(B) = 1 - \\phi_1B\\)\nMA: \\(\\theta(B) = 1 + \\theta_1B\\)\nSAR: \\(\\Phi_P(B^s) = 1 - \\Phi_1B^{12}\\)\nSMA: \\(\\Theta_Q(B^s) = 1 + \\Theta_1B^{12} + \\Theta_2B^{24}\\)\nordinary difference = \\((1-B)\\)\nseasonal difference = \\((1-B^{12})\\)\nGeneral Model:\n\\((1 - \\phi_1B)(1 - \\Phi_1B^{12})(1-B)(1-B^{12})x_t = (1 + \\theta_1B)(1 + \\Theta_1B^{12} + \\Theta_2B^{24})w_t\\)\nWith Coefficients:\n\\((1 - (0.1080)B)(1 - (-0.8787)B^{12})(1-B)(1-B^{12})x_t = (1 + (-0.6706)B)(1 + (0.2005)B^{12} + (-0.7487)B^{24})w_t\\)"
  },
  {
    "objectID": "exploratory_data_analysis.html",
    "href": "exploratory_data_analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "TotalAssaultControlled Substance PossessionMarijuana PossessionMotor Vehicle TheftMurderRobbery\n\n\n\n\n\n\n\n\nThere appears to be some seasonality in the plot, as there are noticeable dips in arrest rate towards the end of each year. There was a slight upward trend towards the end of the 2000’s but that changed dramatically during the 2010’s into a clear downward trend. Since the start of the current decade the trend has reversed course, however. It is hard to tell if the time series is multiplicative or additive, as it seems to hold different characteristics at different points, though this possibly suggests a multiplicative time series. The change in trend could also be part of a longer cycle, though it is impossible to tell from this graph as it only goes back to 2006.\n\n\n\n\n\n\n\n\nLooking at the lag plot up to a lag of 12 months, we can see that the data are clearly not random, as a linear shape exists in each lag that we observe here. Seasonality is harder to observe here, as each month does not appear to be clustered alongside other observations of the same month. This could be due to the large number of points, however.\n\n\n\n\n\n\nIf we look up to a lag of 48 months, we can see that as the lag increases, the data get more random. The Lag 48 plot exhibiting less of a linear pattern than the lag 12 plot, which is fairly linear.\n\n\n\n\n\n\n\n\nWe can see the trend here that we were able to observe on the initial graph. There is horizontal movement during the first half of the observed period and then a clear downward trend until 2020. After that, it reverses and increases until the present day.\nRemoving the trend allows us to see some seasonality. Once the seasonality has been averaged out as it has been here, the clear dips in arrests towards the end of each year remain and there is a clear pattern between the large dips at the end of each year.\nThere seems to be more noise in the more recent data, which could be something interesting to look into.\n\n\n\n\n\n\n\n\nThe ACF values are decaying towards zero as lag increases, which is evidence that this series needs to be differenced.\n\n\n\n\n\nSpikes can be seen at lags of 1, 2, 12, and 13.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  arrests_ts\nDickey-Fuller = -2.7629, Lag order = 5, p-value = 0.2564\nalternative hypothesis: stationary\n\n\nThe ADF tests returns a p-value of 0.26, meaning we do not have enough evidence to reject the null hypotheses, the series is not stationary.\n\n\n\n\n\n\n\n\nThe detrended series (the middle plot) does not seem to be vastly different from the original series. This could be evidence that there is not much of a trend in the data. The differenced series looks to be stationary, which suggests that there may be a trend.\n\n\n\n\n\nThis ACF has positive spikes at 12, 24, and 36, meaning once the trend is removed, the seasonality becomes evident. The ACF of the differenced time series provides strong evidence in support of seasonality. A SARIMA model would best suit this time series.\n\n\n\n\n\n\n\n\n\nThis time series follows a very similar pattern to the overall arrests series. There is an upward trend until around 2013 then a downward trend to the minimum value in April 2020, then a sharp increase until today. The data are clearly seasonal as well, seeming to follow a yearly pattern with the yearly minimum happening each winter.\n\n\n\n\n\n\n\n\nThere is the most correlation between lags at a lag value of 12, further increasing evidence for the seasonality of this series. There appears to be less correlation here than in the total time series, though.\n\n\n\n\n\n\nThere is some correlation between yearly lags, meaning this may be a good candidate for seasonal differencing. This correlation appears to decrease as the lag increases.\n\n\n\n\n\n\n\n\nDecomposition reveals the trend we noticed in the overall plot, with the drop in 2020. The increase and decrease before that point, however, may be less than we initially thought.\n\n\n\n\n\n\n\n\nThe ACF Plot is decaying towards 0 though there is evidence of seasonality in the correlation between lags.\n\n\n\n\n\nSpikes can be seen at lags 1, 2, 3, and 4.\n\n\n\n\n\nWarning in adf.test(assault_ts): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  assault_ts\nDickey-Fuller = -4.6813, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe ADF tests returns a p-value of less than 0.01, meaning we do not have enough evidence to reject the null hypotheses, the series may be stationary.\n\n\n\n\n\n\n\n\nDetrending does not change the time series much, suggesting the trend we saw earlier was not as significant as we thought. The time series was stationary before according to the ADF test, meaning differencing did not do much.\n\n\n\n\n\nThe evidence of seasonality throughout this process indicates a SARIMA model might be the best for this time series.\n\n\n\n\n\n\n\n\n\nThis looks different from the overall arrests time series. There is a clear negative linear trend in this series, though that trend hit its minimum in July 2020 and has generally been increasing since, though only slightly compared to the initial decrease.\n\n\n\n\n\n\n\n\nThis series appears to be highly correlated as all of these lags show high levels of correlation.\n\n\n\n\n\n\nLike the other series, the correlation between lags gets less as the lag increases, though it is still fairly high.\n\n\n\n\n\n\n\n\nThe time series is clearly additive, and the negative trend becomes clear when decomposed. There does appear to be a seasonal component.\n\n\n\n\n\n\n\n\nThis is evidence against seasonality, and evidence that the series should be differenced.\n\n\n\n\n\nSpikes are seen at 1, 2, 3, 8, and 12.\n\n\n\n\n\nWarning in adf.test(controlled_pos_ts): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  controlled_pos_ts\nDickey-Fuller = -4.2983, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe ADF test shows that this series may be stationary.\n\n\n\n\n\n\n\n\nDetrending clearly changes the time series to be more stationary, meaning there is a clear trend. Differencing also moves us closer to a stationary time series.\n\n\n\n\n\nThis series also seems to have a significant seasonal component, we will fit a SARIMA model.\n\n\n\n\n\n\n\n\n\nThis is an interesting one, as there is a clear positive trend at the start of the series which quickly reverses when policy changes were made as discussed before. The series eventually goes to zero as marijuana possession is now legal in New York City.\n\n\n\n\n\n\n\n\nThe lag plots look different from most others as there is less correlation in lag 12 than in lag 1, suggesting the data may not be seasonal.\n\n\n\n\n\n\nThere is very little lag correlation over the 4 year period, even more evidence against seasonality.\n\n\n\n\n\n\n\n\nWe can clearly see the trend observed in the initial plot here in the decomposition. If there were to be a seasonal component it looks to be yearly, but it is hard to tell.\n\n\n\n\n\n\n\n\nWe should difference this series, this ACF plot looks just like the others.\n\n\n\n\n\nThere is a large spike at a lag of 1, then some smaller spikes at 2 and 5.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  marijuana_pos_ts\nDickey-Fuller = -2.8312, Lag order = 5, p-value = 0.2278\nalternative hypothesis: stationary\n\n\nWe do not have enough evidence to reject the null hypothesis, the series may not be stationary.\n\n\n\n\n\n\n\n\n\n\n\nThe differenced plot looks more stationary, suggesting that differencing may be useful here. The strong seasonal component suggests a SARIMA model would be useful.\n\n\n\n\n\n\n\n\n\nThis series looks the most stationary out of any so far, there really does not seem to be any patterns to it.\n\n\n\n\n\n\n\n\nThe lag plots support that conclusion, there seems to be little to no correlation between lags at any level.\n\n\n\n\n\n\nThe absence of correlation continues up through high level of lag.\n\n\n\n\n\n\n\n\nThe random component looks fairly similar to the observed series, which would make sense for a stationary series.\n\n\n\n\n\n\n\n\nThere may be a slight seasonal component, but it is hard to tell.\n\n\n\n\n\nThere are spikes at 1, 2, 3, and 4, and there may be some seasonality.\n\n\n\n\n\nWarning in adf.test(motor_theft_ts): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  motor_theft_ts\nDickey-Fuller = -5.4308, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe do not have enough evidence to reject the null hypothesis that the series may be stationary.\n\n\n\n\n\n\n\n\n\n\n\nThe detrended plot looks very similar to the observed plot. There is no seasonal component here, an ARIMA model will work.\n\n\n\n\n\n\n\n\n\nThe series here is mostly stationary until recently when the trend begins to increase. This is both interesting and concerning and should be looked into.\n\n\n\n\n\n\n\n\nAs seen in the original graph, there is not much evidence for seasonality.\n\n\n\n\n\n\nEven when looking further out, there is not much evidence for seasonality.\n\n\n\n\n\n\n\n\nWe can see that same trend clearly here, there is an obvious increase in murder rate that started around 2020.\n\n\n\n\n\n\n\n\nThe ACF quickly decreases to 0, which means we may not need to difference it.\n\n\n\n\n\nThe PACF plot looks good as well, there are only spikes at 1, 2, and 7.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  murder_ts\nDickey-Fuller = -2.5834, Lag order = 5, p-value = 0.3317\nalternative hypothesis: stationary\n\n\nThere is not enough evidence to reject the null hypothesis, the series may be stationary.\n\n\n\n\n\n\n\n\n\n\n\nThe differenced time series looks stationary, indicating that we may need to difference. There is little evidence for any seasonal components, an ARIMA model would be best.\n\n\n\n\n\n\n\n\n\nThere is a clear negative trend throughout the series until 2020 when it changes significantly and becomes very positive.\n\n\n\n\n\n\n\n\nThere is some evidence for seasonality as there does appear to be some correlation between lagged values.\n\n\n\n\n\n\nSome correlation remains even at larger lag values.\n\n\n\n\n\n\n\n\nWe can see that the trend does not seem to be as negative as we previously thought, and the dip in 2020 may just be a dip as levels have gone back to post-pandemic values.\n\n\n\n\n\n\n\n\nThis series looks like it needs to be differenced as there are many significant ACF values.\n\n\n\n\n\nThere are spikes at 1, 2, and 3.\n\n\n\n\n\nWarning in adf.test(robbery_ts): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  robbery_ts\nDickey-Fuller = -4.3276, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe have enough evidence to reject the null hypothesis, the series may be stationary.\n\n\n\n\n\n\n\n\n\n\n\nThe differenced time series looks to be stationary meaning differencing may be useful. The existence of a clear seasonal component means we should fit a SARIMA model."
  },
  {
    "objectID": "exploratory_data_analysis.html#by-crime",
    "href": "exploratory_data_analysis.html#by-crime",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "TotalAssaultControlled Substance PossessionMarijuana PossessionMotor Vehicle TheftMurderRobbery\n\n\n\n\n\n\n\n\nThere appears to be some seasonality in the plot, as there are noticeable dips in arrest rate towards the end of each year. There was a slight upward trend towards the end of the 2000’s but that changed dramatically during the 2010’s into a clear downward trend. Since the start of the current decade the trend has reversed course, however. It is hard to tell if the time series is multiplicative or additive, as it seems to hold different characteristics at different points, though this possibly suggests a multiplicative time series. The change in trend could also be part of a longer cycle, though it is impossible to tell from this graph as it only goes back to 2006.\n\n\n\n\n\n\n\n\nLooking at the lag plot up to a lag of 12 months, we can see that the data are clearly not random, as a linear shape exists in each lag that we observe here. Seasonality is harder to observe here, as each month does not appear to be clustered alongside other observations of the same month. This could be due to the large number of points, however.\n\n\n\n\n\n\nIf we look up to a lag of 48 months, we can see that as the lag increases, the data get more random. The Lag 48 plot exhibiting less of a linear pattern than the lag 12 plot, which is fairly linear.\n\n\n\n\n\n\n\n\nWe can see the trend here that we were able to observe on the initial graph. There is horizontal movement during the first half of the observed period and then a clear downward trend until 2020. After that, it reverses and increases until the present day.\nRemoving the trend allows us to see some seasonality. Once the seasonality has been averaged out as it has been here, the clear dips in arrests towards the end of each year remain and there is a clear pattern between the large dips at the end of each year.\nThere seems to be more noise in the more recent data, which could be something interesting to look into.\n\n\n\n\n\n\n\n\nThe ACF values are decaying towards zero as lag increases, which is evidence that this series needs to be differenced.\n\n\n\n\n\nSpikes can be seen at lags of 1, 2, 12, and 13.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  arrests_ts\nDickey-Fuller = -2.7629, Lag order = 5, p-value = 0.2564\nalternative hypothesis: stationary\n\n\nThe ADF tests returns a p-value of 0.26, meaning we do not have enough evidence to reject the null hypotheses, the series is not stationary.\n\n\n\n\n\n\n\n\nThe detrended series (the middle plot) does not seem to be vastly different from the original series. This could be evidence that there is not much of a trend in the data. The differenced series looks to be stationary, which suggests that there may be a trend.\n\n\n\n\n\nThis ACF has positive spikes at 12, 24, and 36, meaning once the trend is removed, the seasonality becomes evident. The ACF of the differenced time series provides strong evidence in support of seasonality. A SARIMA model would best suit this time series.\n\n\n\n\n\n\n\n\n\nThis time series follows a very similar pattern to the overall arrests series. There is an upward trend until around 2013 then a downward trend to the minimum value in April 2020, then a sharp increase until today. The data are clearly seasonal as well, seeming to follow a yearly pattern with the yearly minimum happening each winter.\n\n\n\n\n\n\n\n\nThere is the most correlation between lags at a lag value of 12, further increasing evidence for the seasonality of this series. There appears to be less correlation here than in the total time series, though.\n\n\n\n\n\n\nThere is some correlation between yearly lags, meaning this may be a good candidate for seasonal differencing. This correlation appears to decrease as the lag increases.\n\n\n\n\n\n\n\n\nDecomposition reveals the trend we noticed in the overall plot, with the drop in 2020. The increase and decrease before that point, however, may be less than we initially thought.\n\n\n\n\n\n\n\n\nThe ACF Plot is decaying towards 0 though there is evidence of seasonality in the correlation between lags.\n\n\n\n\n\nSpikes can be seen at lags 1, 2, 3, and 4.\n\n\n\n\n\nWarning in adf.test(assault_ts): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  assault_ts\nDickey-Fuller = -4.6813, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe ADF tests returns a p-value of less than 0.01, meaning we do not have enough evidence to reject the null hypotheses, the series may be stationary.\n\n\n\n\n\n\n\n\nDetrending does not change the time series much, suggesting the trend we saw earlier was not as significant as we thought. The time series was stationary before according to the ADF test, meaning differencing did not do much.\n\n\n\n\n\nThe evidence of seasonality throughout this process indicates a SARIMA model might be the best for this time series.\n\n\n\n\n\n\n\n\n\nThis looks different from the overall arrests time series. There is a clear negative linear trend in this series, though that trend hit its minimum in July 2020 and has generally been increasing since, though only slightly compared to the initial decrease.\n\n\n\n\n\n\n\n\nThis series appears to be highly correlated as all of these lags show high levels of correlation.\n\n\n\n\n\n\nLike the other series, the correlation between lags gets less as the lag increases, though it is still fairly high.\n\n\n\n\n\n\n\n\nThe time series is clearly additive, and the negative trend becomes clear when decomposed. There does appear to be a seasonal component.\n\n\n\n\n\n\n\n\nThis is evidence against seasonality, and evidence that the series should be differenced.\n\n\n\n\n\nSpikes are seen at 1, 2, 3, 8, and 12.\n\n\n\n\n\nWarning in adf.test(controlled_pos_ts): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  controlled_pos_ts\nDickey-Fuller = -4.2983, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe ADF test shows that this series may be stationary.\n\n\n\n\n\n\n\n\nDetrending clearly changes the time series to be more stationary, meaning there is a clear trend. Differencing also moves us closer to a stationary time series.\n\n\n\n\n\nThis series also seems to have a significant seasonal component, we will fit a SARIMA model.\n\n\n\n\n\n\n\n\n\nThis is an interesting one, as there is a clear positive trend at the start of the series which quickly reverses when policy changes were made as discussed before. The series eventually goes to zero as marijuana possession is now legal in New York City.\n\n\n\n\n\n\n\n\nThe lag plots look different from most others as there is less correlation in lag 12 than in lag 1, suggesting the data may not be seasonal.\n\n\n\n\n\n\nThere is very little lag correlation over the 4 year period, even more evidence against seasonality.\n\n\n\n\n\n\n\n\nWe can clearly see the trend observed in the initial plot here in the decomposition. If there were to be a seasonal component it looks to be yearly, but it is hard to tell.\n\n\n\n\n\n\n\n\nWe should difference this series, this ACF plot looks just like the others.\n\n\n\n\n\nThere is a large spike at a lag of 1, then some smaller spikes at 2 and 5.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  marijuana_pos_ts\nDickey-Fuller = -2.8312, Lag order = 5, p-value = 0.2278\nalternative hypothesis: stationary\n\n\nWe do not have enough evidence to reject the null hypothesis, the series may not be stationary.\n\n\n\n\n\n\n\n\n\n\n\nThe differenced plot looks more stationary, suggesting that differencing may be useful here. The strong seasonal component suggests a SARIMA model would be useful.\n\n\n\n\n\n\n\n\n\nThis series looks the most stationary out of any so far, there really does not seem to be any patterns to it.\n\n\n\n\n\n\n\n\nThe lag plots support that conclusion, there seems to be little to no correlation between lags at any level.\n\n\n\n\n\n\nThe absence of correlation continues up through high level of lag.\n\n\n\n\n\n\n\n\nThe random component looks fairly similar to the observed series, which would make sense for a stationary series.\n\n\n\n\n\n\n\n\nThere may be a slight seasonal component, but it is hard to tell.\n\n\n\n\n\nThere are spikes at 1, 2, 3, and 4, and there may be some seasonality.\n\n\n\n\n\nWarning in adf.test(motor_theft_ts): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  motor_theft_ts\nDickey-Fuller = -5.4308, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe do not have enough evidence to reject the null hypothesis that the series may be stationary.\n\n\n\n\n\n\n\n\n\n\n\nThe detrended plot looks very similar to the observed plot. There is no seasonal component here, an ARIMA model will work.\n\n\n\n\n\n\n\n\n\nThe series here is mostly stationary until recently when the trend begins to increase. This is both interesting and concerning and should be looked into.\n\n\n\n\n\n\n\n\nAs seen in the original graph, there is not much evidence for seasonality.\n\n\n\n\n\n\nEven when looking further out, there is not much evidence for seasonality.\n\n\n\n\n\n\n\n\nWe can see that same trend clearly here, there is an obvious increase in murder rate that started around 2020.\n\n\n\n\n\n\n\n\nThe ACF quickly decreases to 0, which means we may not need to difference it.\n\n\n\n\n\nThe PACF plot looks good as well, there are only spikes at 1, 2, and 7.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  murder_ts\nDickey-Fuller = -2.5834, Lag order = 5, p-value = 0.3317\nalternative hypothesis: stationary\n\n\nThere is not enough evidence to reject the null hypothesis, the series may be stationary.\n\n\n\n\n\n\n\n\n\n\n\nThe differenced time series looks stationary, indicating that we may need to difference. There is little evidence for any seasonal components, an ARIMA model would be best.\n\n\n\n\n\n\n\n\n\nThere is a clear negative trend throughout the series until 2020 when it changes significantly and becomes very positive.\n\n\n\n\n\n\n\n\nThere is some evidence for seasonality as there does appear to be some correlation between lagged values.\n\n\n\n\n\n\nSome correlation remains even at larger lag values.\n\n\n\n\n\n\n\n\nWe can see that the trend does not seem to be as negative as we previously thought, and the dip in 2020 may just be a dip as levels have gone back to post-pandemic values.\n\n\n\n\n\n\n\n\nThis series looks like it needs to be differenced as there are many significant ACF values.\n\n\n\n\n\nThere are spikes at 1, 2, and 3.\n\n\n\n\n\nWarning in adf.test(robbery_ts): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  robbery_ts\nDickey-Fuller = -4.3276, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nWe have enough evidence to reject the null hypothesis, the series may be stationary.\n\n\n\n\n\n\n\n\n\n\n\nThe differenced time series looks to be stationary meaning differencing may be useful. The existence of a clear seasonal component means we should fit a SARIMA model."
  },
  {
    "objectID": "financial_time_series_models.html",
    "href": "financial_time_series_models.html",
    "title": "Financial Time Series Models (ARCH/GARCH)",
    "section": "",
    "text": "Imports\nlibrary(plotly)\nlibrary(caret)\nlibrary(car)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(gridExtra)\nlibrary(tseries)\nlibrary(quantmod)\nlibrary(fGarch)\nlibrary(FinTS)\n\n\nAn interesting financial asset to model that would be useful in predicting crime rates would be CoreCivic (CXW), a company that owns and operates many private prisons throughout the United States. We will go through the model build process with this asset, considering financial time series models in addition to the traditional ones we have already seen.\n\n\nCode\n# obtain data\ncxw &lt;- getSymbols(\"CXW\", auto.assign = FALSE, from = \"2006-01-01\", to = \"2022-12-31\")\nchartSeries(cxw, theme = chartTheme(\"white\"), # Theme\n            bar.type = \"hlc\",  # High low close \n            up.col = \"green\",  # Up candle color\n            dn.col = \"red\")   # Down candle color\n\n\n\n\n\n\n\nCode\nreturns &lt;- cxw$CXW.Adjusted %&gt;% diff()\nautoplot(returns) + ggtitle(\"CXW Returns\")\n\n\n\n\n\nVolatility clustering is visible, an ARCH model may be appropriate.\n\n\nCode\ngrid.arrange(ggAcf(returns),\n             ggPacf(returns),\n             ncol = 1)\ngrid.arrange(ggAcf(abs(returns)),\n             ggPacf(returns^2),\n             ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation is visible in both plots, meaning conditional variation is present.\n\nArchTest(returns, lags = 1, demean = TRUE)\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns\nChi-squared = 4.7167, df = 1, p-value = 0.02987\n\n\nThe p-value is &lt; 0.05, we reject the null hypothesis and can conclude that ARCH(1) effects are present in our data.\n\n\nCode\ngrid.arrange(ggAcf(returns),\n             ggPacf(returns),\n             ncol = 1)\n\n\n\n\n\nThe first differenced series looks good, we will try the following parameters:\np: 1, 3\nd: 0, 1\nq: 1, 3\n\n\nFit ARCH Model\ni &lt;- 1\n\ntemp &lt;- data.frame()\nls &lt;- matrix(rep(NA,6*17), nrow=17)\n\nfor(p in c(1, 3)){\n  for(q in c(1, 3)){\n    for(d in c(0, 1)){\n      if(p + d + q &lt;= 8){\n        model &lt;- Arima(cxw$CXW.Adjusted, order = c(p, d, q), include.drift = FALSE)\n        ls[i,] &lt;- c(p, d, q, model$aic, model$bic, model$aicc)\n        i &lt;- i +1\n      }\n    }\n  }\n}\n\ntemp &lt;- as.data.frame(ls)\nnames(temp) &lt;- c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\ntemp[which.min(temp$AIC),]\ntemp[which.min(temp$BIC),]\ntemp[which.min(temp$AICc),]\narima113 &lt;- Arima(cxw$CXW.Adjusted, order = c(1, 1, 3), include.drift = FALSE)\narima.res &lt;- arima113$residuals\nauto.arima(cxw$CXW.Adjusted)\n\n\n\n\n  p d q      AIC      BIC    AICc\n4 1 1 3 2672.796 2704.603 2672.81\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 2680.338 2699.422 2680.344\n\n\n  p d q      AIC      BIC    AICc\n4 1 1 3 2672.796 2704.603 2672.81\n\n\n\n\nSeries: cxw$CXW.Adjusted \nARIMA(0,1,0) \n\nsigma^2 = 0.1095:  log likelihood = -1338.85\nAIC=2679.69   AICc=2679.69   BIC=2686.05\n\n\n\n\nAn ARIMA(1, 1, 3) model was clearly the best model, both when fitting by hand and when using auto.arima().\n\n\nCode\ngrid.arrange(ggAcf(arima.res^2), ggPacf(arima.res^2), ncol = 1)\n\n\n\n\n\nCode\nmodel &lt;- list()\ncc &lt;- 1\n\nfor (p in 1:7) {\n  for(q in 1:7) {\n    model[[cc]] &lt;- garch(arima.res, order = c(q, p), trace = FALSE)\n    cc &lt;- cc + 1\n  }\n}\n\nARCH_AIC &lt;- sapply(model, AIC)\nmodel[[which(ARCH_AIC == min(ARCH_AIC))]]\n\n\n\nCall:\ngarch(x = arima.res, order = c(q, p), trace = FALSE)\n\nCoefficient(s):\n       a0         a1         b1         b2         b3         b4         b5  \n2.071e-03  1.732e-01  1.211e-01  9.809e-02  3.180e-01  4.843e-14  3.079e-02  \n       b6         b7  \n2.219e-01  3.947e-02  \n\n\nCode\ncat(\"AIC:\", min(ARCH_AIC))\n\n\nAIC: 1552.66\n\n\n\n\nCode\nsummary(arima.fit &lt;- Arima(cxw$CXW.Adjusted, order = c(1, 1, 3), include.drift = FALSE))\nsummary(final.fit &lt;- garchFit(~garch(1, 7), arima.res, trace = FALSE))\n\n\n\n\nSeries: cxw$CXW.Adjusted \nARIMA(1,1,3) \n\nCoefficients:\n          ar1     ma1      ma2     ma3\n      -0.6765  0.6498  -0.0107  0.0437\ns.e.   0.1296  0.1301   0.0183  0.0164\n\nsigma^2 = 0.1092:  log likelihood = -1331.4\nAIC=2672.8   AICc=2672.81   BIC=2704.6\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE     MASE\nTraining set 0.001000185 0.3302726 0.2127011 -0.023513 1.611635 1.000054\n                     ACF1\nTraining set 0.0002234626\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 7), data = arima.res, trace = FALSE) \n\nMean and Variance Equation:\n data ~ garch(1, 7)\n&lt;environment: 0x1211ae8c0&gt;\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2       beta3  \n0.01000185  0.00224949  0.14429636  0.01865167  0.00050029  0.83022373  \n     beta4       beta5       beta6       beta7  \n0.00000001  0.00318593  0.00000001  0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     1.000e-02   3.933e-03    2.543    0.011 *  \nomega  2.249e-03   4.814e-04    4.673 2.97e-06 ***\nalpha1 1.443e-01   2.421e-02    5.960 2.52e-09 ***\nbeta1  1.865e-02   4.865e-02    0.383    0.701    \nbeta2  5.003e-04   3.330e-02    0.015    0.988    \nbeta3  8.302e-01   1.866e-01    4.450 8.58e-06 ***\nbeta4  1.000e-08   8.599e-02    0.000    1.000    \nbeta5  3.186e-03   3.162e-02    0.101    0.920    \nbeta6  1.000e-08   1.610e-01    0.000    1.000    \nbeta7  1.000e-08   4.504e-02    0.000    1.000    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -784.9991    normalized:  -0.1834539 \n\nDescription:\n Fri Dec  1 14:02:03 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                   Statistic    p-Value\n Jarque-Bera Test   R    Chi^2  3.525303e+04 0.00000000\n Shapiro-Wilk Test  R    W      9.239471e-01 0.00000000\n Ljung-Box Test     R    Q(10)  1.675212e+01 0.08003197\n Ljung-Box Test     R    Q(15)  2.138543e+01 0.12496000\n Ljung-Box Test     R    Q(20)  2.673261e+01 0.14295581\n Ljung-Box Test     R^2  Q(10)  8.988129e+00 0.53323057\n Ljung-Box Test     R^2  Q(15)  1.024198e+01 0.80424880\n Ljung-Box Test     R^2  Q(20)  1.298826e+01 0.87788720\n LM Arch Test       R    TR^2   9.394395e+00 0.66892564\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.3715817 0.3864484 0.3715708 0.3768336 \n\n\n\n\nGARCH(1, 7) is the best model. p-Values for the Ljung-Box test indicate that there is not enough evidence to reject the null hypothesis that there is no autocorrelation between residuals, meaning this is an adequate model.\n\n\nForecast\ninvisible(predict(final.fit, n.ahead = 100, plot = TRUE, table = FALSE))\nht &lt;- final.fit@h.t\ndata &lt;- data.frame(ht, index(cxw))\nggplot(data, aes(x = index(cxw), y = ht)) +\n  geom_line() +\n  ylab('Conditional Variance') + \n  xlab('Date') + \n  ggtitle(\"Volatality plot\")\n\n\n\n\n\n\n\n\n\n\n\n\nPeriods of very high volatility can be seen, especially in the 2010’s."
  },
  {
    "objectID": "arimax_sarimax_var.html",
    "href": "arimax_sarimax_var.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "Code\nunrate_ts &lt;- ts(df$unrate, start = c(2006, 1), frequency = 12)\n\n\n\nExogenous Variables\nThe number of arrests made in any city, especially one as large and diverse as New York City, is a figure that is influenced by many variables in addition to time. This means that modelling the number of arrests only on time may result in overlooking valuable information that could be used to fit a model that could forecast arrests even more accurately. Similar work has been done in the past, relating both the economy and labor markets to crime statistics.\nWe will take that approach here, drawing inspiration on the work mentioned in the introduction by Freeman in which he looks at “the way decisions interact in a market setting” (Freeman 1999). He exhibits an important understanding of the influence economic factors have on crime, treating crime rates as another variable in our complex economic system. Looking at the labor market would be beneficial as well, as in the 1980’s and 90’s crime was seen to be closely related to unemployment rates (Gould, Weinberg, and Mustard 2002).\nAnother variable that might be useful to look at is the state of the current election cycle. It is possible that when public officials are up for re-election, they might be more incentivized to take more action to lower crime rates.\n\n\nModels to Fit\nTwo types of models will be fit here. We will use an ARIMAX or SARIMAX model when looking at the effect of one or more exogenous variables in predicting the number of arrests. We will use a VAR model when looking at how crime and other time series variables are influenced by each other. The specific models are listed below:\n\n(ARIMAX) Total Arrests ~ Unemployment Rate\n(ARIMAX) Total Arrests ~ Election Year\n(ARIMAX) Murder ~ Unemployment Rate + GDP\n(VAR) Robbery ~ Unemployment Rate\n(VAR) Controlled Substance Possession ~ Marijuana Possession\n\n\nTotal Arrests ~ Unemployment RateTotal Arrests ~ Election YearMurder ~ Unemployment RateRobbery ~ Unemployment RateControlled Substance Possession ~ Marijuana Possession\n\n\nWe have a univariate time series in total arrests and we want to see the effect the exogenous variable of unemployment rate has on that time series. We will fit an ARIMAX model, Total Arrests ~ Unemployment Rate. Unemployment data is for the New York City Metropolitan Area and obtained from here.\n\n\nCode\nmonth &lt;- as.Date(arrests_by_crime$month)\ndd &lt;- data.frame(month, arrests = arrests_ts, unrate = unrate_ts) %&gt;%\n  rename(arrests = Series.1)\n\nkable(head(dd))\n\n\n\n\n\nmonth\narrests\nunrate\n\n\n\n\n2006-01-01\n0.5041010\n5.0\n\n\n2006-02-01\n0.2405968\n5.2\n\n\n2006-03-01\n0.8459733\n4.8\n\n\n2006-04-01\n0.4411907\n4.5\n\n\n2006-05-01\n0.6487347\n4.4\n\n\n2006-06-01\n0.4296871\n4.5\n\n\n\n\n\n\n\n\n\nCode\ndd.ts &lt;- ts(dd, start = c(2006, 1), frequency = 12)\n\nautoplot(dd.ts[,c(2,3)], facets = TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Variables influencing Arrests in NYC\")\n\n\n\n\n\nThere looks to be some relation between these two variables, with the spike in 2020 in unemployment rate being at the same time as the low arrest rate, though we should fit a model to see if this relationship is substantial.\n\nFitting Using auto.arima\n\n\nCode\nxreg &lt;- cbind(unrate = dd.ts[,\"unrate\"])\n\nauto_model &lt;- auto.arima(dd.ts[,\"arrests\"], xreg = xreg)\nsummary(auto_model)\n\n\nSeries: dd.ts[, \"arrests\"] \nRegression with ARIMA(0,1,2)(2,0,0)[12] errors \n\nCoefficients:\n          ma1      ma2    sar1    sar2     xreg\n      -0.4433  -0.2617  0.4377  0.3516  -0.0498\ns.e.   0.0716   0.0717  0.0636  0.0664   0.0125\n\nsigma^2 = 0.04707:  log likelihood = 19.18\nAIC=-26.36   AICc=-25.93   BIC=-6.48\n\nTraining set error measures:\n                       ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set -0.008752053 0.2137413 0.149255 18.41134 68.24703 0.4816106\n                    ACF1\nTraining set -0.02046308\n\n\nCode\ncheckresiduals(auto_model)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,2)(2,0,0)[12] errors\nQ* = 23.882, df = 20, p-value = 0.2476\n\nModel df: 4.   Total lags used: 24\n\n\nWe have a SARIMAX model, a regression model with ARIMA(0,1,2)(2,0,0)[12] errors.\n\n\nFitting Manually\nWe first fit a linear regression model predicting arrests using unemployment rate. Then we will fit a SARIMA model for the residuals.\n\nfit.reg &lt;- lm(arrests ~ unrate, data = dd)\nsummary(fit.reg)\n\n\nCall:\nlm(formula = arrests ~ unrate, data = dd)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7469 -0.7450  0.3090  0.8008  1.3700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -0.31028    0.19502  -1.591    0.113  \nunrate       0.04907    0.02880   1.704    0.090 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9953 on 202 degrees of freedom\nMultiple R-squared:  0.01416,   Adjusted R-squared:  0.009283 \nF-statistic: 2.902 on 1 and 202 DF,  p-value: 0.09\n\n\n\nResidualsDifferenced ResidualsSeasonally Differenced Residuals\n\n\n\nres.fit &lt;- ts(residuals(fit.reg), start = c(2006, 1), frequency = 12)\nggtsdisplay(res.fit)\n\n\n\n\n\n\n\nggtsdisplay(res.fit %&gt;% diff())\n\n\n\n\n\n\n\nggtsdisplay(res.fit %&gt;% diff() %&gt;% diff(12))\n\n\n\n\n\n\n\nWe will try the following parameters:\n\np: 1, 4\nd: 0, 1\nq: 1, 4\nP: 1\nD: 0, 1\nQ: 1, 2, 3\n\n\n\nFitting Model\ni &lt;- 1\n\ntemp &lt;- data.frame()\nls &lt;- matrix(rep(NA,9*28), nrow=28)\n\nfor(p in c(1,4)){\n  for(q in c(1,4)){\n    for(d in c(0,1)){\n      for(P in c(1)){\n        for(Q in c(1,2,3)){\n          for(D in c(0,1)){\n            if(p + d + q + P + D + Q&lt;= 9){\n              tryCatch({\n                model &lt;- Arima(res.fit, order = c(p, d, q), seasonal = c(P, D, Q))\n                ls[i,] &lt;- c(p, d, q, P, D, Q, model$aic, model$bic, model$aicc)\n              }, error = function(err) {\n                cat()\n              }, finally = {\n                i &lt;- i + 1\n              })\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\ntemp &lt;- as.data.frame(ls)\nnames(temp) &lt;- c(\"p\",\"d\",\"q\",\"P\", \"D\", \"Q\", \"AIC\",\"BIC\",\"AICc\")\n\n\n\n\nModel Selection and Diagnostics\n\n\nCode\nkable(temp[which.min(temp$AIC),], digits = 2)\nkable(temp[which.min(temp$BIC),], digits = 2)\nkable(temp[which.min(temp$AICc),], digits = 2)\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n12\n1\n1\n1\n1\n1\n3\n-9.62\n13.15\n-9\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n8\n1\n1\n1\n1\n1\n1\n-9.3\n6.96\n-8.97\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n12\n1\n1\n1\n1\n1\n3\n-9.62\n13.15\n-9\n\n\n\n\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=1, Q=3\\). We now check the model diagnostics.\n\n\nCode\nset.seed(621)\nmodel_output &lt;- capture.output(sarima(res.fit, 1, 1, 1, 1, 1, 3, 12))\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 1, 2)12. The information criteria are all similar between the auto model and this model. We will proceed with this model.\n\nFitted vs. Actual\n\n\nCode\nmodel_fit &lt;- Arima(arrests_ts, order = c(1, 1, 1), seasonal = c(1, 1, 3), xreg = unrate_ts)\nplot(arrests_ts, col = \"blue\")\nlines(fitted(model_fit), col = \"green\")\nlegend(x = \"topright\", legend = c(\"res.fit\", \"fit1\"), fill = 4:1)\n\n\n\n\n\nThe the fitted model looks fairly similar to the actual model.\n\n\n\nCross Validation\n\n\nCode\n# minimum data length for fitting\nk &lt;- 48\nn &lt;- length(res.fit)\n\nst &lt;- tsp(res.fit)[1] + (k - 2)/12 # ending point: October 2009\n\n\nrmse1 &lt;- matrix(NA,n-k,12)\nrmse2 &lt;- matrix(NA,n-k,12)\n\nfor(i in seq(10, n - k, by = 5)) {\n  tryCatch({\n    xtrain &lt;- window(res.fit, end = st + i/12)\n    xtest &lt;- window(res.fit, start = st + (i + 1)/12, end = st + (i + 12)/12)\n    \n    fit1 &lt;- Arima(xtrain, \n                 order = c(1, 1, 1), \n                 seasonal = c(1, 1, 3),\n                 method = \"ML\")\n    \n    fcast1 &lt;- forecast(fit1, h = 12)\n    \n    fit2 &lt;- Arima(xtrain, \n                 order = c(0, 1, 2), \n                 seasonal = c(2, 0, 0),\n                 method = \"ML\")\n    \n    fcast2 &lt;- forecast(fit2, h = 12)\n    \n    rmse1[i, 1:length(xtest)] &lt;- sqrt((fcast1$mean-xtest)^2)\n    rmse2[i, 1:length(xtest)] &lt;- sqrt((fcast2$mean-xtest)^2)\n  }, error = function(err) {\n                cat()\n              })\n}\n\nplot(1:12, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, colMeans(rmse2,na.rm=TRUE), type=\"l\", col=3, xlab=\"horizon\", ylab=\"RMSE2\")\nlegend(\"topleft\",legend=c(\"manual fit\",\"auto fit\"),col=2:3,lty=1)\n\n\n\n\n\nThe manual fit looks slightly better, we will continue and forecast with that model.\n\nsummary(model_fit)\n\nSeries: arrests_ts \nRegression with ARIMA(1,1,1)(1,1,3)[12] errors \n\nCoefficients:\n         ar1      ma1    sar1     sma1    sma2    sma3     xreg\n      0.4156  -0.7760  0.3179  -1.0422  0.2259  0.1069  -0.0438\ns.e.  0.1147   0.0753  0.3602   0.3584  0.2805  0.0859   0.0146\n\nsigma^2 = 0.04395:  log likelihood = 26.55\nAIC=-37.1   AICc=-36.31   BIC=-11.08\n\nTraining set error measures:\n                     ME     RMSE       MAE     MPE     MAPE      MASE\nTraining set -0.0123099 0.199098 0.1405135 31.1782 73.41981 0.4534039\n                   ACF1\nTraining set 0.02897804\n\n\nModel Equation:\n\\(\\text{arrests}_t = 0.4156(\\text{arrests})_{t-1} - 0.7760(\\text{arrests})_{t-1} + 0.3179(\\text{arrests})_{t-12} - 1.0422(\\text{arrests})_{t-24} + 0.2259(\\text{arrests})_{t-36}- 0.0438(\\text{unrate})_{t}\\)\n\n\nForecasting\n\n\nCode\nunrate_forecast &lt;- forecast(auto.arima(unrate_ts))\n\nautoplot(forecast(model_fit, xreg = unrate_forecast$mean))\n\n\n\n\n\n\n\n\nIt would be interesting to see if total arrests changes in the presence of an election year. In theory, an election year would mean elected officials are under more pressure to bring about real change in their city, so you may see arrests change as a result. Maybe they go up as they want to appear as if they are doing more to get criminals off the streets, or maybe they go down as they want to decriminalize certain actions. Regardless, it would be interesting to look at total arrests in that context.\nNew York City holds mayoral elections every 4 years, with the most recent being in 2021.\n\n\nCode\nmonth &lt;- as.Date(arrests_by_date$month)\ndd &lt;- data.frame(month, arrests = arrests_ts) %&gt;%\n  rename(arrests = Series.1)\n\nelection_years &lt;- c(2021, 2017, 2013, 2009)\n\ndd &lt;- dd %&gt;% \n  mutate(election = if_else(year(month) %in% election_years, 1, 0))\n\nkable(head(dd))\n\n\n\n\n\nmonth\narrests\nelection\n\n\n\n\n2006-01-01\n0.5041010\n0\n\n\n2006-02-01\n0.2405968\n0\n\n\n2006-03-01\n0.8459733\n0\n\n\n2006-04-01\n0.4411907\n0\n\n\n2006-05-01\n0.6487347\n0\n\n\n2006-06-01\n0.4296871\n0\n\n\n\n\n\n\n\n\n\nCode\ndd.ts &lt;- ts(dd, start = c(2006, 1), frequency = 12)\n\nautoplot(dd.ts[,c(2,3)], facets = TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Variables influencing Arrests in NYC\")\n\n\n\n\n\nIt is hard to tell if there is any relationship, we should continue with the model build process.\n\nFitting Using auto.arima\n\n\nCode\nxreg &lt;- cbind(election = dd.ts[,\"election\"])\n\nauto_model &lt;- auto.arima(dd.ts[,\"arrests\"], xreg = xreg)\nsummary(auto_model)\n\n\nSeries: dd.ts[, \"arrests\"] \nRegression with ARIMA(1,1,1)(0,1,1)[12] errors \n\nCoefficients:\n         ar1      ma1     sma1    xreg\n      0.4274  -0.7401  -0.6937  0.0189\ns.e.  0.1397   0.1042   0.0554  0.0593\n\nsigma^2 = 0.04599:  log likelihood = 21\nAIC=-31.99   AICc=-31.67   BIC=-15.73\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.01432022 0.2053334 0.1438347 37.75989 73.85223 0.4641209\n                   ACF1\nTraining set 0.01935804\n\n\nCode\ncheckresiduals(auto_model)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,1,1)(0,1,1)[12] errors\nQ* = 19.631, df = 21, p-value = 0.5447\n\nModel df: 3.   Total lags used: 24\n\n\nWe have a SARIMAX mode, a regression model with ARIMA(1,1,1)(0,1,1)[12] errors\n\n\nFitting Manually\nWe first fit a linear regression model predicting arrests using unemployment rate. Then we will fit a SARIMA model for the residuals.\n\nfit.reg &lt;- lm(arrests ~ election, data = dd)\nsummary(fit.reg)\n\n\nCall:\nlm(formula = arrests ~ election, data = dd)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3642 -0.9092  0.2859  0.8338  1.4699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.02838    0.08016   0.354    0.724\nelection    -0.12060    0.16525  -0.730    0.466\n\nResidual standard error: 1.001 on 202 degrees of freedom\nMultiple R-squared:  0.00263,   Adjusted R-squared:  -0.002307 \nF-statistic: 0.5327 on 1 and 202 DF,  p-value: 0.4663\n\n\n\nResidualsDifferenced ResidualsSeasonally Differenced Residuals\n\n\n\nres.fit &lt;- ts(residuals(fit.reg), start = c(2006, 1), frequency = 12)\nggtsdisplay(res.fit)\n\n\n\n\n\n\n\nggtsdisplay(res.fit %&gt;% diff())\n\n\n\n\n\n\n\nggtsdisplay(res.fit %&gt;% diff() %&gt;% diff(12))\n\n\n\n\n\n\n\nWe will try the following parameters:\n\np: 1, 2, 4\nd: 0, 1\nq: 1, 4\nP: 1\nD: 0, 1\nQ: 1, 2, 3\n\n\n\nFitting Model\ni &lt;- 1\n\ntemp &lt;- data.frame()\nls &lt;- matrix(rep(NA,9*28), nrow=28)\n\nfor(p in c(1,2,4)){\n  for(q in c(1,4)){\n    for(d in c(0,1)){\n      for(P in c(1)){\n        for(Q in c(1,2,3)){\n          for(D in c(0,1)){\n            if(p + d + q + P + D + Q&lt;= 9){\n              tryCatch({\n                model &lt;- Arima(res.fit, order = c(p, d, q), seasonal = c(P, D, Q))\n                ls[i,] &lt;- c(p, d, q, P, D, Q, model$aic, model$bic, model$aicc)\n              }, error = function(err) {\n                cat()\n              }, finally = {\n                i &lt;- i + 1\n              })\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\ntemp &lt;- as.data.frame(ls)\nnames(temp) &lt;- c(\"p\",\"d\",\"q\",\"P\", \"D\", \"Q\", \"AIC\",\"BIC\",\"AICc\")\n\n\n\n\nCode\nkable(temp[which.min(temp$AIC),], digits = 2)\nkable(temp[which.min(temp$BIC),], digits = 2)\nkable(temp[which.min(temp$AICc),], digits = 2)\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n7\n1\n1\n1\n1\n0\n1\n-26.91\n-10.34\n-26.6\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n8\n1\n1\n1\n1\n1\n1\n-26.91\n-10.64\n-26.58\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n7\n1\n1\n1\n1\n0\n1\n-26.91\n-10.34\n-26.6\n\n\n\n\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1, P=1, D=0, Q=1\\). We now check the model diagnostics.\n\n\nCode\nset.seed(621)\nmodel_output &lt;- capture.output(sarima(res.fit, 1, 1, 1, 1, 0, 1, 12))\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest that there is no correlation between residuals, meaning we have a good enough model. We will proceed with the model SARIMA(1, 1, 1)(1, 0, 1)12. The information criteria are all similar between the auto model and this model. We will proceed with this model.\n\nFitted vs. Actual\n\n\nCode\nmodel_fit &lt;- Arima(arrests_ts, order = c(1, 1, 1), seasonal = c(1, 0, 1), xreg = dd$election)\nplot(res.fit, col = \"blue\")\nlines(fitted(model_fit), col = \"green\")\nlegend(x = \"topright\", legend = c(\"res.fit\", \"fit1\"), fill = 4:1)\n\n\n\n\n\nThe fitted model looks fairly similar to the actual model.\n\n\n\nCross Validation\n\n\nCode\n# minimum data length for fitting\nk &lt;- 48\nn &lt;- length(res.fit)\n\nst &lt;- tsp(res.fit)[1] + (k - 2)/12 # ending point: October 2009\n\n\nrmse1 &lt;- matrix(NA,n-k,12)\nrmse2 &lt;- matrix(NA,n-k,12)\n\nfor(i in seq(1, n - k, by = 1)) {\n  tryCatch({\n    xtrain &lt;- window(res.fit, end = st + i/12)\n    xtest &lt;- window(res.fit, start = st + (i + 1)/12, end = st + (i + 12)/12)\n    \n    fit1 &lt;- Arima(xtrain, \n                 order = c(1, 1, 1), \n                 seasonal = c(1, 0, 1),\n                 method = \"ML\")\n    \n    fcast1 &lt;- forecast(fit1, h = 12)\n    \n    fit2 &lt;- Arima(xtrain, \n                 order = c(1, 1, 1), \n                 seasonal = c(0, 1, 1),\n                 method = \"ML\")\n    \n    fcast2 &lt;- forecast(fit2, h = 12)\n    \n    rmse1[i, 1:length(xtest)] &lt;- sqrt((fcast1$mean-xtest)^2)\n    rmse2[i, 1:length(xtest)] &lt;- sqrt((fcast2$mean-xtest)^2)\n  }, error = function(err) {\n                cat()\n              })\n}\n\nplot(1:12, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, colMeans(rmse2,na.rm=TRUE), type=\"l\", col=3, xlab=\"horizon\", ylab=\"RMSE2\")\nlegend(\"topleft\",legend=c(\"manual fit\",\"auto fit\"),col=2:3,lty=1)\n\n\n\n\n\nThe auto fitted model is clearly better, we will continue to forecast with that model.\n\n\nCode\nmodel_fit &lt;- Arima(arrests_ts, order = c(1, 1, 1), seasonal = c(0, 1, 1), xreg = dd$election) \nelection_forecast &lt;- forecast(auto.arima(ts(dd$election)), h = 60)\nautoplot(forecast(model_fit, xreg = election_forecast$mean))\n\n\n\n\n\nThis looks very similar to the original arrests model, meaning we cannot conclude that the presence of an election year has any impact on arrest rate.\n\nsummary(model_fit)\n\nSeries: arrests_ts \nRegression with ARIMA(1,1,1)(0,1,1)[12] errors \n\nCoefficients:\n         ar1      ma1     sma1    xreg\n      0.4274  -0.7401  -0.6937  0.0189\ns.e.  0.1397   0.1042   0.0554  0.0593\n\nsigma^2 = 0.04599:  log likelihood = 21\nAIC=-31.99   AICc=-31.67   BIC=-15.73\n\nTraining set error measures:\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.01432022 0.2053334 0.1438347 37.75989 73.85223 0.4641209\n                   ACF1\nTraining set 0.01935804\n\n\nModel Equation:\n\\(\\text{arrests}_t = 0.4274(\\text{arrests})_{t-1} - 0.7401(\\text{arrests})_{t-1} - 0.6937(\\text{arrests})_{t-12} + 0.0189(\\text{election})_{t}\\)\n\n\n\nIt would be interesting to look at the relationship between unemployment rate and a specific crime, such as murder.\n\n\nCode\nmonth &lt;- as.Date(arrests_by_crime$month)\ndd &lt;- data.frame(month, murder = murder_ts, unrate = unrate_ts) %&gt;% \n  rename(murder = Series.1)\n\nkable(head(dd))\n\n\n\n\n\nmonth\nmurder\nunrate\n\n\n\n\n2006-01-01\n0.2787145\n5.0\n\n\n2006-02-01\n-1.1396010\n5.2\n\n\n2006-03-01\n-0.3887281\n4.8\n\n\n2006-04-01\n0.0284236\n4.5\n\n\n2006-05-01\n-0.3887281\n4.4\n\n\n2006-06-01\n-0.1801522\n4.5\n\n\n\n\n\n\n\n\n\nCode\ndd.ts &lt;- ts(dd, start = c(2006, 1), frequency = 12)\n\nautoplot(dd.ts[,c(2,3)], facets = TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Variables influencing Arrests in NYC\")\n\n\n\n\n\nThere looks to be some relation between these two variables, though we should fit a model to see if this relationship is substantial.\n\nFitting Using auto.arima\n\n\nCode\nxreg &lt;- cbind(unrate = dd.ts[,\"unrate\"])\n\nauto_model &lt;- auto.arima(dd.ts[,\"murder\"], xreg = xreg)\nsummary(auto_model)\n\n\nSeries: dd.ts[, \"murder\"] \nRegression with ARIMA(0,1,1) errors \n\nCoefficients:\n          ma1     xreg\n      -0.7259  -0.0206\ns.e.   0.0674   0.0379\n\nsigma^2 = 0.4875:  log likelihood = -214.49\nAIC=434.98   AICc=435.1   BIC=444.92\n\nTraining set error measures:\n                   ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.043429 0.6930606 0.5283975 12.96559 166.7963 0.6966557\n                   ACF1\nTraining set 0.05571558\n\n\nCode\ncheckresiduals(auto_model)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,1) errors\nQ* = 49.236, df = 23, p-value = 0.001159\n\nModel df: 1.   Total lags used: 24\n\n\nWe have an ARIMAX model, a regression model with ARIMA(0,1,1) errors.\n\n\nFitting Manually\nWe first fit a linear regression model predicting arrests using unemployment rate. The we will fit an ARIMA model for the residuals.\n\nfit.reg &lt;- lm(murder ~ unrate, data = dd)\nsummary(fit.reg)\n\n\nCall:\nlm(formula = murder ~ unrate, data = dd)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5842 -0.6704 -0.2491  0.2845  3.2433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.23933    0.19558   1.224    0.223\nunrate      -0.03785    0.02889  -1.310    0.192\n\nResidual standard error: 0.9982 on 202 degrees of freedom\nMultiple R-squared:  0.008426,  Adjusted R-squared:  0.003517 \nF-statistic: 1.717 on 1 and 202 DF,  p-value: 0.1916\n\n\n\nResidualsDifferenced Residuals\n\n\n\nres.fit &lt;- ts(residuals(fit.reg), start = c(2006, 1), frequency = 12)\nggtsdisplay(res.fit)\n\n\n\n\n\n\n\nggtsdisplay(res.fit %&gt;% diff())\n\n\n\n\n\n\n\nWe will try the following parameters:\n\np: 1, 2, 4\nd: 0, 1\nq: 1\n\n\n\nCode\ni &lt;- 1\n\ntemp &lt;- data.frame()\nls &lt;- matrix(rep(NA,6*5), nrow=5)\n\nfor(p in c(1,4)){\n  for(q in c(1,4)){\n    for(d in c(0,1)){\n      if(p + d + q + P + D + Q&lt;= 9){\n        tryCatch({\n          model &lt;- Arima(res.fit, order = c(p, d, q))\n          ls[i,] &lt;- c(p, d, q, model$aic, model$bic, model$aicc)\n        }, error = function(err) {\n          cat()\n        }, finally = {\n          i &lt;- i + 1\n        })\n      }\n    }\n  }\n}\n\ntemp &lt;- as.data.frame(ls)\nnames(temp) &lt;- c(\"p\",\"d\",\"q\", \"AIC\",\"BIC\",\"AICc\")\n\n\n\n\nModel Selection and Diagnostics\n\n\nCode\nkable(temp[which.min(temp$AIC),], digits = 2)\nkable(temp[which.min(temp$BIC),], digits = 2)\nkable(temp[which.min(temp$AICc),], digits = 2)\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n433.24\n443.18\n433.36\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n433.24\n443.18\n433.36\n\n\n\n\n\n\n\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n1\n433.24\n443.18\n433.36\n\n\n\n\n\n\n\n\n\n\n\nIt is clear that the best model is one with parameters \\(p=1, d=1, q=1\\). We now check the model diagnostics.\n\n\n\n\n\nThe Ljung-Box statistic p-values suggest there may be some correlation between residuals, thoug the normal Q-Q plot looks fairly linear. We can proceed with the model build process.\n\nFitted vs. Actual\n\n\nCode\nmodel_fit &lt;- Arima(murder_ts, order = c(1, 1, 1), xreg = unrate_ts)\nplot(murder_ts, col = \"blue\")\nlines(fitted(model_fit), col = \"green\")\nlegend(x = \"topright\", legend = c(\"res.fit\", \"fit1\"), fill = 4:1)\n\n\n\n\n\nThe the fitted model looks fairly similar to the actual model, though with less variance.\n\n\n\nCross Validation\n\n\nCode\n# minimum data length for fitting\nk &lt;- 48\nn &lt;- length(res.fit)\n\nst &lt;- tsp(res.fit)[1] + (k - 2)/12 # ending point: October 2009\n\n\nrmse1 &lt;- matrix(NA,n-k,12)\nrmse2 &lt;- matrix(NA,n-k,12)\n\nfor(i in seq(1, n - k, by = 1)) {\n  tryCatch({\n    xtrain &lt;- window(res.fit, end = st + i/12)\n    xtest &lt;- window(res.fit, start = st + (i + 1)/12, end = st + (i + 12)/12)\n    \n    fit1 &lt;- Arima(xtrain, \n                 order = c(1, 1, 1),\n                 method = \"ML\")\n    \n    fcast1 &lt;- forecast(fit1, h = 12)\n    \n    fit2 &lt;- Arima(xtrain, \n                 order = c(0, 1, 1),\n                 method = \"ML\")\n    \n    fcast2 &lt;- forecast(fit2, h = 12)\n    \n    rmse1[i, 1:length(xtest)] &lt;- sqrt((fcast1$mean-xtest)^2)\n    rmse2[i, 1:length(xtest)] &lt;- sqrt((fcast2$mean-xtest)^2)\n  }, error = function(err) {\n                cat()\n              })\n}\n\nplot(1:12, colMeans(rmse1,na.rm=TRUE), type=\"l\", col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:12, colMeans(rmse2,na.rm=TRUE), type=\"l\", col=3, xlab=\"horizon\", ylab=\"RMSE2\")\nlegend(\"topleft\",legend=c(\"manual fit\",\"auto fit\"),col=2:3,lty=1)\n\n\n\n\n\nThe manual fit is clearly better than the auto fit. We will proceed with ARIMAX(1, 1, 1).\n\n\nForecasting\n\n\nCode\nunrate_forecast &lt;- forecast(auto.arima(unrate_ts))\n\nautoplot(forecast(model_fit, xreg = unrate_forecast$mean))\n\n\n\n\n\nAdding unemployment rate resulted in little change from the previous model, suggesting that the two are not related.\n\nsummary(model_fit)\n\nSeries: murder_ts \nRegression with ARIMA(1,1,1) errors \n\nCoefficients:\n         ar1      ma1     xreg\n      0.1436  -0.8117  -0.0218\ns.e.  0.0956   0.0593   0.0377\n\nsigma^2 = 0.4851:  log likelihood = -213.53\nAIC=435.06   AICc=435.26   BIC=448.31\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.05240214 0.6896166 0.5255024 26.40197 153.1133 0.6928387\n                    ACF1\nTraining set -0.02506703\n\n\nModel Equation:\n\\(\\text{muder}_t = 0.1436(\\text{murder})_{t-1} - 0.8117(\\text{murder})_{t-1} - 0.0218(\\text{unrate})_{t}\\)\n\n\n\nThe first relationship we will look at between two time series will be between robbery and unemployment rate.\n\n\nCode\nmonth &lt;- as.Date(arrests_by_crime$month)\ndd &lt;- data.frame(month, robbery = robbery_ts, unrate = unrate_ts) %&gt;% \n  rename(robbery = Series.1)\n\nkable(head(dd))\n\n\n\n\n\nmonth\nrobbery\nunrate\n\n\n\n\n2006-01-01\n1.2970231\n5.0\n\n\n2006-02-01\n0.0812727\n5.2\n\n\n2006-03-01\n1.3571426\n4.8\n\n\n2006-04-01\n-0.2660846\n4.5\n\n\n2006-05-01\n1.2369036\n4.4\n\n\n2006-06-01\n1.7712994\n4.5\n\n\n\n\n\n\n\n\n\nCode\ndd.ts &lt;- ts(dd, start = c(2006, 1), frequency = 12)\n\nautoplot(dd.ts[,c(2,3)], facets = TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Variables influencing Robbery Arrests in NYC\")\n\n\n\n\n\nThere looks to be some relation between these two variables, with the spike in 2020 in unemployment rate being at the same time as the low arrest rate, though we should fit a model to see if this relationship is substantial.\n\nFitting Using VARSelect\n\n\nCode\nVARselect(dd[,c(2,3)], lag.max = 10, type = \"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     3      3      3      3 \n\n$criteria\n                1          2          3          4          5          6\nAIC(n) -1.2058407 -1.3637208 -1.4750019 -1.4575550 -1.4631654 -1.4722165\nHQ(n)  -1.1512737 -1.2818704 -1.3658681 -1.3211377 -1.2994647 -1.2812322\nSC(n)  -1.0710836 -1.1615852 -1.2054878 -1.1206624 -1.0588943 -1.0005668\nFPE(n)  0.2994437  0.2557177  0.2287997  0.2328474  0.2315755  0.2295318\n                7          8          9         10\nAIC(n) -1.4410391 -1.4347946 -1.4294481 -1.4061713\nHQ(n)  -1.2227714 -1.1892435 -1.1566135 -1.1060532\nSC(n)  -0.9020109 -0.8283880 -0.7556629 -0.6650076\nFPE(n)  0.2368595  0.2384194  0.2397933  0.2455606\n\n\nClearly p = 3 is a good parameter, we will also try VAR(1) in addition to VAR(3)\n\n\nCode\nsummary(vars::VAR(dd[, c(2,3)], p = 1, type = \"both\"))\nsummary(vars::VAR(dd[, c(2,3)], p = 3, type = \"both\"))\n\n\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: robbery, unrate \nDeterministic variables: both \nSample size: 203 \nLog Likelihood: -446.854 \nRoots of the characteristic polynomial:\n0.9406 0.2057\nCall:\nvars::VAR(y = dd[, c(2, 3)], p = 1, type = \"both\")\n\n\nEstimation results for equation robbery: \n======================================== \nrobbery = robbery.l1 + unrate.l1 + const + trend \n\n           Estimate Std. Error t value Pr(&gt;|t|)    \nrobbery.l1  0.20839    0.06938   3.004  0.00301 ** \nunrate.l1  -0.05831    0.01950  -2.990  0.00314 ** \nconst       1.39536    0.19884   7.018 3.47e-11 ***\ntrend      -0.01003    0.00118  -8.495 4.60e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6477 on 199 degrees of freedom\nMultiple R-Squared: 0.5853, Adjusted R-squared: 0.579 \nF-statistic: 93.62 on 3 and 199 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation unrate: \n======================================= \nunrate = robbery.l1 + unrate.l1 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \nrobbery.l1 -0.033302   0.089298  -0.373   0.7096    \nunrate.l1   0.937992   0.025102  37.367   &lt;2e-16 ***\nconst       0.511873   0.255939   2.000   0.0469 *  \ntrend      -0.001216   0.001519  -0.800   0.4244    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.8337 on 199 degrees of freedom\nMultiple R-Squared: 0.884,  Adjusted R-squared: 0.8822 \nF-statistic: 505.5 on 3 and 199 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n         robbery   unrate\nrobbery  0.41953 -0.01788\nunrate  -0.01788  0.69508\n\nCorrelation matrix of residuals:\n         robbery   unrate\nrobbery  1.00000 -0.03312\nunrate  -0.03312  1.00000\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: robbery, unrate \nDeterministic variables: both \nSample size: 201 \nLog Likelihood: -407.212 \nRoots of the characteristic polynomial:\n0.9289 0.6725 0.5582 0.5582 0.3277 0.3277\nCall:\nvars::VAR(y = dd[, c(2, 3)], p = 3, type = \"both\")\n\n\nEstimation results for equation robbery: \n======================================== \nrobbery = robbery.l1 + unrate.l1 + robbery.l2 + unrate.l2 + robbery.l3 + unrate.l3 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \nrobbery.l1  0.117336   0.069210   1.695 0.091619 .  \nunrate.l1  -0.081983   0.055853  -1.468 0.143772    \nrobbery.l2  0.192277   0.068429   2.810 0.005467 ** \nunrate.l2  -0.090901   0.087309  -1.041 0.299115    \nrobbery.l3  0.213591   0.070565   3.027 0.002808 ** \nunrate.l3   0.146713   0.055323   2.652 0.008669 ** \nconst       0.786443   0.230902   3.406 0.000802 ***\ntrend      -0.006095   0.001464  -4.162 4.74e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6101 on 193 degrees of freedom\nMultiple R-Squared: 0.6397, Adjusted R-squared: 0.6267 \nF-statistic: 48.96 on 7 and 193 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation unrate: \n======================================= \nunrate = robbery.l1 + unrate.l1 + robbery.l2 + unrate.l2 + robbery.l3 + unrate.l3 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \nrobbery.l1 -0.152458   0.086542  -1.762  0.07971 .  \nunrate.l1   1.304373   0.069839  18.677  &lt; 2e-16 ***\nrobbery.l2  0.186272   0.085565   2.177  0.03070 *  \nunrate.l2  -0.562589   0.109173  -5.153 6.31e-07 ***\nrobbery.l3  0.154524   0.088236   1.751  0.08149 .  \nunrate.l3   0.203963   0.069177   2.948  0.00359 ** \nconst       0.165255   0.288726   0.572  0.56774    \ntrend       0.001686   0.001831   0.921  0.35835    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.7629 on 193 degrees of freedom\nMultiple R-Squared: 0.9055, Adjusted R-squared: 0.9021 \nF-statistic: 264.2 on 7 and 193 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n         robbery   unrate\nrobbery  0.37226 -0.05351\nunrate  -0.05351  0.58205\n\nCorrelation matrix of residuals:\n        robbery unrate\nrobbery   1.000 -0.115\nunrate   -0.115  1.000\n\n\n\n\n\n\nCross Validation\n\n\nCross Validation\nts &lt;- ts(dd[,c(2,3)], start = c(2006, 1), frequency = 12)\n\nk &lt;- 72\n\nrmse1 &lt;- matrix(NA, 132, 2)\nrmse2 &lt;- matrix(NA, 132, 2)\n\nyear &lt;- c()\n\nst &lt;- tsp(ts)[1] + (k - 1)/12\n\nfor(i in 1:11) {\n  xtrain &lt;- window(ts, end=st + i-1)\n  xtest &lt;- window(ts, start=st + (i-1) + 1/12, end=st + i)\n  \n  # first model\n  fit &lt;- VAR(ts, p=1, type='both')\n  fcast &lt;- predict(fit, n.ahead = 12)\n  \n  frob&lt;-fcast$fcst$robbery\n  funr&lt;-fcast$fcst$unrate\n  \n  ff&lt;-data.frame(frob[,1],funr[,1]) #collecting the forecasts for 2 variables\n  \n  year&lt;-st + (i-1) + 1/12 #starting year\n  \n  ff&lt;-ts(ff,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11\n  b= 12*i \n  \n  tryCatch({\n    rmse1[c(a:b),]  &lt;- sqrt((ff-xtest)^2)\n  }, error = function(err) {\n      cat(a, b)\n      rmse1[c(a:b),]  &lt;- sqrt((ff-xtest)^2)\n  })\n  \n  \n  # second model\n  fit2 &lt;- VAR(ts, p=3, type='both')\n  fcast2 &lt;- predict(fit2, n.ahead = 12)\n  \n  frob&lt;-fcast2$fcst$robbery\n  funr&lt;-fcast2$fcst$unrate\n  \n  ff2&lt;-data.frame(frob[,1],funr[,1]) #collecting the forecasts for 3 variables\n  \n  year&lt;-st + (i-1) + 1/12 #starting year\n  \n  ff2&lt;-ts(ff2,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11\n  b= 12*i \n  \n  tryCatch({\n    rmse2[c(a:b),]  &lt;- sqrt((ff2-xtest)^2)\n  }, error = function(err) {\n      cat(a, b)\n      rmse2[c(a:b),]  &lt;- sqrt((ff2-xtest)^2)\n  })\n}\n\n\n\n\nPlot Results\nyr = rep(c(2012:2022),each =12) #year\nm = rep(paste0(1:12),11) #month\n\nrmse1 &lt;- data.frame(yr,m,rmse1)\nrmse1$date &lt;- as.Date(paste(rmse1$yr, rmse1$m, \"01\", sep = \"-\"))\nnames(rmse1) &lt;- c(\"Year\", \"Month\",\"robbery\",\"unemployment\", \"Date\")\nrmse2 &lt;- data.frame(yr,m,rmse2)\nrmse2$date &lt;- as.Date(paste(rmse2$yr, rmse2$m, \"01\", sep = \"-\"))\nnames(rmse2) &lt;- c(\"Year\", \"Month\",\"robbery\",\"unemployment\", \"Date\")\n\nggplot() + \n  geom_line(data = rmse1, aes(x = Date, y = robbery),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Date, y = robbery),color = \"red\") +\n  labs(\n    title = \"CV RMSE for robbery\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\nPlot Results\nggplot() + \n  geom_line(data = rmse1, aes(x = Date, y = unemployment),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Date, y = unemployment),color = \"red\") +\n  labs(\n    title = \"CV RMSE for unemployment\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\nPlot Results\nmean(rmse1$robbery)\n\n\n[1] 0.8851127\n\n\nPlot Results\nmean(rmse2$robbery)\n\n\n[1] 0.8789264\n\n\nPlot Results\nmean(rmse1$unemployment)\n\n\n[1] 2.254941\n\n\nPlot Results\nmean(rmse2$unemployment)\n\n\n[1] 1.948857\n\n\nThe models are very close in performance, but it looks like VAR(3) is slightly better so we will forecast with that model.\n\n\nForecasting\n\n\nCode\nfit &lt;- VAR(ts, p = 3, type = \"both\")\nplot(forecast(fit, 24))\n\n\n\n\n\nRobbery and unemployment rate appear to be inversely related, as their forecasts look similar but in opposite directions. Robbery arrests are expected to decrease in the coming years, while unemployment rate is expected to increase.\n\nsummary(fit)\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: robbery, unrate \nDeterministic variables: both \nSample size: 201 \nLog Likelihood: -407.212 \nRoots of the characteristic polynomial:\n0.9289 0.6725 0.5582 0.5582 0.3277 0.3277\nCall:\nVAR(y = ts, p = 3, type = \"both\")\n\n\nEstimation results for equation robbery: \n======================================== \nrobbery = robbery.l1 + unrate.l1 + robbery.l2 + unrate.l2 + robbery.l3 + unrate.l3 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \nrobbery.l1  0.117336   0.069210   1.695 0.091619 .  \nunrate.l1  -0.081983   0.055853  -1.468 0.143772    \nrobbery.l2  0.192277   0.068429   2.810 0.005467 ** \nunrate.l2  -0.090901   0.087309  -1.041 0.299115    \nrobbery.l3  0.213591   0.070565   3.027 0.002808 ** \nunrate.l3   0.146713   0.055323   2.652 0.008669 ** \nconst       0.786443   0.230902   3.406 0.000802 ***\ntrend      -0.006095   0.001464  -4.162 4.74e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.6101 on 193 degrees of freedom\nMultiple R-Squared: 0.6397, Adjusted R-squared: 0.6267 \nF-statistic: 48.96 on 7 and 193 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation unrate: \n======================================= \nunrate = robbery.l1 + unrate.l1 + robbery.l2 + unrate.l2 + robbery.l3 + unrate.l3 + const + trend \n\n            Estimate Std. Error t value Pr(&gt;|t|)    \nrobbery.l1 -0.152458   0.086542  -1.762  0.07971 .  \nunrate.l1   1.304373   0.069839  18.677  &lt; 2e-16 ***\nrobbery.l2  0.186272   0.085565   2.177  0.03070 *  \nunrate.l2  -0.562589   0.109173  -5.153 6.31e-07 ***\nrobbery.l3  0.154524   0.088236   1.751  0.08149 .  \nunrate.l3   0.203963   0.069177   2.948  0.00359 ** \nconst       0.165255   0.288726   0.572  0.56774    \ntrend       0.001686   0.001831   0.921  0.35835    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.7629 on 193 degrees of freedom\nMultiple R-Squared: 0.9055, Adjusted R-squared: 0.9021 \nF-statistic: 264.2 on 7 and 193 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n         robbery   unrate\nrobbery  0.37226 -0.05351\nunrate  -0.05351  0.58205\n\nCorrelation matrix of residuals:\n        robbery unrate\nrobbery   1.000 -0.115\nunrate   -0.115  1.000\n\n\nThis shows us model equations for both robbery and unemployment rate. Robbery rate appears to be a better predictor of unemployment rate than vice versa.\n\n\n\nLooking at the relationship between two drug crimes would be interesting.\n\nmonth &lt;- as.Date(arrests_by_crime$month)\ndd &lt;- data.frame(month, controlled = controlled_pos_ts, marijuana = marijuana_pos_ts) %&gt;% \n  rename(controlled = Series.1,\n         marijuana = Series.1.1)\n\nkable(head(dd))\n\n\n\n\nmonth\ncontrolled\nmarijuana\n\n\n\n\n2006-01-01\n1.0731108\n0.2764828\n\n\n2006-02-01\n0.8159753\n0.2200679\n\n\n2006-03-01\n1.2638094\n0.4420078\n\n\n2006-04-01\n1.0558864\n0.4829241\n\n\n2006-05-01\n1.1604630\n0.5697162\n\n\n2006-06-01\n0.8012115\n0.4606061\n\n\n\n\n\n\n\n\n\nCode\ndd.ts &lt;- ts(dd, start = c(2006, 1), frequency = 12)\n\nautoplot(dd.ts[,c(2,3)], facets = TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Controlled Substance and Marijuana Possession Arrests in NYC\")\n\n\n\n\n\nThere looks to be some relation between these two variables, as when marijuana arrests hit 0, controlled substance arrests rose for the first time.\n\nFitting Using VARSelect\n\nVARselect(dd[,c(2,3)], lag.max = 10, type = \"both\")\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     9      1      1      9 \n\n$criteria\n                  1            2            3           4            5\nAIC(n) -6.511681992 -6.485344544 -6.528033097 -6.51655163 -6.587315798\nHQ(n)  -6.457115076 -6.403494170 -6.418899264 -6.38013434 -6.423615050\nSC(n)  -6.376924954 -6.283208988 -6.258519022 -6.17965904 -6.183044686\nFPE(n)  0.001485996  0.001525695  0.001462015  0.00147903  0.001378169\n                  6            7            8            9           10\nAIC(n) -6.590980025 -6.556831567 -6.562486672 -6.643458339 -6.637545519\nHQ(n)  -6.399995819 -6.338563902 -6.316935550 -6.370623758 -6.337427480\nSC(n)  -6.119330394 -6.017803417 -5.956080004 -5.969673152 -5.896381813\nFPE(n)  0.001373384  0.001421446  0.001413882  0.001304431  0.001312809\n\n\np = 6 and 9 seem like good parameters, we will try both VAR(6) and VAR(9).\n\n\nCode\nsummary(vars::VAR(dd[, c(2,3)], p = 6, type = \"both\"))\nsummary(vars::VAR(dd[, c(2,3)], p = 9, type = \"both\"))\n\n\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: controlled, marijuana \nDeterministic variables: both \nSample size: 198 \nLog Likelihood: 118.205 \nRoots of the characteristic polynomial:\n0.951 0.8609 0.7545 0.7545 0.7395 0.7395 0.7383 0.7383 0.7306 0.7306 0.6895 0.2869\nCall:\nvars::VAR(y = dd[, c(2, 3)], p = 6, type = \"both\")\n\n\nEstimation results for equation controlled: \n=========================================== \ncontrolled = controlled.l1 + marijuana.l1 + controlled.l2 + marijuana.l2 + controlled.l3 + marijuana.l3 + controlled.l4 + marijuana.l4 + controlled.l5 + marijuana.l5 + controlled.l6 + marijuana.l6 + const + trend \n\n               Estimate Std. Error t value Pr(&gt;|t|)    \ncontrolled.l1  0.168650   0.100410   1.680  0.09473 .  \nmarijuana.l1   0.029242   0.123430   0.237  0.81299    \ncontrolled.l2 -0.030578   0.118815  -0.257  0.79719    \nmarijuana.l2   0.147341   0.169155   0.871  0.38486    \ncontrolled.l3  0.509902   0.118424   4.306 2.70e-05 ***\nmarijuana.l3  -0.489623   0.169150  -2.895  0.00426 ** \ncontrolled.l4  0.086337   0.122235   0.706  0.48088    \nmarijuana.l4  -0.230342   0.172259  -1.337  0.18281    \ncontrolled.l5 -0.083414   0.120365  -0.693  0.48918    \nmarijuana.l5   0.372071   0.170976   2.176  0.03082 *  \ncontrolled.l6 -0.070728   0.098143  -0.721  0.47204    \nmarijuana.l6   0.148065   0.129024   1.148  0.25264    \nconst          0.765453   0.172415   4.440 1.55e-05 ***\ntrend         -0.007556   0.001619  -4.667 5.88e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2432 on 184 degrees of freedom\nMultiple R-Squared: 0.9446, Adjusted R-squared: 0.9407 \nF-statistic: 241.2 on 13 and 184 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation marijuana: \n========================================== \nmarijuana = controlled.l1 + marijuana.l1 + controlled.l2 + marijuana.l2 + controlled.l3 + marijuana.l3 + controlled.l4 + marijuana.l4 + controlled.l5 + marijuana.l5 + controlled.l6 + marijuana.l6 + const + trend \n\n               Estimate Std. Error t value Pr(&gt;|t|)    \ncontrolled.l1 -0.436764   0.081576  -5.354 2.54e-07 ***\nmarijuana.l1   0.969598   0.100279   9.669  &lt; 2e-16 ***\ncontrolled.l2 -0.061242   0.096530  -0.634 0.526584    \nmarijuana.l2   0.163875   0.137427   1.192 0.234619    \ncontrolled.l3  0.378015   0.096212   3.929 0.000121 ***\nmarijuana.l3  -0.455087   0.137423  -3.312 0.001117 ** \ncontrolled.l4  0.004183   0.099308   0.042 0.966444    \nmarijuana.l4  -0.070335   0.139949  -0.503 0.615865    \ncontrolled.l5  0.062998   0.097788   0.644 0.520233    \nmarijuana.l5   0.147411   0.138907   1.061 0.289982    \ncontrolled.l6 -0.183714   0.079735  -2.304 0.022337 *  \nmarijuana.l6   0.177225   0.104824   1.691 0.092590 .  \nconst          0.530862   0.140077   3.790 0.000204 ***\ntrend         -0.005208   0.001315  -3.959 0.000108 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1976 on 184 degrees of freedom\nMultiple R-Squared: 0.9644, Adjusted R-squared: 0.9619 \nF-statistic: 383.7 on 13 and 184 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n           controlled marijuana\ncontrolled    0.05913   0.03325\nmarijuana     0.03325   0.03903\n\nCorrelation matrix of residuals:\n           controlled marijuana\ncontrolled      1.000     0.692\nmarijuana       0.692     1.000\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: controlled, marijuana \nDeterministic variables: both \nSample size: 195 \nLog Likelihood: 133.385 \nRoots of the characteristic polynomial:\n0.9634 0.8886 0.8886 0.8871 0.8871 0.8658 0.8658 0.8598 0.8598 0.8534 0.8464 0.8464 0.7841 0.7841 0.7046 0.7046 0.5531 0.5531\nCall:\nvars::VAR(y = dd[, c(2, 3)], p = 9, type = \"both\")\n\n\nEstimation results for equation controlled: \n=========================================== \ncontrolled = controlled.l1 + marijuana.l1 + controlled.l2 + marijuana.l2 + controlled.l3 + marijuana.l3 + controlled.l4 + marijuana.l4 + controlled.l5 + marijuana.l5 + controlled.l6 + marijuana.l6 + controlled.l7 + marijuana.l7 + controlled.l8 + marijuana.l8 + controlled.l9 + marijuana.l9 + const + trend \n\n               Estimate Std. Error t value Pr(&gt;|t|)    \ncontrolled.l1  0.176637   0.101152   1.746 0.082523 .  \nmarijuana.l1   0.063460   0.124271   0.511 0.610234    \ncontrolled.l2  0.007320   0.121334   0.060 0.951959    \nmarijuana.l2   0.064843   0.171259   0.379 0.705427    \ncontrolled.l3  0.506367   0.121376   4.172 4.75e-05 ***\nmarijuana.l3  -0.424332   0.171100  -2.480 0.014083 *  \ncontrolled.l4  0.037569   0.126780   0.296 0.767328    \nmarijuana.l4  -0.236271   0.174420  -1.355 0.177287    \ncontrolled.l5 -0.052540   0.123341  -0.426 0.670650    \nmarijuana.l5   0.309209   0.173263   1.785 0.076055 .  \ncontrolled.l6 -0.106393   0.124753  -0.853 0.394919    \nmarijuana.l6   0.205201   0.174989   1.173 0.242531    \ncontrolled.l7 -0.036502   0.123189  -0.296 0.767344    \nmarijuana.l7   0.191893   0.173734   1.105 0.270885    \ncontrolled.l8  0.116662   0.121056   0.964 0.336524    \nmarijuana.l8  -0.487069   0.172821  -2.818 0.005383 ** \ncontrolled.l9 -0.095039   0.098772  -0.962 0.337276    \nmarijuana.l9   0.292566   0.130150   2.248 0.025830 *  \nconst          0.812548   0.209513   3.878 0.000149 ***\ntrend         -0.007974   0.001949  -4.092 6.51e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2391 on 175 degrees of freedom\nMultiple R-Squared: 0.9478, Adjusted R-squared: 0.9421 \nF-statistic: 167.2 on 19 and 175 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation marijuana: \n========================================== \nmarijuana = controlled.l1 + marijuana.l1 + controlled.l2 + marijuana.l2 + controlled.l3 + marijuana.l3 + controlled.l4 + marijuana.l4 + controlled.l5 + marijuana.l5 + controlled.l6 + marijuana.l6 + controlled.l7 + marijuana.l7 + controlled.l8 + marijuana.l8 + controlled.l9 + marijuana.l9 + const + trend \n\n               Estimate Std. Error t value Pr(&gt;|t|)    \ncontrolled.l1 -0.414932   0.078736  -5.270 3.98e-07 ***\nmarijuana.l1   0.994857   0.096732  10.285  &lt; 2e-16 ***\ncontrolled.l2 -0.049927   0.094445  -0.529 0.597732    \nmarijuana.l2   0.086355   0.133306   0.648 0.517969    \ncontrolled.l3  0.353311   0.094478   3.740 0.000249 ***\nmarijuana.l3  -0.419259   0.133183  -3.148 0.001933 ** \ncontrolled.l4 -0.087257   0.098684  -0.884 0.377798    \nmarijuana.l4  -0.036391   0.135767  -0.268 0.788983    \ncontrolled.l5  0.083437   0.096008   0.869 0.386003    \nmarijuana.l5   0.086543   0.134866   0.642 0.521910    \ncontrolled.l6 -0.160851   0.097107  -1.656 0.099427 .  \nmarijuana.l6   0.174636   0.136210   1.282 0.201500    \ncontrolled.l7 -0.047619   0.095889  -0.497 0.620096    \nmarijuana.l7   0.169475   0.135233   1.253 0.211804    \ncontrolled.l8  0.215914   0.094229   2.291 0.023133 *  \nmarijuana.l8  -0.578613   0.134523  -4.301 2.82e-05 ***\ncontrolled.l9 -0.212893   0.076883  -2.769 0.006229 ** \nmarijuana.l9   0.456117   0.101308   4.502 1.22e-05 ***\nconst          0.695540   0.163083   4.265 3.27e-05 ***\ntrend         -0.006725   0.001517  -4.434 1.63e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1861 on 175 degrees of freedom\nMultiple R-Squared: 0.9698, Adjusted R-squared: 0.9665 \nF-statistic: 295.4 on 19 and 175 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n           controlled marijuana\ncontrolled    0.05717   0.02994\nmarijuana     0.02994   0.03464\n\nCorrelation matrix of residuals:\n           controlled marijuana\ncontrolled     1.0000    0.6728\nmarijuana      0.6728    1.0000\n\n\n\n\n\n\nCross Validation\n\n\nCross Validation\nts &lt;- ts(dd[,c(2,3)], start = c(2006, 1), frequency = 12)\n\nk &lt;- 72\n\nrmse1 &lt;- matrix(NA, 132, 2)\nrmse2 &lt;- matrix(NA, 132, 2)\n\nyear &lt;- c()\n\nst &lt;- tsp(ts)[1] + (k - 1)/12\n\nfor(i in 1:11) {\n  xtrain &lt;- window(ts, end=st + i-1)\n  xtest &lt;- window(ts, start=st + (i-1) + 1/12, end=st + i)\n  \n  # first model\n  fit &lt;- VAR(ts, p=6, type='both')\n  fcast &lt;- predict(fit, n.ahead = 12)\n  \n  frob&lt;-fcast$fcst$controlled\n  funr&lt;-fcast$fcst$marijuana\n  \n  ff&lt;-data.frame(frob[,1],funr[,1]) #collecting the forecasts for 2 variables\n  \n  year&lt;-st + (i-1) + 1/12 #starting year\n  \n  ff&lt;-ts(ff,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11\n  b= 12*i \n  \n  tryCatch({\n    rmse1[c(a:b),]  &lt;- sqrt((ff-xtest)^2)\n  }, error = function(err) {\n      cat(a, b)\n      rmse1[c(a:b),]  &lt;- sqrt((ff-xtest)^2)\n  })\n  \n  \n  # second model\n  fit2 &lt;- VAR(ts, p=9, type='both')\n  fcast2 &lt;- predict(fit2, n.ahead = 12)\n  \n  frob&lt;-fcast2$fcst$controlled\n  funr&lt;-fcast2$fcst$marijuana\n  \n  ff2&lt;-data.frame(frob[,1],funr[,1]) #collecting the forecasts for 3 variables\n  \n  year&lt;-st + (i-1) + 1/12 #starting year\n  \n  ff2&lt;-ts(ff2,start=c(year,1),frequency = 12)\n  \n  a = 12*i-11\n  b= 12*i \n  \n  tryCatch({\n    rmse2[c(a:b),]  &lt;- sqrt((ff2-xtest)^2)\n  }, error = function(err) {\n      cat(a, b)\n      rmse2[c(a:b),]  &lt;- sqrt((ff2-xtest)^2)\n  })\n}\n\n\n\n\nPlot Results\nyr = rep(c(2012:2022),each =12) #year\nm = rep(paste0(1:12),11) #month\n\nrmse1 &lt;- data.frame(yr,m,rmse1)\nrmse1$date &lt;- as.Date(paste(rmse1$yr, rmse1$m, \"01\", sep = \"-\"))\nnames(rmse1) &lt;- c(\"Year\", \"Month\",\"controlled\",\"marijuana\", \"Date\")\nrmse2 &lt;- data.frame(yr,m,rmse2)\nrmse2$date &lt;- as.Date(paste(rmse2$yr, rmse2$m, \"01\", sep = \"-\"))\nnames(rmse2) &lt;- c(\"Year\", \"Month\",\"controlled\",\"marijuana\", \"Date\")\n\nggplot() + \n  geom_line(data = rmse1, aes(x = Date, y = controlled),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Date, y = controlled),color = \"red\") +\n  labs(\n    title = \"CV RMSE for controlled substance possession\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\nPlot Results\nggplot() + \n  geom_line(data = rmse1, aes(x = Date, y = marijuana),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Date, y = marijuana),color = \"red\") +\n  labs(\n    title = \"CV RMSE for marijuana possession\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\nPlot Results\nmean(rmse1$controlled)\n\n\n[1] 1.147872\n\n\nPlot Results\nmean(rmse2$controlled)\n\n\n[1] 1.145504\n\n\nPlot Results\nmean(rmse1$marijuana)\n\n\n[1] 1.101446\n\n\nPlot Results\nmean(rmse2$marijuana)\n\n\n[1] 1.212972\n\n\nBoth models are very close in performance, we will go with VAR(6) as it is the more parsimonious model.\n\n\nForecasting\n\n\nCode\nfit &lt;- VAR(ts, p = 6, type = \"both\")\nplot(forecast(fit, 24))\n\n\n\n\n\nThe models have similar forecasts, suggesting they are closely related. Despite the uptick in controlled substance possession arrests, we expect the trend to reverse when considering how the marijuana arrests series has behaved over the years.\n\nsummary(fit)\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: controlled, marijuana \nDeterministic variables: both \nSample size: 198 \nLog Likelihood: 118.205 \nRoots of the characteristic polynomial:\n0.951 0.8609 0.7545 0.7545 0.7395 0.7395 0.7383 0.7383 0.7306 0.7306 0.6895 0.2869\nCall:\nVAR(y = ts, p = 6, type = \"both\")\n\n\nEstimation results for equation controlled: \n=========================================== \ncontrolled = controlled.l1 + marijuana.l1 + controlled.l2 + marijuana.l2 + controlled.l3 + marijuana.l3 + controlled.l4 + marijuana.l4 + controlled.l5 + marijuana.l5 + controlled.l6 + marijuana.l6 + const + trend \n\n               Estimate Std. Error t value Pr(&gt;|t|)    \ncontrolled.l1  0.168650   0.100410   1.680  0.09473 .  \nmarijuana.l1   0.029242   0.123430   0.237  0.81299    \ncontrolled.l2 -0.030578   0.118815  -0.257  0.79719    \nmarijuana.l2   0.147341   0.169155   0.871  0.38486    \ncontrolled.l3  0.509902   0.118424   4.306 2.70e-05 ***\nmarijuana.l3  -0.489623   0.169150  -2.895  0.00426 ** \ncontrolled.l4  0.086337   0.122235   0.706  0.48088    \nmarijuana.l4  -0.230342   0.172259  -1.337  0.18281    \ncontrolled.l5 -0.083414   0.120365  -0.693  0.48918    \nmarijuana.l5   0.372071   0.170976   2.176  0.03082 *  \ncontrolled.l6 -0.070728   0.098143  -0.721  0.47204    \nmarijuana.l6   0.148065   0.129024   1.148  0.25264    \nconst          0.765453   0.172415   4.440 1.55e-05 ***\ntrend         -0.007556   0.001619  -4.667 5.88e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.2432 on 184 degrees of freedom\nMultiple R-Squared: 0.9446, Adjusted R-squared: 0.9407 \nF-statistic: 241.2 on 13 and 184 DF,  p-value: &lt; 2.2e-16 \n\n\nEstimation results for equation marijuana: \n========================================== \nmarijuana = controlled.l1 + marijuana.l1 + controlled.l2 + marijuana.l2 + controlled.l3 + marijuana.l3 + controlled.l4 + marijuana.l4 + controlled.l5 + marijuana.l5 + controlled.l6 + marijuana.l6 + const + trend \n\n               Estimate Std. Error t value Pr(&gt;|t|)    \ncontrolled.l1 -0.436764   0.081576  -5.354 2.54e-07 ***\nmarijuana.l1   0.969598   0.100279   9.669  &lt; 2e-16 ***\ncontrolled.l2 -0.061242   0.096530  -0.634 0.526584    \nmarijuana.l2   0.163875   0.137427   1.192 0.234619    \ncontrolled.l3  0.378015   0.096212   3.929 0.000121 ***\nmarijuana.l3  -0.455087   0.137423  -3.312 0.001117 ** \ncontrolled.l4  0.004183   0.099308   0.042 0.966444    \nmarijuana.l4  -0.070335   0.139949  -0.503 0.615865    \ncontrolled.l5  0.062998   0.097788   0.644 0.520233    \nmarijuana.l5   0.147411   0.138907   1.061 0.289982    \ncontrolled.l6 -0.183714   0.079735  -2.304 0.022337 *  \nmarijuana.l6   0.177225   0.104824   1.691 0.092590 .  \nconst          0.530862   0.140077   3.790 0.000204 ***\ntrend         -0.005208   0.001315  -3.959 0.000108 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 0.1976 on 184 degrees of freedom\nMultiple R-Squared: 0.9644, Adjusted R-squared: 0.9619 \nF-statistic: 383.7 on 13 and 184 DF,  p-value: &lt; 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n           controlled marijuana\ncontrolled    0.05913   0.03325\nmarijuana     0.03325   0.03903\n\nCorrelation matrix of residuals:\n           controlled marijuana\ncontrolled      1.000     0.692\nmarijuana       0.692     1.000\n\n\nEquations for both models can be seen in the output above. Both models are excellent predictors of each other, suggesting they are closely related.\n\n\n\n\n\n\n\n\n\nReferences\n\nFreeman, Richard B. 1999. “Chapter 52 The Economics of Crime.” In Handbook of Labor Economics, 3:3529–71. Elsevier. https://doi.org/10.1016/S1573-4463(99)30043-2.\n\n\nGould, Eric D., Bruce A. Weinberg, and David B. Mustard. 2002. “Crime Rates and Local Labor Market Opportunities in the United States: 1979–1997.” Review of Economics and Statistics 84 (1): 45–61. https://doi.org/10.1162/003465302317331919."
  },
  {
    "objectID": "data_visualization.html",
    "href": "data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "It is important to get an understanding of what types of crimes are commonly committed in New York City, the area we are currently looking at. We want to look at crimes that give us a good idea of the state of public safety in New York City. Those crimes along with their total counts from 2006-2022 are listed below.\n\n\n\n\n\npd_desc\ntotal\n\n\n\n\nassault\n681166\n\n\ncontrolled_substance_possession\n336350\n\n\ncontrolled_substance_sale\n241811\n\n\nlarceny\n396885\n\n\nmarijuana_possession\n445336\n\n\nmarijuana_sale\n67848\n\n\nmotor_vehicle_theft\n19099\n\n\nmurder\n16997\n\n\nrobbery\n179078\n\n\n\n\n\n\n\nThese categories include multiple types of crimes, each of which is outlined below.\n\narrests_by_crime_detail &lt;- df %&gt;%\n  filter(str_detect(pd_desc, \"ASSAULT|ROBBERY|MARIJUANA, POSSESSION|MARIJUANA, SALE|CE,P|NCE, PE,I|CE, I|E,S|E, S|MURDER,UNCLASSIFIED|AUTO|LARCENY\")) %&gt;% group_by(pd_desc) %&gt;%\n  summarize(total = n())\n\nkable(arrests_by_crime_detail)\n\n\n\n\npd_desc\ntotal\n\n\n\n\nAGGRAVATED GRAND LARCENY OF ATM\n434\n\n\nASSAULT 2,1,PEACE OFFICER\n17301\n\n\nASSAULT 2,1,UNCLASSIFIED\n197972\n\n\nASSAULT 3\n460078\n\n\nASSAULT POLICE/PEACE OFFICER\n4729\n\n\nCONTROLLED SUBSTANCE, INTENT T\n2053\n\n\nCONTROLLED SUBSTANCE, INTENT TO SELL 5\n16135\n\n\nCONTROLLED SUBSTANCE, SALE 4\n1498\n\n\nCONTROLLED SUBSTANCE, SALE 5\n4022\n\n\nCONTROLLED SUBSTANCE,POSSESS.\n2727\n\n\nCONTROLLED SUBSTANCE,POSSESS. 1\n6746\n\n\nCONTROLLED SUBSTANCE,POSSESS. 2\n5472\n\n\nCONTROLLED SUBSTANCE,POSSESS. 3\n2386\n\n\nCONTROLLED SUBSTANCE,POSSESS. OF PROCURSERS\n126\n\n\nCONTROLLED SUBSTANCE,SALE 1\n2265\n\n\nCONTROLLED SUBSTANCE,SALE 2\n1963\n\n\nCONTROLLED SUBSTANCE,SALE 3\n88904\n\n\nLARCENY,GRAND BY ACQUIRING LOS\n86\n\n\nLARCENY,GRAND BY CREDIT CARD USE\n518\n\n\nLARCENY,GRAND BY EXTORTION\n1868\n\n\nLARCENY,GRAND BY THEFT OF CREDIT CARD\n23\n\n\nLARCENY,GRAND FROM BUILDING (NON-RESIDENCE) UNATTENDED\n13\n\n\nLARCENY,GRAND FROM BUILDING,UNCLASSIFIED\n43\n\n\nLARCENY,GRAND FROM OPEN AREAS,\n2688\n\n\nLARCENY,GRAND FROM OPEN AREAS, UNATTENDED\n27738\n\n\nLARCENY,GRAND FROM OPEN AREAS,UNCLASSIFIED\n85217\n\n\nLARCENY,GRAND FROM PERSON,UNCL\n3598\n\n\nLARCENY,GRAND FROM PERSON,UNCLASSIFIED\n19451\n\n\nLARCENY,GRAND OF AUTO\n19099\n\n\nLARCENY,PETIT BY ACQUIRING LOS\n218\n\n\nLARCENY,PETIT FROM OPEN AREAS,\n66643\n\n\nLARCENY,PETIT FROM OPEN AREAS,UNCLASSIFIED\n188347\n\n\nMARIJUANA, POSSESSION\n10196\n\n\nMARIJUANA, POSSESSION 1, 2 & 3\n11357\n\n\nMARIJUANA, POSSESSION 4 & 5\n423783\n\n\nMARIJUANA, SALE 1, 2 & 3\n3255\n\n\nMARIJUANA, SALE 4 & 5\n64593\n\n\nMURDER,UNCLASSIFIED\n16997\n\n\nROBBERY,CAR JACKING\n137\n\n\nROBBERY,CARJACKING OF MV OTHER THAN TRUCK\n289\n\n\nROBBERY,GAS STATION\n83\n\n\nROBBERY,OPEN AREA UNCLASSIFIED\n34851\n\n\nROBBERY,UNCLASSIFIED,OPEN AREA\n455\n\n\nROBBERY,UNCLASSIFIED,OPEN AREAS\n143263\n\n\nVEHICULAR ASSAULT (INTOX DRIVE\n894\n\n\nVEHICULAR ASSAULT (INTOX DRIVER)\n192\n\n\n\n\n\n\n\nLets look at how each of these crimes is changing over time.\n\n\n\n\n\n\nThis graph includes the eight most common types of arrests made over the time period. Some interesting patterns reveal themselves when the data are categorized as such, particularly the arrests for marijuana possession. They spike around 2011 but then drop off quickly and today are non-existent. This is an excellent example to use to observe the effect that real-world events have on the trends we observe here. In 2014, New York City mayor Bill de Blasio told the NYPD to stop arrests for marijuana possession and instead issue tickets in attempts to decriminalize marijuana (Dizard 2014). A drop in arrests is clearly seen around that time period. Around the same time, New York Governor Andrew Cuomo signed legislation which would allow the use of cannabis for medicinal purposes (Campbell 2014). Then, in 2021, recreational cannabis was legalized for adults over 21, up to a specific amount. Since then, the NYPD has not listed any crime related to marijuana, as evidenced by there being no arrests for possession on the graph past 2021."
  },
  {
    "objectID": "data_visualization.html#by-crime",
    "href": "data_visualization.html#by-crime",
    "title": "Data Visualization",
    "section": "",
    "text": "It is important to get an understanding of what types of crimes are commonly committed in New York City, the area we are currently looking at. We want to look at crimes that give us a good idea of the state of public safety in New York City. Those crimes along with their total counts from 2006-2022 are listed below.\n\n\n\n\n\npd_desc\ntotal\n\n\n\n\nassault\n681166\n\n\ncontrolled_substance_possession\n336350\n\n\ncontrolled_substance_sale\n241811\n\n\nlarceny\n396885\n\n\nmarijuana_possession\n445336\n\n\nmarijuana_sale\n67848\n\n\nmotor_vehicle_theft\n19099\n\n\nmurder\n16997\n\n\nrobbery\n179078\n\n\n\n\n\n\n\nThese categories include multiple types of crimes, each of which is outlined below.\n\narrests_by_crime_detail &lt;- df %&gt;%\n  filter(str_detect(pd_desc, \"ASSAULT|ROBBERY|MARIJUANA, POSSESSION|MARIJUANA, SALE|CE,P|NCE, PE,I|CE, I|E,S|E, S|MURDER,UNCLASSIFIED|AUTO|LARCENY\")) %&gt;% group_by(pd_desc) %&gt;%\n  summarize(total = n())\n\nkable(arrests_by_crime_detail)\n\n\n\n\npd_desc\ntotal\n\n\n\n\nAGGRAVATED GRAND LARCENY OF ATM\n434\n\n\nASSAULT 2,1,PEACE OFFICER\n17301\n\n\nASSAULT 2,1,UNCLASSIFIED\n197972\n\n\nASSAULT 3\n460078\n\n\nASSAULT POLICE/PEACE OFFICER\n4729\n\n\nCONTROLLED SUBSTANCE, INTENT T\n2053\n\n\nCONTROLLED SUBSTANCE, INTENT TO SELL 5\n16135\n\n\nCONTROLLED SUBSTANCE, SALE 4\n1498\n\n\nCONTROLLED SUBSTANCE, SALE 5\n4022\n\n\nCONTROLLED SUBSTANCE,POSSESS.\n2727\n\n\nCONTROLLED SUBSTANCE,POSSESS. 1\n6746\n\n\nCONTROLLED SUBSTANCE,POSSESS. 2\n5472\n\n\nCONTROLLED SUBSTANCE,POSSESS. 3\n2386\n\n\nCONTROLLED SUBSTANCE,POSSESS. OF PROCURSERS\n126\n\n\nCONTROLLED SUBSTANCE,SALE 1\n2265\n\n\nCONTROLLED SUBSTANCE,SALE 2\n1963\n\n\nCONTROLLED SUBSTANCE,SALE 3\n88904\n\n\nLARCENY,GRAND BY ACQUIRING LOS\n86\n\n\nLARCENY,GRAND BY CREDIT CARD USE\n518\n\n\nLARCENY,GRAND BY EXTORTION\n1868\n\n\nLARCENY,GRAND BY THEFT OF CREDIT CARD\n23\n\n\nLARCENY,GRAND FROM BUILDING (NON-RESIDENCE) UNATTENDED\n13\n\n\nLARCENY,GRAND FROM BUILDING,UNCLASSIFIED\n43\n\n\nLARCENY,GRAND FROM OPEN AREAS,\n2688\n\n\nLARCENY,GRAND FROM OPEN AREAS, UNATTENDED\n27738\n\n\nLARCENY,GRAND FROM OPEN AREAS,UNCLASSIFIED\n85217\n\n\nLARCENY,GRAND FROM PERSON,UNCL\n3598\n\n\nLARCENY,GRAND FROM PERSON,UNCLASSIFIED\n19451\n\n\nLARCENY,GRAND OF AUTO\n19099\n\n\nLARCENY,PETIT BY ACQUIRING LOS\n218\n\n\nLARCENY,PETIT FROM OPEN AREAS,\n66643\n\n\nLARCENY,PETIT FROM OPEN AREAS,UNCLASSIFIED\n188347\n\n\nMARIJUANA, POSSESSION\n10196\n\n\nMARIJUANA, POSSESSION 1, 2 & 3\n11357\n\n\nMARIJUANA, POSSESSION 4 & 5\n423783\n\n\nMARIJUANA, SALE 1, 2 & 3\n3255\n\n\nMARIJUANA, SALE 4 & 5\n64593\n\n\nMURDER,UNCLASSIFIED\n16997\n\n\nROBBERY,CAR JACKING\n137\n\n\nROBBERY,CARJACKING OF MV OTHER THAN TRUCK\n289\n\n\nROBBERY,GAS STATION\n83\n\n\nROBBERY,OPEN AREA UNCLASSIFIED\n34851\n\n\nROBBERY,UNCLASSIFIED,OPEN AREA\n455\n\n\nROBBERY,UNCLASSIFIED,OPEN AREAS\n143263\n\n\nVEHICULAR ASSAULT (INTOX DRIVE\n894\n\n\nVEHICULAR ASSAULT (INTOX DRIVER)\n192\n\n\n\n\n\n\n\nLets look at how each of these crimes is changing over time.\n\n\n\n\n\n\nThis graph includes the eight most common types of arrests made over the time period. Some interesting patterns reveal themselves when the data are categorized as such, particularly the arrests for marijuana possession. They spike around 2011 but then drop off quickly and today are non-existent. This is an excellent example to use to observe the effect that real-world events have on the trends we observe here. In 2014, New York City mayor Bill de Blasio told the NYPD to stop arrests for marijuana possession and instead issue tickets in attempts to decriminalize marijuana (Dizard 2014). A drop in arrests is clearly seen around that time period. Around the same time, New York Governor Andrew Cuomo signed legislation which would allow the use of cannabis for medicinal purposes (Campbell 2014). Then, in 2021, recreational cannabis was legalized for adults over 21, up to a specific amount. Since then, the NYPD has not listed any crime related to marijuana, as evidenced by there being no arrests for possession on the graph past 2021."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "What is a Time Series?\nAny metric that is measured over regular time intervals makes a Time Series. A time series is a sequence of data points or observations collected or recorded over a period of time at specific, equally spaced intervals. Each data point in a time series is associated with a particular time stamp or time period, making it possible to analyze and study how a particular variable or phenomenon changes over time. Time series data can be found in various domains and can represent a wide range of phenomena, including financial data, economic indicators, weather measurements, stock prices, sales figures, and more.\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones. The analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\nKey characteristics of time series data include:\nTemporal Order: Time series data is ordered chronologically, with each data point representing an observation at a specific point in time. The order of data points is critical for understanding trends and patterns over time.\nEqually Spaced Intervals: In most cases, time series data is collected at regular intervals, such as hourly, daily, weekly, monthly, or yearly. However, irregularly spaced time series data can also exist.\nDependency: Time series data often exhibits temporal dependency, meaning that the value at a given time is influenced by or related to the values at previous times. This dependency can take various forms, including trends, seasonality. This serial correlation is called as autocorrelation.\nComponents: Time series data can typically be decomposed into various components, including:\nTrend: The long-term movement or direction in the data. Seasonality: Repeating patterns or cycles that occur at fixed intervals. Noise/Irregularity: Random fluctuations or variability in the data that cannot be attributed to the trend or seasonality.\nApplications: Time series data is widely used for various applications, including forecasting future values, identifying patterns and anomalies, understanding underlying trends, and making informed decisions based on historical data.\nAnalyzing time series data involves techniques like time series decomposition, smoothing, statistical modeling, and forecasting. This class will cover but not be limited to traditional time series modeling including ARIMA, SARIMA, the multivariate Time Series modeling including; ARIMAX, SARIMAX, and VAR models, Financial Time Series modeling including; ARCH, GARCH models, and E-GARCH, M-GARCH..ect, Bayesian structural time series (BSTS) models, Spectral Analysis and Deep Learning Techniques for Time Series. Researchers and analysts use software tools like Python, R, and specialized time series libraries to work with and analyze time series data effectively.\nTime series analysis is essential in fields such as finance, economics, epidemiology, environmental science, engineering, and many others, as it provides insights into how variables change over time and allows for the development of predictive models to forecast future trends and outcomes."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thomas Sigall\nGeorgetown DSAN"
  },
  {
    "objectID": "deep_learning_for_ts.html",
    "href": "deep_learning_for_ts.html",
    "title": "Deep Learning for Time Series",
    "section": "",
    "text": "Let’s fit three deep learning models to the overall arrests time series."
  },
  {
    "objectID": "deep_learning_for_ts.html#dropout-regularized-gru",
    "href": "deep_learning_for_ts.html#dropout-regularized-gru",
    "title": "Deep Learning for Time Series",
    "section": "Dropout-regularized GRU",
    "text": "Dropout-regularized GRU\n\n\nCode\n# training and evaluating a GRU-based model\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\n\nmodel = Sequential()\nmodel.add(layers.GRU(32, \n                     dropout=0.2,\n                     recurrent_dropout=0.2,\n                     input_shape=(None, float_data.shape[-1])))\nmodel.add(layers.Dense(1))\n\nmodel.compile(optimizer=RMSprop(), loss='mae', metrics=RootMeanSquaredError())\nhistory = model.fit_generator(train_gen,\n                              steps_per_epoch=500,\n                              epochs=40,\n                              validation_data=val_gen,\n                              validation_steps=val_steps)\n\nrgru_rmse = history.history['val_root_mean_squared_error']\n\n\n\n\n\n\n\nThis one looks worse than the previous model, there is more evidence of overfitting."
  }
]